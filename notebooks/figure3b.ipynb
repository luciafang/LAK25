{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T04:21:07.646929Z",
     "start_time": "2024-06-23T04:21:07.645274Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T04:21:09.236625Z",
     "start_time": "2024-06-23T04:21:09.234336Z"
    }
   },
   "cell_type": "code",
   "source": "all_files = '../transcripts_processed/*'",
   "id": "183e0c3d16b2b7a7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T04:32:20.816740Z",
     "start_time": "2024-06-23T04:32:20.777086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_file_transcripts = []\n",
    "for filename in glob.glob(all_files):\n",
    "    print(filename)\n",
    "    transcript_df = pd.read_csv(filename)\n",
    "    transcript_df.fillna('', inplace=True)\n",
    "    file_transcript = ' '.join(transcript_df.transcript.to_list())\n",
    "    all_file_transcripts.append(file_transcript)"
   ],
   "id": "c1ae230289e08f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../transcripts_processed/20190221_transcript_all.csv\n",
      "../transcripts_processed/20190430_transcript_all.csv\n",
      "../transcripts_processed/20190423_transcript_all.csv\n",
      "../transcripts_processed/20190307_transcript_all.csv\n",
      "../transcripts_processed/20190418_transcript_all.csv\n",
      "../transcripts_processed/20190425_transcript_all.csv\n",
      "../transcripts_processed/20190321_transcript_all.csv\n",
      "../transcripts_processed/20190207_transcript_all.csv\n",
      "../transcripts_processed/20190416_transcript_all.csv\n",
      "../transcripts_processed/20190502_transcript_all.csv\n",
      "../transcripts_processed/20190226_transcript_all.csv\n",
      "../transcripts_processed/20190409_transcript_all.csv\n",
      "../transcripts_processed/20190404_transcript_all.csv\n",
      "../transcripts_processed/20190326_transcript_all.csv\n",
      "../transcripts_processed/20190328_transcript_all.csv\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T04:34:29.384463Z",
     "start_time": "2024-06-23T04:34:29.370935Z"
    }
   },
   "cell_type": "code",
   "source": "all_file_transcripts",
   "id": "6e5013a47c06e0d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"So I'm going to talk to you for a minute about what we did. I thought it would be worth it for somebody to help me through this. So I'm going to go ahead and open this up. All right. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So number one is today's lecture is going to be a little bit all over the place, there's some parts of lectures that I didn't quite get to, or I didn't want to have enough time left in the lecture to finish, I think, appropriately, so we're actually going to go back and revisit some slides that I didn't get to, and I added to them, and I've added some new details, so instead of this being a very prepared lecture, we're going to actually touch back on some topics before, and then there may even be some time at the end of class for you guys to meet and ask me more questions, I know there's a lot of people kind of with burning questions that they want to ask you about, but I thought I'd go through some big items, the next time we meet is going to be in this classroom, please definitely show up on time, ready to go, laptop charge, you don't necessarily know where there's going to be a plug, so definitely make sure that that's all ready to go, again, it has to be on this Mac laptop, if you don't have a Mac, or anyone on your team has a Mac, definitely come console with me so that I can get you a Mac laptop, no accessories, you can't have a mouse or anything like that, so it's going to be the trackpad as it is, you can configure the sensitivity as you wish, you can even configure the sensitivity for your users. which you can have that sort of mouse cursor preferences open. Definitely make sure that your code runs. I've never really had a case where a team shows up and it's totally broken. But obviously, if it doesn't work, it doesn't compile in a job, it doesn't compile in quotes, then obviously it's going to be hard for you to be able to participate. So definitely make sure it doesn't show up with it broken or exploded. You have to be able to let it go. I will be going around the room along with our two TAs to check legality and just make sure everything is fair. And we'll give you feedback as we go around as well. Also, be sure the scaffold code is set to number repeats equals one, which means that every button appears once. It repeats exactly one time, and everything happens once. Make sure your code works with higher numbers. We might set that to 10 in the bake-offs. So definitely make sure you can change that to 2, 3, 4, 5, whatever, a million, some 10 that's reasonable. Probably a number between 1 and 10 is OK for the bake-off. So make sure, a lot of people have been asking me, oh, can we have the buttons disappear after you click on them once? And the answer is twofold. One, no, that's illegal, because every button has to be equally clickable at all times. And obviously, if it disappears, they're not going to make it very clickable. And number two, you can't guarantee that in the bake-off, it is going to disappear, because it's probably going to repeat at least n times. And you should play around with that behavior. Sadly, I think there's two or three teams that I've seen no ideas from at all. So that's making me a little bit worried for you. I don't want you to be blowing your whole weekend on this. I mean, hopefully, two weeks is enough time to get this done. If you're actually starting substantially today, which is like 90% of your time expires, that's definitely not good group practice. And definitely, as I mentioned before, iteration is going to take actually more time. So you really want to get your first working data as soon as possible, because the polish actually is probably. 50% of the work. So definitely email me ideas. One clarification I was going to make, and I'm not going to be a little bit, admittedly, a little bit mysterious on this clarification, because I don't want to give away a core idea. But as I've said to many teams, you're allowed to decorate sort of the target square however you want. You know, if you want to make it right now like blue, you want to make it flash or have like little tiny ants inside of it or something crazy, all power to you. Inside the box is totally fine. If you're doing anything outside of the square, that has to be fair to all buttons. So you have to treat them all equally. So any extra button activities should be equal. And again, email me your ideas. Send me sketches, send me little videos to make sure that it's legal. Because again, it's like design. It's no right or wrong. It's like all gray area. So some things that seem very illegal, I've actually let teams do. So you can try to push the boundaries and see where those are. That's being creative, OK? Any general questions? And of course, you definitely can ask me specific idea questions at the end of class. OK, so just come ready to participate on Tuesday. And it's going to be a very different, literally no lecture, of course. There's going to be a lot, a lot of clicking. Tens of thousands of clicks are going to happen in this room. Let's do a quick quiz. Get that out of the way. It's a little bit easier than the last one. So I will go to the other end. How about that? I'm going to go to the front door here. I don't have a quiz yet. I don't have a quiz yet. Do you have a quiz? Do you have a quiz? For question... 5. It's what you could plausibly want to take back. What you might consider taking back. You don't have to take back. but you've considered it. Okay, 45 seconds. How many people have done it? Raise your hand if you've done it. Okay, 10 newtons. Okay, start passing them in. How many people thought this one was easier than the last one? Raise your hand. That's not what we're doing. We just went over the material last lecture. It's easier. Okay, got them all, yes? The file is closing in five seconds. Yes, is there one more? Okay, last call, yes? Okay, very quickly we get the answer. So, one of the two HCI monikers, the first one being know thy user, the second HCI moniker is user is not like me. Right, recognizing that people are different. How many people put user not like me? Okay, with surveys. In an online survey, like SurveyMonkey, you're just calling up phone numbers. You generally learn. You know you can call thousands, because you can blast a meme of a thousand people, so hopefully people know that it's a thousands of people, but it would be weird to send a SurveyMonkey to just two people you can send an email to. And the answer here is that you learn a few things, but about a thousand people is the kind of score that you can assess A, okay? Contextual inquiry advocates which model? And that is the master-apprentice model, the 100 master-apprentice model. You literally discovered this 48 hours ago. Going to a workplace to see how work unfolds helps you better understand what? How do people put the floor plan? It does. I mean, that's not a wrong answer, it's just not the best answer. The context? Yes, literally it's in the name. So it helps you better understand the context. The other ones are true. You learn about the floor plan, you understand better about the workplace equipment, and you'll even learn something about the cafeteria, but that's not really the best answer. The best answer is to try to understand the whole context which encompasses all those things. Okay, when visiting the workplace, the probability of connection increases, you might consider taking back a view and assume you have permission. How do people put photos? Yes, I think photos are acceptable. Notes? Yes. Sketches? Yes. Auto-recordings? Yes. You might consider, again, we talked about the pros and cons, none of them are perfect, but I think all those do. And who puts furniture? Yes, generally you probably don't take the furniture unless it's a very good reason. Okay, and the purpose of, this is probably the hardest one here, but the purpose of rephrasing an answer or detail to someone that you're studying, this is the rephrase and validate part of when people are unsure, and you're telling them your understanding of it, and this is to understand, so this is B, confirm your understanding. You rephrase it to them, and they go, ah, that's mostly correct, or yeah, that's correct. That's confirming your understanding. They already know their understanding, so B is the best answer here. How many people put B? Okay, okay, good, see, it picked up. Okay, next thing I want to catch up on is the video prototype. I had a chance, I think all the grades are posted already, but I wanted to pull out some of the ones that I thought were kind of interesting and cool, and ones that were kind of ridiculous and funny, and I was just going to show you four examples of what some of the other teams did. There was a lot of good ones, and actually, I think this is the best year in terms of coming up with good motivations. I would say 95% of the teams had actually really good motivations, and I'm really thrilled with you. I want to have a good story, a good setup, and no one just jumped right into their opening shot. This is an interface, so I'm going to click through. Good job on that, but let me just show you what some of the other teams did. Oh my gosh, keynote. Here we go. Song is leaving his house to get to class. He checks his calendar for any upcoming events. Oh no, an exam tomorrow, he needs a quiet place to study ASAP. But all the places Song knows about are so loud. Oh wait, he has the CMU app. Finding a quiet place is a breeze with the CMU app. He first presses browser. So this was a still with a narrator and photoshop. Then he's able to browse through all study areas and see their noise level. Great, a quiet place to study. Very simple, very kind of simply done, but very effective. It doesn't have to be complicated. Very good kind of example to be a first time. Here's another one. So instead of filling out the document that Song left out, Song opens up the document. He makes a code, starts a code. and she leaves it in the designated area, Gate 6. Janice gets a notification on her smartwatch from Free Software that there's a new item offered. A canvas tote bag that's just what she needs. Clicking into the notification, she sees more details of the item and decides to claim it. She heads over to Gate and finds the bag in the area. Again, very, very simple setup. Super fussy, and it kind of gets to the interface really fast. And you can pretty much understand the whole idea within 30 seconds. Now the next video I'm going to show you, and I don't mean to, like, super criticize, but it takes a long time to get to the actual point. So I'll play this one. We all live extremely busy lives, from being a student to having a job hanging out with friends, meetings with professors, and et cetera. There are so many things going on in our lives that makes scheduling things between people quite difficult. So there's a bit of money this day we end up opening our eyes to. Where are they? I'm waiting all day for them. It's very smartphone-centric. I'll fast forward here. Some interesting terms. We all live extremely busy lives. Yeah, okay, I'll be there in, like, two days. One of the students said that'll be enough time to get ahold of us. Oh, thanks! Yeah, actually, I was going to a meeting with some of my classmates, but I went to the wrong UC. But you said that. That's what I was thinking. Well, we should reschedule, right? Reschedule. I was, like, tracking the timings. You get the idea. It's interesting. It's pretty tongue-in-cheek. And then here's one final one. So it's video as opposed to stills. It's paper instead of Photoshop. Very fast narration just to get jumped right into the play. So I think this is a really cool idea, that it's basically, like, a socially driven music. But actually, based on what building you're in, you can basically listen collaboratively to what everyone else is listening to. So it's sort of like a cultural kind of, like, self-assembly. But it's kind of an interesting idea. Anyway, I like this. one a lot and so I'm going to give them the built video prototype award where is this team? Raise your hand. Only one of you is here. Naughty. Well you'll have to give this to your teammates. I expect you to wear them on campus today. Okay, moving on. So what I want to resume on is actually do the second affinity diagram exercise that we didn't get to do. A bit more of a sort of a real scenario not just all the things that you touch in one given day. So this time of course your groups will be different so don't worry about that. I'll hand back out post-it notes. So I have all the post-it notes still here. What I wanted to do is imagine like the like downtown district partnership is trying to come up with how do we get like more senior students to come downtown more often, okay? And this could be a real thing. They really do want senior students to come downtown more often but there's various reasons why you probably do a lot less than you should. So what I wanted to do is again individually we're going to do this individually before meeting in your groups. I'll give you three to five minutes just to get down as many ideas as you can. We have a lot of experts to consider as part of this panel. Just put them in your neighborhood if you can. So again, good affinity diagramming practice is to try to boil it down to a single thought. Don't just be like, weather, you know, that's too loose. B, it's too cold. Rain would be a different story, right? So try to make it as small and actionable as possible. Who needs more positiveness? And this is you personally. Don't try to extract what others are doing. Just think about what you personally don't agree on. ಠಲಜಲರನಿಪತತಲನಿಕತನಮನಿ ಪಿಢನದರನಿರನಿರನವದರನ ಪಲಗತದನಾಜಲದದವದರನಿ ಪಢಲಾಕರಡಲನಿರನಿಭಏತಥದದದದ ಮಭಡಫಕಲಪಾಟನಿಠರಾಂಗಪಟಾರಲಾನಯಿರನದದರನಾಜಲದರನಾಗಧರನದದಗರ ಪಶಿಯಂಗರರನಗರರಲನಾಜಯಂಗರರಲದರನದ� ಇಪಡಿ ಸಬಗತಾನದರನವರನಾಜಲತಾಂ ಪರಬಹದರನಾಗತ ಇಪಡಿ ಸಬಗತಾನದರನಾಗತ See if you can do a better job of clustering it, and again, we'll go around the room and see how, this is a more complicated problem than what you touch every day, so there'll be different headings for different people. So roughly, eight people. And again, if you have the same answer, you stack them, so if you have the same note, just put them on top of each other. Okay, and we're at 8,000 now. Okay, so Esther's doing really good. Do you have anything you wanna add, Esther? Okay, cool. Hey, maybe one more minute to get your ideas up on the board. Okay. Okay, let's go around the room and we'll see if the teams that are still wrapping up can wrap up. Okay, let's hear what the different groups are doing. It's not vital that you get everything done. Let's start. Tell us all of your categories from left to right. We have some problems here that we can see. I'm nervous. I don't have time to go in. Where were you? Yeah, we also have a lot of problems. Yeah, a lot of problems. I didn't tell you whether there's going to be any issues. No. It's a very vital thing that we're going to have. Not a good reason. Which is, you have, okay. Zoom. Okay. Next one. Alright, so we have weather. Should I just do it all? Yeah, just do it all. I'll clarify. Okay. Weather. So we've got weather. Yeah. Let's just get there. Inconvenience, schedule for safety, especially at night, cost of care, alternatives to going there, and activities that either don't have to be there over the past year. Personal problems, it's just not having a presence there. Yeah, it seems like a common theme. Okay, got it. Next team. Okay. I think, Sam, I think that I would like to support everyone on downtown and the fun things you can do there. But not important things to do is the number one thing. Yeah. As well as, like, distance, and then it comes from that. Like, you have, you don't want to worry about planning for it. There's other things. It's available to you much closer. And then the price, whether it's 60 cents a day. Okay. Okay. So definitely some common themes. Next team. Yeah, we have some pretty similar stuff. Like, not knowing what to do. One of our issues we have here today is we can't drink there. We don't drink wine. We weather CMUs, like, we're in a bubble that we're trapped in. The lagginess of the port authority sometimes. Yeah. My friend's stuff. And we also have noise stuff. Wow, see, the two noise issue ones. It's not a good thing. I've never heard that. I've never thought about how to prove it. Next. Yeah. So, yeah, there are two. Yeah. We work so hard for it. There's a lot of information about what you can have. You can have money. So there's a lot of devices you can have. I think we have a lot of questions about the weather and the ships that we are going to use. Okay. Next team. So we have 20, that's a pretty powerful amount of money on this. We saw our parking, transportation, weather. Okay. Okay. Interesting. Okay. Finally. So we have weather, transportation, lots of crowds, lack of activities, costs, safety, and we also have cell reception. Yeah. Now would that maybe go under safety? Like do we already communicate? Oh yeah. Friends? I don't know. That's a little bit of an oddball one. But yeah. Okay. Next team. So we have a lot of questions about the weather and the ships that we are going to use. Okay. Next team. So we have weather, transportation, lack of activities, costs, safety, and we also have cell reception. Okay. Next team. So we have weather, transportation, lack of activities, costs, safety, and we also have cell reception. Okay. Next team. close it down early, the restaurants are empty by 8. You can't even get food after 8 there. Yeah, we can talk about that. I mean, if you like Walnut Street, then you'll fall in love with them. Burn! Okay, sit back down. We're going to move on. Okay, so just to wrap up a little bit on observation and niche finding, there's two sort of effects that I wanted to briefly mention that can sometimes impair your ability to get a good contextual view. So number one is this notion of expert blindness. How many people have heard the term expert blindness before? It's a really important term. So basically this is this notion that you are so good at what you do that you basically can no longer see the pitfalls. This is sort of like the Snapchat interface effect. You're so good at using it and you're so adept at mobile applications that you can sort of figure it out. But that isn't expert blindness. If you think that's an easy interface to use, it's because you're suffering from expert blindness. So it's very dangerous. Masters who you are trying to learn from will exhibit expert blindness. So they'll skip over. It's like, oh, I'm just going to spool this to the printer. And they'll push like 15 buttons really fast and you'll have no idea what happened. And you may assume, oh, they just went to like file, print, and they hit return or something like that. But actually it might have been this really complicated system where they had to check some box and collate it and double-cite it. So you have to be very cognizant that the master is suffering from expert blindness. And you yourself as like sort of an expert in HCI or UX or computer science, whatever your discipline may be, that you're going to suffer from it in sort of an opposite way. So this is sort of doubly important that when you're in these activities, that you go through the process. Don't just let them demonstrate for you, because things that seem so trivial actually may actually be really complicated. And if you do it yourself, you'll see, oh, hold on, I have to select this like other weird printer, I have to print it in order to get it to like the claims department, just sort of like virtual school box or some really esoteric thing. You'll discover that if you make them do it. That's one of the reasons why it's a master-apprentice model, that you're meant to learn, you're meant to do it. You're going to do it in a very poor, slow way, but it forces you to have to basically kind of learn their job. Again, not just an interview. You want to have that kind of relationship. So tech deployments is a double. So here's this one example. I pulled this out of an old Microsoft video. This is them testing what would become Windows 95, but back in 93. And it's someone using a mouse for the first time. And you might think, oh, that's a really intuitive thing. But it's actually pretty difficult. So let's watch this video. And he's doing a think-aloud. Go. I'm using a double-click mouse. I'm going to icon. I'm coming down here. I'm going to get there. I'm going to be sure that it's on, though. It's on twice. I'm going to click this. I'm going to read down here. Let me read it again to see if I did something wrong. Double-click. Oh, I'm using a double-click. It took me to the left twice. I did that. Yeah, great. You did a good job. Your mouse pointer is jumping between the clicks, so that's why it's not working. Oh. Boy, that's too sensitive. and over here. Pause it there. Okay, so I'll just pause it there. Is, you know, we take for granted using a trackpad and a mouse. And this person's using it really for the first time. And what they notice in that, when they're testing this early, we're like a beta version of Windows 95, is that we have, right now, all of us have this mouse memory where if we double click with the mouse, we know to keep it still. But for someone who hasn't really developed that sort of physical intuition, he was clicking, and in between the click, he was just nudging it. You know, the click was causing the mouse to jump like one or two pixels, you know, a very small amount. And in response, Windows was rejecting it as a double click. It's like clicks have to happen in the same spot. It seems like such a tiny thing, but this person actually couldn't do it. I mean, the video, I'm sure, was longer, and it was probably, he was encountering this problem in many different spots in the interface. So again, it seems so obvious to us, and we can pretty much walk up to any computer with a mouse to track that, and we can use it immediately. And that's because you have tens of thousands of hours of practice on those things that you can basically walk up to like an airport kiosk for getting your ticket, and you can use it immediately, right? Because you're so good at it. And I'm sure you've been next to someone else online, or like the automated checkout at like Giant Eagle, and there's just someone who is just getting destroyed by that interface, and just cannot get to the finish line. And you might be like, oh, they're dumb, but they're not, they're just inexperienced. And you're an expert, and you just don't realize that you're an expert. Here's another interesting item. Sorry, this popped up so many times. I'll turn off the light, it's probably gonna be a popped page. So there's a really interesting little kind of micro-study that they ran. Can anyone, if you look closely at this, can you notice something unusual? So they gave this to radiologists, okay? They gave this to radiologists who are experts at looking at these things, and basically saying, oh, there's like a tumor here, or there's like, I don't know, like a lung cavity, and there's like, you know, some sort of like water in the lungs, or something like that. And I think this is a cross-section of someone's lung. And if you're in the front row, see what really where is it? What is that? It's a little gorilla inside someone's lungs. And what they found, if you stand in the back row, it's very solid, like you can sort of see it. And they showed this to radiologists, like, do you notice anything wrong about this human? It's a healthy human. So they're like, no, the CT scan looks fine. There's a freaking gorilla in there, which is clearly not like fun. We think that's the most obvious thing to talk about if you're an expert. But the fact that they're so tuned to pattern matching for actual medical things, that they're basically blind, cognitively blind, is something that is very remarkable. So for example, expert blindness, novices sometimes can see things more clearly than even experts in some cases. Okay, here's a totally different effect that some of you may be aware of. So if you know the answer to this, sort of mums the word, let other people sort of think it through. But it's another effect that comes up all the time when you're studying the context and the contextual background of problems. So the original, kind of the classic study of this actually goes back to the 40s, to World War II. And what they were looking at is the military was trying to figure out where they could put extra armor. They had a certain payload capacity on the design of bombers, and they could add on a certain amount of extra armor. And what they did is they said, well, let's do this in a data-driven way. Rather than just like, yes, where we should put the armor, let's do this in a data-driven way. And so when we look at lots of aircraft that are coming back from combat situations, we can look at, you know, where they're getting shot, right? And so for the military, look at the bomber, the recurrence of enemy territory, and they recorded where those planes had taken the most damage. And over and over again, they saw the same pattern, right? That bullets tended to accumulate, damage to the aircraft tended to accumulate along the wings, around the tail gunner, and down the center of the bomb. Okay, they saw this pattern, this is where it's accumulating the most damage. So, taking this into account, where would you put the armor if you were the military? Okay. Yeah, so back in the day, these big bombers had like little, kind of like little turrets. So if an enemy fighter was tailing the bomber, they'd have a way to shoot back at the person that was flying on. So he'd tail gun him in a little bubble at the back with his own machine gun. And there was normally a roof underneath and even one in the front on the left of the bigger bomb. Does anyone have a theory on this? Yes? Set that aside for now. Let's just say there's 300 pounds of free weight that we can just add armor onto. Yes? Yes, did everyone hear that? So, the answer is you put the armor where there are no bullet holes. Okay? If you think about this, the planes that are returning, so I have a nice succinct quote here. So here's an example of, for example, an aircraft. So this is not the bomber. So you see, you're getting planes like this returning. You say, oh my gosh, look at the whole wing that's blown off this fighter, little tiny fighter. Right? So you may say, well, where should we make it stronger? And you think, okay, so the first intuition is to put it on the wing, to strengthen up that part that fell off. But the fact is, if this plane lands back at the airfield, it shows you that you can actually lose that part of the wing and still make it back. If you have planes coming where there's no holes, that are the ones where, because it's missing from your data set, it means those are the ones that got shot there and probably just like exploded and they never came back. So the ones that are coming back with holes is where actually it shows you can be shot and survive. Okay? It's a little bit counterintuitive. So let me just say that more simply. The holes actually show where the plane is the strongest, because it can still make it home despite having damage. Okay? And so what you want to do is you want to put the armor everywhere you don't see holes, because that's presumably, assume it's a random distribution of holes, essentially. And so you put armor where you don't see holes. you see no damage because those are the ones that got shot and they come back. Does that make sense? How many people, does this make sense? Okay, good. So, the planes that weren't there are the ones that needed the extra protection, the holes, the sort of thing that you build locations in, these are the least additional ones. So this effect is called survivorship bias. You have to realize that you see survivorship bias in all different things. Because of just natural selection, not just in military aircraft, but in processes and fashion and everything, all you're seeing are the exemplars that survive. So, for example, not the bomb armor, but the notion that older buildings are more beautiful. Pittsburgh has an amazing collection of turn of the century and Gilded Age skyscrapers. But it's actually not true, like all these wonderful Victorians like in Shadyside and stuff. The fact is older buildings are not more beautiful, quantitatively. We have kept, it happened to be that the ones that we decided to preserve and we kept for like a hundred years happened to be the nice ones. That's why we kept them. All the really ugly old buildings, and there were many of them, we've torn them down because they just weren't worth keeping. Same with old cars. People were like, oh my gosh, like a Ford, Thunderbird, like cars back in the 60s were just so elegant and so beautiful and they're still driving. They have like a Jeep from the 50s, it's so reliable. But that's because you're not hearing about like all the cars that were so terrible that it's like trashed them, even in the 60s, they just tossed them out. And there's lots of ones like the Pinto that would just occasionally just explode and kill people, like for no reason. None of those are still driving on the road, even though it's a terrible design. So there's also the notion that they don't make stuff like they used to because someone has some really old tool that happened to be, you know, it's a dishwasher that is still surviving and you bought in the 80s and it still was working. That's because that's an example of something that happened to survive. And you don't have exemplars in the day. Here's some other ones and songs from the 80s. If you watch like Stranger Things, it's always like hit songs. Wow, what an amazing musical era. Because again, if you watch any show or movie set in the 80s, it's like the same collection of 50 really awesome songs from the 80s, and they're not playing all the ranked number 100 song from May of 1982. And if you've never heard of that song, it's not got any kind of nostalgia for you. It's just been essentially lost in time. And same, and I see this a lot. I have a startup myself, and everyone over here can have these crazy best practices. But you have to realize that startups, like the people that got super successful, sort of the Mark Zuckerbergs, for every Mark Zuckerberg, there's probably a thousand other people that had exactly the same idea and failed in different ways. It wasn't just like, he happens to think that his one path was probably like anyone could have done it if he just followed those steps. But there was actually things before Facebook, like Friendster, that were arguably better and run by more adept businessmen, and failed. So you have to take everything with a little bit of a grain of salt. So whenever you're observing people and processes, and even just artifacts, you have to realize that this effect is happening and magnifying certain things. Yep. But they survived for so long, people imagine being in a different company than them, no? Yes. It's correlated, and sometimes they survive just in pure coincidence, like certain movies or cult classics that have survived, or a certain car that, for whatever reason, just accidentally happened to be more stronger than some counterpart. So yes, there normally is a rationale, and I'm sure, Dr. Burton, not to take on poor duck, but I'm sure he wasn't terrible at his job, but I do think that you could probably find other people that were better, and still lost out. So there's lots of examples, like better products that basically were a huge failure, like the Alcoa and Starbreeze era, right? I think so. Like, I understand what's more in terms of pain, because like, oh yes, they survived, but you know that you have to defend them, like men in the home, that didn't. Right. But some, like Dr. Burton's stuff, I don't know what I would put somewhere in that. Or like old cars, too. Like, really, those cars are super great, and people love them, and they're not super safe, but I may like them for a reason, and other parts of the era, you know? Yeah, so. The main point I want to do, and obviously these are moving away from your tip, but I wanted to give some other touchstones that you probably have seen that have happened. But the key thing is that when you go to a context, like you go to an office and you observe processes, if sometimes you have to realize that those processes are not around necessarily because they're good, but there may be tangential factors that have kept them and that they have survived and other things were tried and kind of failed along the way. So it's just to be cognizant that there is going to be a bias in what you see. Even the people that are having to be in the office, there could be a sort of a survivorship bias among the employees, certain personality types or training. You're not hearing from all the employees that the software is so bad, only the people that love this terribly fun software. And so they all give you glowing feedback. It may be because all the people with a humanities degree couldn't figure it out and quit the company six months ago. You just have to be, there's no rules you must follow, but it's good to be cognizant that these sort of processes play out. And so when you do, you always want to question, you know, why is this still happening? Why is it this demographic of people? Is this some sort of, and I would talk about other effects that are happening like this throughout the semester. And this is one of them that's really strong. Here's just one final, this is like an ex-PCB example. This is sort of like the start of a day of election. Right. So never stop buying lottery tickets, no matter what anyone tells you. I failed again and again, but I never gave up. I took extra jobs and put all my money into tickets. And here I am, proof that if you put in the time, it pays off. Obviously, there's the 10,000 other people that followed exactly the same formula. Maybe put more money in the lottery tickets. And I think that's the next step. OK, so that's just two quick things on niche finding, sort of possible niche finding pitfalls. And then I want to jump back to usability. We talked a lot about, back just like two lectures ago, we talked about how you do like the funness of an arcade, how you do like the memorability and the gesture. And I want to give you one case study that actually sort of. to how you measure some of these very qualitative effects. So this is where we left off at the end of that lecture. And I wanted to show you this case study when they were designing Halo 3. So this is a great article, and you can read it. But we care about how Microsoft Labs invented this new science of play. And they were trying to figure out kind of quantitative ways to make Halo 3 more fun. So for example, I just pulled out some choice quotes out of this Y article. So after each session, the experimenter analyzes data as a pattern. For example, for these snapshots of where players are locating the game at various points in time, five minutes in, one hour in, eight hours in, et cetera. They show how they're advancing. If they're going too fast, the game would be too easy. And if it's too slow, it might be too hard. And this is just one example of how they did this. This is actually, I think, one minute interval. So here's one level. And what they've done is they've color-coded where all the players who play something. So Microsoft does a huge amount of user testing where they bring people in to play, basically, data versions of these games. And basically, they're able to say, OK, where was that player at one minute, two minutes, three minutes, four minutes, and it's color-coded. As you can see, some people, you see some different colors in different zones. Obviously, there's some orange in here. There's some clear orange in here. And then when it's blue, there's light blue, dark blue. There's some people that move that here very quickly. So this might be minute one. And this is minute three. And some people were able to get here within a minute. And some people here within three, let's say. And you can see, again, some people are a little bit faster than others. Some yellow things in here, and so on. And so they're doing these sort of tests to look at pace of the game. And you can see it's actually relatively even. Most players, it doesn't matter if you're skillful or not skillful, you see that it tends to flow for even. What you don't want to see, it was really complicated. And people that have played Halo 1 and 2, they're just flying through this. And then you find people that have never played Halo before. And they were back here for 20 minutes. And obviously, it's not going to be that exciting. They're just walking literally around in circles. And that's a clue that actually there's going to be a stratification of expertise. Another interesting one here, so the designer can generate a map showing where people are dying to identify the topographical features that may be making a battle onerous. And they can produce these charts and how they can do that. This is a heat map of where players are dying in this particular well. So again, they can see, like, why is it this one spot? Again, they're not necessarily videotaping. They are probably videotaping all these players, but they're not necessarily observing every single one of them. They're not bringing in 100 people a day to play through the entire game. And obviously they're walking around, but they're not recording exactly what everything happens, but they can record it digitally, and then basically make these sort of infographics that do these sort of analyses. And obviously, you want to have different points Maybe extra-difficult bosses or something like that. Then you can basically look at these patterns to see if people are, like, there's not even any enemies there. Why is there so many deaths? Maybe they're, like, falling down a ravine, or maybe there's some sort of a glitch in the level where you, like, drop through the map. I'm sure people have played games where there's little errors like that. So here's another quote. So a similar report shows that in the game's very first level, players often ran out of ammunition for their rifles. This was a mystery because the designers were careful to leave more than enough ammunition laying around. They even checked the video record, and they found that people were firing at the aliens, the enemies, when they were too far away, misjudging the range of the weapon and wasting bullets. So this is, you know, they're in the first level, and they've never played anything from the Halo series before, and they see one of these kind of creatures in the distance, and so they just start firing at it. But because it's so far away, basically, this doesn't even register as a hit. In fact, the game may actually discard the bullet. It may be programmatically impossible to even cause any damage to it. So first, the designers couldn't figure out how to fix this problem, so they stumbled on this elegant hack. They made this targeting reticule. This is like a little crosshair that you see in a lot of games, and also on guns, like actual guns have a reticule as well. It turns red when the enemies are in range. So it's like, I think it's green or white normally, but then when you go over someone, go over one of these enemies, it changes color. I'm sure people have seen this effect a bit today, like a first-person shooter. And so there's this little, tiny, subtle cue that basically suggests that they are within range and actually firing at this enemy was going to be productive. And so then, you know, they have these kind of percentages like this, so lastly, 52% of the players gave, you know, the jungle level a 5 out of 5 for fun. And then in the beginning, when it wasn't making a lot of sense, it was only 10% of players gave it a 5 out of 5. So it's a way, again, you can read the whole article, but there's lots of little tiny things that they're trying to bring quantitative measures to bear to make these levels. Which again, they're measuring fun. We talked about fun, that's how these are paid. But they're trying to really get billions of dollars at stake for something like the Halo franchise. They want to make sure that that's a real success. They want to develop all these techniques and tools to be able to analyze fun. It's actually quite interesting. So, just to reiterate, you want to identify the right goals and you're trying to figure out usability goals. What is the point of Halo? Is it fun? Is it for them to buy more Halo products? You have to figure out what your metrics are so that you know that you're designing your system right. Then you want to devise a drive way to ensure that your goals are met and what you have your goals are. What are the metrics and how are you going to work towards those? And finally, there's a lot of iteration that we've talked about many times in the semester and hopefully what you're experiencing in Vega 1 is you need to go through these iterative loops to make these improvements. Okay, that's it on the lecture side. So you have 20 minutes left in class. I would encourage you to meet in your groups because I know a couple of groups are having a hard time finding time to meet. If you are super, so super confident in your idea, you are welcome to leave. I'm not going to keep you here. And I hope you use this time to ask me final remaining questions. On your bake off. Otherwise, I'll see you on Tuesday. Thank you. I'm not sure how important it is, but I think it's important to do what you will, and make the best decisions you can about what you want to do. You can't do this all by yourself. You have to have a specific idea. And it has to be equaled out by everyone. So it isn't 100% everything is decided by everyone. You can't just make a decision. There are plenty of those. Just really quickly, I want to make a quick announcement. Just to remind you, I mentioned this before, but just to remind you, when participants sit down at your table, to the completion point, and you can talk to them, you can touch the trackpad, so you have, you can give them, basically, all these questions they need to figure out your design. And the second time you run it, it has to meet the time trial. So just a reminder. I hope I answered all your questions. I hope I answered all your questions. I hope I answered all your questions. I hope I answered all your questions. I hope I answered all your questions. So like, there's not, it's not even addressed here in the box, I mean, the box is kind of functional. As long as, you know, as long as you've done the work, that was all good. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yes. Yes. Yes. Yes. We may want to look at, there's a rectangle node that places it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center, or you can place it in the center. Can you make a straight line from the cursor to the target box down here? It doesn't matter. Just go straight to the other box on the point 4 and the other box on the right. But it's just that it doesn't make it like a heart. It's just a box on the right. You can decide to make just one right here. It's fine. That's fine. You don't have to worry about it. That's fine. That's a good point. Then, as I gave you last year's lecture, I believe that the core gives you the assignment of the script, like, before you start running it. So, obviously, I'm giving you all the boxes to go. I'd like to do a little bit more here. Okay. And I'll actually give you a list of all the scripts you can send. Yes. That's what the scaffold library does. We download that from my website. We go to the top left. I don't have that anymore. It's commented. It's commented in the preset you want. Oh, okay. We can look at that. So, any other ideas? It sounds like you need a lot more ideas. We need the line idea. That's going to give you a point one for that. We have some ideas. We already have it. I'm going to just start another one. Okay. Thank you. Thank you. Okay. All right. Yes, goodbye. Thank you. Yes. Thank you. So I think, since it won't be hard for you guys to come to the press conference, I think the idea is for you to basically call yourself. Um... I've got one more question. I have one more question. So, send me an email after the press conference. I will have a discussion with you guys. I will get to you by midnight Friday. Next Friday. Um, and then you can... you can... don't have to send it to me. This is my email. That's why I saw your email. Thank you. Do you guys want to ask me? No, we were just asking. You look like you're from the business side. So, joining online for the conference. Yeah. So that's the process there. That's all. That's all. Um, do you want to show it first? I'm sure we have a... we have a show. No. Actually, we could always do it. No, we won't. No, we're fine. We can do it. We can do it. It helps to make it. Yeah. You can actually test it. Side by side. I felt that I was iterating while I was speaking. It's a good exercise. Yeah. I think we're about done. We're going to go. We're going to go. Thanks. All right. All right. Thanks. All right. All right. Bye. Bye. Bye. See you. See you. Bye. Bye. Bye. Bye. Thank you. Bye. Bye. Bye. Bye. Bye. Bye. Bye. What are you projecting? Oh, it's really close? I think we need to... Yeah, I think we need to... Where? Try talking to it. It's weird. It's not even the one that's closer. It's like, you're here. It's kind of weird. It seems fair. It just, like, sees the mouth direction. Oh, really? I think it's just, like, closest. Closest to... No, it's not, like, it's like... Yeah, it looks legal. Thank you very much. Do you have a video? Yes. Uh, we have a video. Why? Those look darker than they should be. Oh, so these can't see? Okay. So, the idea here is, like, every grid has, like, a... Yeah. So, we call it a change here. Yeah, so, like, you can, like... Yeah. Okay. Yeah, yeah. That's legal. It's already a good idea. Yeah, yeah. Okay. There's some big ones still that we haven't discussed. One of the big ones, but there's still more. Okay. I'm representing a... Yeah. So, I think the initial idea is we have our one center of legality. So, uh, we have the crazy little big bar in the middle. Uh, and maybe also the last jumping from the center of one square to the center of the other with, like, a little jump. As long as it's still open. Yeah. Yeah. ... ... ... ... like, just changing it to things that are a little bit more eye-catching, like yellow for the target, and then... So you can decorate just the target, but you can modify it back and forth, which would be great. You can make it, like, more yellow or something. And just, maybe, would it be possible to brighten whichever square the mouse is pointing to? Sure, that's fair. It doesn't, like, really matter. It's like, all of those things would treat me differently. I think that should be a valid reason. Okay. I, it's just... It's been an interesting team to work with, but... Yeah, it should, it's fine. Okay. But, yeah. Yeah, it's fine. We can talk about that. Mike's here. Yes. Okay. No. I cannot make a model... Yes. Okay. Mm-hmm. Oh. I mean, you should look at the original code. Definitely. Just double-check, just look at the scaffold code, just make sure that... You know, we don't want to lose the place. It's like, you know, it's in, like, 128. And... Double-check. Mm-hmm. And you do that so that it's there on the students, or because it's, I mean, you could, but think about how this would work in practice. Like, you wouldn't have your, you know, if the computer would guess that you're going to go to Chrome. But it'd be very weird if all your icons on the screen, like, went to grass, and like one went like that, and it's kind of strange. So it's, it's, it's fun about, it's not because it's, like, illegal in the world, but just because I'm trying to keep a level of fairness. No one can modify the background. No one can modify the map. I am letting people play around with the cardboard square, just a little bit of the design of the windows. The rules are, are just made up by me. You know, so there's no, like, car gets legal and it takes the eye. Or, I thought it's like, I mean, I'm just, like, like, kind of cool that you're writing them, but I, I thought that I could apply it, like, in your study, if you don't mind. Right. And this is sort of a new study that I'm just trying to constrain. And, and so that's just one of my kind of lines of thought. Because I had, in previous times, I didn't let the people do crazy things, I made them all invisible. And it was, it was just getting crazy. And so I was like, what's this supposed to mean? Trying to, trying. It's not 100% realistic, but it's still amazing. Thank you. Thank you. Thank you. So typically, when they make mistakes, it's a pretty impressive liability. Is it not? Yeah. Well, we do get it sometimes, and then they're like, if you can't have it at all, you can't do it. I just gave a hard time. So now, how do you approach it? So for example, if like, I'm a customer, right? I'm a sponsor. And so I basically just pay that high price. That's no issue, right? I think it does cost a lot of money. Like, this is a way for that. It costs a lot of money. I don't know what I'm going to say today, but it costs a lot of money. Why is it so expensive? For example, if you charge a lot of money for this, it costs a lot of money. Because we get the whole country. Yeah. And then, when we're actually in the box, it is going to cost a lot of money. I think it's important to understand that if you, if this is the target, then it's going to cost a lot of money. I'm curious why you think that. I don't know. I'm not a big fan of it. Yeah, it's important to know. And so, let's find out. So if it's targeted, then you add, like, your consultation, like, your client, and then, I don't think that's the right way to do it. I don't know. Oh, I don't know. If it's targeted, then you're, like, like, somebody's going to find it out. Do you want to do the same one? Yeah. Oh, yeah. I'll give it to you. Oh, perfect. So I can find you. Somebody found this already. I don't know how to do it. You can move your mouse over. Move your mouse over. I think that's the only way I can do it. I can do it. That was very good. I can do it. I can do it. I can do it. I can do it. You want to do it? I can do it. Yeah. I'm sorry. It doesn't matter. Then you can. I think you can do it. You can change the color. Yeah. That's one thing I did. The other thing is, I can't do anything. I can't do anything. I can't do it. I can't do it. I can't do it.\",\n",
       " \"I mean, you can do a proof for this, right? I mean, let's say I'm calling the consumer, and my bank is holding an X-file on the left side of the line. So, my bank is holding an X-file on the right side of the line. So, I need to find, in fact, a point where X-file is in the middle. So, I have to find X-file. So, Y-0, x-y, x-y-0, I need to find X-file there. I need to find X-file there. So, and the other concept that I'm looking for is, X-file has to lie on a circle. I need to find X-file. I assume Y-0. Is it the same? Oh, I thought it was the same. Thanks for doing this. It's been a pleasure. My pleasure. You have to, you know, I guess, just a picture, I mean, you need to be able to touch your X-file and you can rewind this to find it, and that's all you need to do. Can you tell what we're talking about today? I'm sorry. I can't hear you. Thank you. Okay, so let's start with a discussion of the trade-off. As usual, I'm always curious what people notice in terms of design trends correlating with performance. Especially the participants, again, always have probably the best insights, but I'm sure people running experiments also do. Yeah, I definitely noticed that QWERTY, in general, even though it's an unusual arrangement of letters, it did seem that the teams that used QWERTY, you know, it's kind of skewed higher. I didn't crunch the numbers, but probably a couple words per minute, you know, benefit. That's something that's more exotic. What else do people notice? When I was a participant, I was caffeinated, which meant that I was rather jittery and had issues pushing buttons for QWERTY keywords because they were so small and I always typed the wrong ones. Okay? Well, it is a very small space, which is why we don't really see text entry on smartwatches very often. But yes, that's definitely a problem. Or people that have, not even just caffeinated, but people that have reduced motor skill when they get older, if they have tremors or Parkinson's or something like that, it's going to be an issue. And obviously there's an entire branch of human-computer interaction that is dedicated to accessibility research and we even have some of the world's best faculty in it here at CMU like Jeff Bingham. What else do people notice? A lot of the designs seem to be playing on familiarity, going with things close to keyboards that we all already are well trained in using in our daily lives. Yeah. And that can just be helpful. Helpful, especially given the limited amount of training. Right. Yeah. Yeah? How do you see a really little change made a big difference? So there's a couple groups that had, after you tapped, you essentially received a notification by the color changing. Mm-hmm. And it's such a small thing, but it made it so much faster. You didn't have to wait for the letter to show up. Mm-hmm. And you could just, like, do what you wanted. Right. There was a team that just, I think when you hit the key, the whole key turned, like, green. Yeah. You know, and another team, I think, had one where, like, the letter would turn orange. I think it's actually even the team that made it to the farm had this, like, it turned orange. But the letter's underneath your finger. Right. So you don't see that feedback in the same way as the whole sort of square lighting up in green. Yep. Definitely saw a good notification. I think I, in every make-up, I always, if I ever give feedback to teams, it's like, better feedback, better confirmation. Turn something green. Make it a little sound. In this case, you could have vibrated the phone. If you wanted to do a key press, it's a way to access that API on the phone. Definitely helps people want that confirmation so they can start moving to the next letter rather than having to. So you think about it sort of like that kind of GOMS model. Like, every operation is like 100 milliseconds, 200 milliseconds, you know, and move your eye here. Make sure it's the letter that you type. Then move your eyes back down. You think about all this linear accumulation of steps. You can sometimes do some parallelization. But in general, more steps is going to be longer time. What else did people notice? Yeah. Yeah, so it definitely let you be a lot sloppier. In this case, a lot of. you were holding the phone really close and you're sort of using your phone like this. I guess a lot of people hunched over the various bake-off designs. But obviously it's much harder when you're on your own wrist. I mean, you could do it like this, I suppose. And if you can imagine while you're walking, it's going to be so much harder to do stable input. And so if you have a little bit of sloppiness involved, and we'll talk about that in today's lecture, it does tremendously improve performance. Of course, one of the teams that was in the final final, you know, the ultimate finals, was just a straight up QWERTY keyboard with just a whole QWERTY keyboard on the screen with no auto-correction at all, as far as I could tell. And even still, it's pretty accurate, which is amazing. Is that the team that won? I forget that there's the team that won. So was it just like, were you guys just the QWERTY keyboard or no? What else do people notice? Any other high-level trends? Did anyone see a design that they'd actually want on, how many people have a smartwatch, number one? Raise your hand if you wear a smartwatch. Okay. Would you want any of those keyboards? Tell me about, tell me about, well, it's only a two-week assignment, so give it a look. So actually, tell us about, I'm actually curious, I don't wear a smartwatch myself, just by researching it, is, what's the keyboard right now? What do you use, if you're just having a Wi-Fi password or something, what does it do? I think there's like a, it's like a drawing pad, so you can like, draw a letter, and then there's like, standard answers, so it's really good. Yeah, this is Apple, right? Apple Watch? Yeah. Does anyone else have an Android one? Yeah? So how do they do text entry? How do they do text entry? So I would say both Apple and Android solutions, both of those are pretty basic. I feel like people essentially build keyboards as sophisticated as that in like two weeks. And like with the thousands of engineers at Apple and Google, like that's the best they can do. It's a pretty sad state of affairs actually. So at a high level, someone else had a comment over here, do you have a comment, do you have a comment? I'm going to say that a lot of people have a comment, but it's such a wide variety of things, like Google and Android. Yeah. Yeah, so there's quite a variance among the participants. Yes. I have a theory that if you had done this with a group like ten years ago, the old school tech would be great, but it's a lot better than it is. Do you know what you're saying? Probably, yeah. I mean people used to be really good at it. And now that like era has gone, you know, I saw people that were just unbelievable at triple typing. It's like faster than like a text entry. Yeah, that was you. Okay. Yeah. Yeah, right. Exactly. So here are my thoughts. And actually I think they mirror most of yours. So number one, as usual, I would like to emphasize that, you know, the design makes a big difference here. So the lowest mean was 5.4 and the best mean was 15.9. So actually, you guys have been very consistent all semester between the best and worst teams. There's only got a 3x differential, which is pretty, you know, that's substantial. I could get my emails done three times faster. I'm the best at a max single trial was 36 words per minute, which is pretty obscene. That's faster than some people can actually type on a full size physical keyboard. But that was a one time trial on their own design. As I noted in class, you know, three out of the six teams made it to the finals with any sort of. auto-correction or typing, and I think one out of the three that made it to the Super Final had no auto-correction. QWERTY keyboard worked really well. If you tried to innovate a little bit too much beyond that, it actually hurt your performance. But the more exotic you made it, the crazier it got. And one of the teams that made it to the Final Finals was literally just a QWERTY keyboard on the screen with nothing else. And it shows you that it's the same power of QWERTY. And I think it's because really it's simple can sometimes be better. If you just think about it in sort of what I was saying, you know, kind of model human processor or physical perspective, the more kind of little mini operations. Now you click into the button, then you swipe to the left, and then you double tap this. And the more things you're layering onto the interaction, the more, you know, kind of the slower it becomes overall. So kind of more clicks, more taps does seem to be a work. So if you can simplify it down to the single stroke or single tap, it tends to have a better performance. So two questions is for you guys is if I want to punish a future DATS class, how much smaller do you think I should go on the screen? Or I can go bigger. How many people think I should go bigger? How many people think I should go? It's the right size. Just right. One inch. Okay. How many people think I should go smaller? Okay. You guys aren't very exciting, but we'll keep it at roughly the same size. Now I think, I don't know what the latest Apple watches are. I think the new watches are actually larger than an inch, I believe. But I think what I would say is that no one wants a gigantic smartwatch, right? Like if you ask people what they want, they probably say they want a smaller smartwatch. Same, I think, with phones. Like no one really is saying, like, I want a gigantic phone in my hands. Like you don't ever get that on their feedback form. But what people want to do is do more cool stuff on their phone. So this evolution where like larger screens on smartwatches, larger screens on phones is because people want to watch big movies, you know, have an email, a big email thread open, blah, blah, blah. But no one really wants the big screen size. They sort of want the big screen size, but not the big screen. That's why we're getting things like foldables now. It's a pretty interesting way to do that. Then finally, in terms of the three bake-offs you've done so far, if you've had a favorite of the three, we'll take a vote. To how many people their favorite so far has been bake-off one? Raise your hand. Bake-off two? Raise your hand. Bake-off three? Okay. Bake-off four? Okay. It's pretty even. Good. I mean, I don't want there to be one really fun one and one that's not very intellectually intriguing in their own way. So as you probably saw, this is actually wrong. I moved it to Friday. So peer review three I released just before classes ended, even though it's also posted to Canvas. That's due on Friday midnight. Should be enough time. Hopefully it only takes you five, ten minutes to submit these things anyway. And really, you should be in sort of full launch mode on bake-off four. So you have two weeks, pretty much exactly at this point, to go all in and really polish it up nicely. Some things I've neglected to mention about bake-off four that I should make some clarifications on is that phase one, so you have two kind of phases. You have the choose four phase and the choose two phase. They have to use different modalities, input modalities. So in my scaffold code, I have the first one being a rotation. So you have to kind of twist around in circles to select. And then the second mode is an up and down acceleration. And so those are two different styles of input. One's rotation, and one's like swiping up and down in the air. So that's legal. And I also use the proximity sensor just to have like another layer. There's no obligation to use the proximity sensor. There's no obligation to select anything either. You could have it that the first target you touch auto selects, right? So you can think about your design. But importantly for the rules is that you have to use basically two different sensors. Or if you do use the same sensor, you have to use it in two very different ways for the two different phases. So it can't be rotation and rotation. or accelerometer, accelerometer. You have to think about, and this is arbitrary, it's not, but this is good interface design, but I want you to think more creatively and want it to make it a bit more of a fun, expressive bake-off. And mostly just to show you that using sensors in these phones is actually really easy. What else did I want to teach you? You can't use voice, so no voice in Bliquid. You can use sound. Obviously, there's no way to bias towards the targets. Other than you can highlight them, draw them in a special color, but obviously you can't make them easier to click or anything. So in that regard, and you can't show the stage two before you show the stage, until they get through stage one. So you have to choose four, and then you show them one of choose two. So it's like you go into Netflix and then play whatever most recent show or something like that, okay? And the other thing that's unique about this bake-off is there's no error penalty. This is the only bake-off we're gonna have no error penalty, and that's because in order to get to the next trial, you have to perform it correctly, okay? It used to be, when I did this the very first time, is that I don't, like kind of my error checking code would only let them go, would only let teams go through if they got it correct. But people started doing crazy things. There was one bake-off that was an absolute two, and that's because teams would have like a rotation, let's say, and what they'd do is they'd just be checking to see if it was correct, but they'd just be like people spinning in circles, right? Until it was the right value, or they'd just be shaking the phone up, down, up, down, up, down, up, down, until they got to some end, and it was like a total madhouse in there. So now, I've built in a penalty that if you get the trial wrong, you actually move back one trial. So you actually get a penalty. You don't go back to the beginning of the trial, you actually go back one whole trial. So that means in order to get to that final screen, you actually have to complete the whole sequence correctly. So just think of it as like some sort of really long gesture sequence, okay? So when you get to the end, you have to have done everything correct, there's no penalty. It is however long it took you divided by the total number of trials, and you should definitely try to run the scaffold code to make sure that you can get it working. So any general questions on Bake Out 4? Hopefully we've got lots of time. Oh, the other thing I should mention is, you should probably just like delete my design entirely. There's no obligation for like four circles or red squares, like you can go to town on the visual design. In fact, you probably want to almost start us from scratch. This is the most open-ended design kind of of any of the BAGOGs. You just have to find a way to choose one of four things and then one of two things. As long as you follow the scaffold code with respect to sort of honoring the errors and so on like that, you're fine. So you can go, the sky's the limit. Yeah? And if during our... The big box is during our final. Is that for four hours? We'll probably do it for just the first two hours, hour and a half, because you guys will get to escape early. May 13th. This room. Okay. Any last questions before we move on? Okay. So start, start emailing me, now you have a weekend to recover, start emailing me ideas. Okay. So today we're going to talk about typing and text entry, continuing sort of our discussion on input and output technologies. We've just basically ran a big multi-faceted experiment. So can anyone remind me, what are the important metrics, sort of like, you know, in Fitts' Law it was size and distance, but in the text entry experiment, what do you think are the important things to measure? How do we measure performance in a text entry experiment? Yeah. So words per minute is one. There's another metric that's important too, is words per minute. Yeah? Accuracy. Accuracy or error, right? Because you can be at a thousand words per minute, but if it's 1% accuracy, it's not going to be very useful. So it's speed and error. So we're going to talk a little bit about speed and error. We're going to talk a little bit about speed and error. We're going to talk a little bit about accuracy. We're going to talk a little bit about accuracy. We're going to talk a little bit about speed and error. We're going to talk a little bit about speed and error. We're going to talk a little bit about speed and error. Because you can be a thousand words per minute, but if it's 1% accuracy, it's not going to be very useful. So it's speed and error. Okay? Now there's a couple of different ways to do this. Words per minute is the most common, and the field has to be...used to be variable. The field is standardized that actually it's not really words per minute, it's how long it takes to get five characters. So five is about the typical length of a word in English language, including spaces. and punctuation, like if you run through any kind of modern text. It's about five. And so you don't actually calculate the words, you actually just divide by five for the string length. Errors are normally done by this mean string distance. We use the Levenstein distance, and just for you who didn't actually look at that code, what it does, it's much more fair. So if I type in, for example, I don't know what I'm typing here. I'll type in, let's say, so here's Harrison, right, but I accidentally type in H-A, or someone misspelled my name, with a single R, the naive approach would just be the algorithm that just compares, like this, right? And so this would say it's correct, it's correct, it's correct, it's wrong, wrong, wrong, wrong, right? So it'd be an error of four. But that's not really a good evaluation of that string distance, because there's really only one error here. If they had just inserted an R, it'd be 100% correct. So what Levenstein distance does, what the algorithm does, and what mean string distance does, is it is the distance including deletions, replacements, and swaps, right? So this actually has a distance of one, because if I just insert one R, and now if I'm correct, if I accidentally get to R-R, I get like R-J, that's also just an error of one. By the swap of J to R, it'd be a distance of one. So mean string distance is pretty much the standard way to compute the error between two strings, and you saw that the job implementation that I put into the bake-off is only like 10 lines long. And then finally, and this is what I was actually going to make you do for your homework, but because we got a little bit crunched on time, I've dropped your last homework assignment in which you did the bake-off, is I want to make you do your keystrokes for character, basically sort of like how we did a Fitts' law, and maybe you can compute the Fitts' law coefficient, like the index of performance for your bake-off one code, I was going to make you do the keystrokes for character for this bake-off, and what you do is you take the, basically you count all of the presses, all of the interactions that the user has. to do, whether that's clicking on a button or swiping, is you calculate the total number of those, including the average time of backspace. So if you're an errorful technique and you have to delete a lot, that gets encountered, too. Divided by the final number of characters in the string, divided by all the things you have to do in it. So if you have a 10-character word, but it took you 23 things to do it, or let's say 20 things to do it, so swipes, taps, deletes, whatever it may be. Now it would be a keystroke per character of 2, because it took you 2 keystrokes to type in one character. And what they found, as I mentioned a couple slides ago, is sort of, in general, the more things you have to do to get to typing a character, the worse it is. So on a keyboard like my keyboard, it's pretty much a keystroke per character of 1. I type the G key, and I get a G. I type the X key, I get an X. So one key press to one character, pretty straightforward. For any of you that had a swipe between the team that had one that had to swipe from left to right, sometimes you'd have runs where you're all in the same half. But every now and again, you'd have to swap over to the other side of the keyboard. Now the worst case scenario is you'd have to swap every single time. You'd have to click, swipe, click, swipe. That'd be a keystroke per character of 2. But because you end up with little runs on the same side, they probably have a keystroke per character of 1.3 or 1.5 or somewhere between 1 and 2. And any of your designs that involve clicking, and then clicking, you click on the A, B, C key, and then you type in B. So it took two key presses. That'd be a keystroke per character of 2. So basically, it gives you just a very high-level metric of how errorful your design is and how many kind of interactions you're having to do in order to get the sentences done. It tends to correlate extremely well with performance. And if you had autocorrect or autocomplete, so I mean, again, the one example that I saw was someone typed in I-N-S-U, and it brought up the word insulation, which was the one that it wanted. So they only typed in four characters for like a nine-character word. So when they hit insulation, that's the fifth key press. They got nine characters for free, which actually means that they were efficient in their auto-suggestions. You can actually have a keystroke per character that's below, what, like 0.7 or 0.5. So you're getting two characters for every key press, and that'd be pretty damn impressive, right? And that's sort of the holy grail, is to push this down as far as you can. So again, edit distance, we already talked about this. It's just the minimum number of single-character edits, that's insertions, deletions, or substitutions, to change levels. The time to do that is the time that you have to basically meet your requirements. So what I want to show you today is that there's an incredible— people have been thinking about text entry for a really, really, really long time, okay? Like, it goes back a long time ago. I've even brought a device that's 100 years old to show you a particular effect. And so we're going to talk about some of those sort of design methods. But before we go into it, as we're getting towards the end of the semester, I wanted to slip in one more sort of comedic video here that talks about Google's efforts in text entry. Watch this video. So I've always known that I was related to Samuel F. B. Morse. He's my great-grandfather's grandfather's brother. I read Morse first came to my office and said he had an idea for bringing Morse code back. I got really excited. Gmail taps replaces the default keyboard in the Gmail application with one that only has two buttons. And every single letter in the alphabet can be spelled using just those two commands. Morse code is perfect. It's just a dot and a dash. What's simpler than that? When we first conceived of Gmail Tap, it was all about making input on the phone faster. We realized that we could double the speed again by doubling the number of people in the house. You can actually split your screen up. As I said, two messages at once is two completely different messages. Gmail Tap multitasking is an improvement over speech. You can say two things with your fingers, but your mouth can only say one of them. I'm really excited to see where tapping takes us. I think the people who will be best served by Gmail Tap are people with fat fingers. People are going to be twice as productive. People are going to be able to write emails any time they want. Tap it in the morning. Tap it at night. Tap it at home. It's a dad and a dad to have a conversation with the entire world. It's working. Again, everyone has experienced the pain of mobile text entry. Humor is always a great method to see if something has gone mainstream enough. Even if your parents would snicker at this video, you'd know it's something that we've all faced. Let's go back to the very origins of typing and text entry. Handwriting goes way back, 1,300 B.C.E. People, obviously, are kind of original people, kind of pressed into tablets, but typical handwriting is around 22 to 31 words per minute. That's like a typical writing speed. Most of our designs, other than the peak performance, were really worse than just handwriting alone. If you can handwrite, that's going to be better than texting away on your smartwatch with little tiny buttons. But a big problem with handwriting, and this is something that really frustrated people for a long time, is that there's a tremendous amount of variation in the script and in the quality. And so the very early origins of typing, not writing, but text entry, of typing was really to standardize the font. So you have a whole bunch of forms like this, right? And rather than having government clerks fill this out with some sort of crazy cursive writing, or maybe even different alphabets, and obviously in Europe they know there's Cyrillic and other sort of languages and writing forms, but they really wanted a way to standardize it. So as the Industrial Revolution sort of kicked into gear and the kind of bureaucracy of government swelled, they wanted to find a way to make things legible. And so typing devices were a major form of innovation, but they wanted to standardize the font and the size of government documents. You know, the same way they're trying to standardize railway gauges and kind of have forms like this. They wanted a way to make text entry uniform. So it's all about legibility and standardization. And so the very earliest keyboards were pretty terrible, very, very, very slow. These are two common ones called index keyboards. So they're not really the kind of key layout that we would normally imagine. So what happens on this one is you can see there's this little deal here, and it has letters going off, and it's on a disk. And what you do is you take this index over here. Maybe I'll do it from this side so people can see it. You can take this little pointer right here, and you move it, you translate it to the letter that you like. You can see that it's actually a very unusual kind of edge. It's hatched over here in this lower case. You'll take this and you move it to the O key. And by moving it left and right, I think tilted this barrel, and moving it up and down basically moved it in and out. And then what happens is you press one of these keys, and it would push this barrel to the paper, and you get a letter of that one. Then you move it again, another letter, move it again, another letter, and you would do it. So this is basically kind of a matrix, an index to the letter that you want to type. So that's a 2D one. Here's the 1D version where you basically just move this wheel. So you take this little handle, and you slide it. You probably can't see in the projection here, but it's this O, U, F, you know, V. It has all the letters and numbers here. So you move this to, you know, the G, and then I don't know what you press. Some button somewhere, you press it, and it writes a G, okay? So these solve. problem of having a standardized font. You'd have this on with a key, and it would let you imprint these standardized letters, standardized fonts, onto the page. And these were indexed people. Now, these were horrendously slow and not really commercially successful. The first commercially successful version was this one, the Tansens Writing Wall. Okay, so it was sold. It was not really very successful. But there's a very good reason why it is shaped like this. Anyone guess why you'd want to have it in a ball form? Because it seems kind of weird to have your typewriter as a speeder. That's a good guess, but no. There'd be a hell of a lot of buttons on your hand. It's a good guess. Yeah? That'd be interesting, but no. Nope. It's a mechanical or geometric problem that this is solving. Yeah? Not quite. This is translated on a piece of paper. Yes. So they'd all point to the same place. So it was very hard. On the other machines, you really have to go incrementally move your paper over and so on. This one was meant to be put on top of a piece of paper, but the problem is if you had it as a grid, for every letter, if it just went straight down, you'd have to always be centering the key to your next letter. And so what made this a little bit more efficient was that wherever you type the key, the little kind of stamper would come to the same point. So you could type the O key, then just move it over one letter, and then you could type another key, and it'd all go down one by one. And this actually was a pretty interesting design, and it meant that it was sort of more mobile as opposed to feeding it into this big machine. But nonetheless, this was really slow, not really super successful. So when they started to have success was when the QWERTY keyboard came out. This was done by Christopher Latham-Scholes around sort of late 1860s. This is an image of that very first keyboard, and it's the most common key design that's a QWERTY. For those of you who don't know, I just saw someone spell QWERTY with a U in the bake-off, but QWERTY is the topmost row of keys up our left corner. It's not the only arrangement, and there's a lot of discussion about why Chris Watham-Scholes went for this design, and a lot of theories about why this design as opposed to something more kind of intuitive and natural came out. So here's, here's actually the kind of common story that I'm going to share with you, but it turns out it's probably false. So in the early design, so in a typewriter like this, you have this typewriter, and I'll show you an example. And because they were very heavy initially, because the keys would jam, okay, so I actually have brought not quite an 18- to 16-inch typewriter, this is more like probably 1920, so this is 100 years old, but you'll see when I press a key, and I apologize for those who can't see, is when you press a key, this comes up, and it strikes the paper, okay? Now, these are, this is not only a very old machine, but is that there's a weight to those arms, and of course it takes time for the arm to go all the way up, and all the way down. And what happens is if you type too fast, the keys will jam, okay? So I actually tried, and we can actually do a demonstration here, you ready? You ever type on a typewriter before? So try to type your name over and over again, as fast as you can. You have to go pretty hard. So, and you probably can't see what's happening, but actually right now the type bars look like this. What happened is, as they come up and strike the paper, if you go too fast, they'll hit behind each other, so you probably didn't notice, but your things are actually stacking up multiple, because it didn't retreat fast enough, and actually right now the typewriter is jammed, because there's two that have basically stuck into one another, okay? So here, I'll, I'll, I'll bring it down for maybe someone else to see, because you guys can kind of see them this way. Ready? There. You can do this as fast as you can. So you have to, none of you are actually typing hard enough, in order to get that ink on the paper, you actually have to type a lot harder. That's pretty good, that's pretty good. Anyway, if you guys want to play with that, you can. But what happens is, generally, especially on these original machines that are like 1870s era quality stuff, stamped components, heavy steel, is you get this effect where they all kind of get mushed up and the thing's jammed. And so the theory is that in order to, because he couldn't make his machine any faster and smoother, what Christopher Lattenshaw did is that he tried to put the most common letter pairings far apart. This is the theory, that you put the common letter pairings far apart to slow people down. Now originally, people did not ten-finger type, they one-finger typed. That's how all typists did it, before like 1890, or even like 1900. And so if you put common things apart, it would just literally take time for your finger to move from one side of the keyboard to the other. And the theory is that that would prevent these jams. So first off, when keys are near each other, because they're sort of on similar vectors, they're more likely to jam than if they're coming in opposite sides. And also, just slowing people down would allow the type bar to come back down. So how many people have heard this story in grade school? Yeah, so unfortunately, it seems like all the evidence seems to suggest it's false, because some of the most common letter pairings, like ER and TH, are right next to each other on a QWERTY keyboard. So even that own story doesn't make sense, because you just literally look with your eyeballs at a QWERTY keyboard. So it's almost certain that Christopher Glass and Michels actually did it for very different reasons. or design-oriented, or trying to differentiate from other people than anything else. You'll also notice on this key, we have the upper and lower case. This little thing goes up or down, so basically only type one character. If you've never looked at a typewriter up close, it's actually worth really looking at it. If you like mechanical stuff, I would highly encourage you to look at how these things are done. I just noticed on the typewriter that I got a couple weekends ago, there's a couple typewriters upstairs in the lab, and I just noticed that there's little tabs on the back of it, so you can actually set your tap key on these devices, and it has that little arrow that's in Microsoft Word. None of Microsoft Word has little arrows. I never knew where that symbology came from. It's almost certainly skeuomorphic from typewriters, because I have a typewriter that's from the 50s, and it has exactly that icon on it, and Word still uses it 70 years later, which is pretty awesome, and it's cool because it's mechanical. It's like a little stopper, so when you hit tab, it just slides on that spring until it hits the first tab, and it stops right there in the column. I don't know if this one has it. No, this one's too old, I think, to have the tab. Okay. So, importantly, Schultz, and everyone else at the time, when they were making these index typewriters, and Christopher Latham Schultz was making this QWERTY keyboard, no one ever thought that typing was going to be faster than handwriting. This is so super important. Everyone thinks about typing is so much faster than handwriting. It wasn't. No one thought. Even Christopher Latham Schultz, who invented, basically, the modern typewriter, is everyone faster than typewriting? No, it's impossible. Handwriting, 20, 30 words a minute. That's pretty amazing. But a couple things happened that actually started to break that trend down. So, first, let's look at Longleaf. So, this is when Latham invented. And, again, this was during an era of incredible industrialization in the United States, incredible kind of a spread of bureaucracy and kind of capitalism, and so typewriters are really important for that spread. So, typewriters started appearing in all these offices. Everything was done by correspondence. Everything was type banked. Accounts were done by type. And so these typewriters were a huge phenomenon when they basically were able to be... mass-produced. So ten years after, you know, Scholz had put forth his QWERTY design, it started to become sort of more common, although there were other competing designs as well with just different layouts entirely. But Mrs. Longley, in Cincinnati, she started it from, like, ten-finger typing. Now, importantly, you still looked at the keys, but you used all ten of your fingers as opposed to the hunk and peck. And this started to, and she started training her pupils in this style of ten-finger typing. And then simultaneously, I guess, a decade later again, there was a very highly publicized typing contest. This guy called Frank Uringer, just a federal court clerk, you know, he's been writing stuff, probably at that point, for 20 years of his life. He's gone really good at typing documents. And people were sort of fascinated at that time with mechanical inventions and improving the efficiency of man and so on. And so he won this contest, and he used ten-finger typing and touch typing. So touch typing is when you don't look at the keys, you look at what you're typing, and you can just basically do it all memorized. And both of these people used Scholes' QWERTY keyboard, and that sort of cemented the success. People realized for the first time that you could dramatically exceed handwriting input, and it could be done in this legible, standardized way. So pretty much 1890s onwards, it was standardized, QWERTY had sort of won the keyboard layout kind of wars, and all of a sudden manufacturers started appealing to this new demographic of people that were using typewriters as part of their business practice, and there's all these mechanical improvements as well. Now again, with that handsome writing ball, you might be able to move it over one spot at a time, but you're totally covering over the text. Whereas something like this typewriter design allows you to actually see what you're typing very easily. So it's standardized pretty fast. So here comes the big question. Why are we still using QWERTY 150 years after we've invented it? This would be like buying a Tesla, and it just comes with growth and sales. the equivalent technologically you just have like a bunch of ropes and you tie knots to like move around not autonomously right so why are we using this ancient technology on smartphones that don't even have it just a software keyboard yeah they also have competing designs and there are various like the French don't use Cordia they use like some slight variation the Z key moves up into the top row so yes now it has mostly standardized and that's but they're all these internet there are different standards I mean if you if you travel to Europe you'll eventually end up on a keyboard in like a friend's house that is something a little bit weird but they're mostly similar there was an effort first again because global standardization was important too and so there was a an effort to try to get things together but yeah they have their own standards that are sort of Cordia inspired I don't know why they're not just totally different but certainly you go to buy some Asian character sets and it is yeah yeah that's a that's a major thing what else that's definitely I'll say one part of the puzzles that once you've learned it you don't want to learn something else like this one you've trained on how many how many people how many tens of thousands of hours you've trained on Cordy it actually be interesting to try to calculate can someone calculate this on their smartphone let's say you type on average one hour per day and you did it from like age eight onwards how many hours is that I mean that's a really brutal estimate but maybe it's within the order of magnitude so 365 times 14 years or something So I would guess you've done at least 10,000 hours of typing, I think a lot of you do more than an hour. But yes, so it's probably in the tens of thousands of hours would be, I think, a reasonable estimate. So yes, so you've learned it, and if I said, okay, I can give you another keyboard that's 20% faster, you just have to put another 10,000 hours of training, would you take that bet? So if I could speed you up 10%, but it would require 10,000 hours of training, how many people would take the Chris keyboards, top row is Chris, would you take that? Really? Wow. Must have a lot of free time, 10,000 hours is a lot. No one else would take that bet? How about 30% faster? 50% faster typing? Twice as fast. How about it doesn't matter, like you're just not learning, like I'm done with school, I'm not learning anything else in my life. So yeah, so I mean, even with those odds, that's pretty tough. But there is actually another really important factor about why we still have QWERTY beyond the switching cost, which is more of an economic cost. Yeah? Like, is it better to have these keyboards than if you have an office that you're not going to? Mostly, there are like European variants and stuff, but yes, it is fairly standard, that is true. But that, so that's part of the switching cost, is that you don't have to go to, you move to Poland, but then you have to like learn a totally new keyboard layout. So yeah, that's part of the switching cost, I would say. Yeah? Is the idea to have a switch that's already a nice keyboard? Yeah, that's part of the switching cost, I think. If all of a sudden Donald Trump was like, we're only using the Trump keyboard, you know, it would cost a huge amount of profit going to switch. So the other really important thing, okay, besides the fact that there is this very, often you'll hear people say we don't get rid of it because of the switching cost, but the other really important thing to keep in mind is that actually QWERTY turns out to be pretty good. That people have spent a huge amount... amount of time trying to make something better than QWERTY, and the best is like 10% or 20% faster. And that's just not good enough for you to have to train for another 10,000 hours. Most keyboard designs that you'd want to adopt are probably in the order of like 0% to 5% faster. And that's just not good enough. So if QWERTY was terrible, if I could offer you something that made you 50% or 100% faster, you probably would take it. But the fact is that no one has invented that thing yet. Probably the closest that's come to widespread adoption is Swype, where you gesture through the keys on your mobile phone, and that is quite a bit faster than having to type on single characters. And that has been developed, and that is quite different from a QWERTY keyboard, if you think about it. It just happens to follow that QWERTY layout. But because QWERTY is so weird, you actually get these really interesting patterns that actually enables Swype to work pretty well. Yeah, and QWERTY is not very good at this, because, for example, an ER key on your keyboard is on your same hand. They are next to each other, but they're on the same hand. And so you sometimes will use the same finger to type those, and it does slow you down. So people did try this. We'll talk about this in a second. So here is just some stats on QWERTY, just to give you sort of like, you know, whatever like Dungeons and Dragons style like performance, right? Is if you two-finger press, which is how people start out, this sort of pre-Madeiran day, you sort of have the 20, 27 to 37 word per minute. Most people are in the sort of 40 word per minute kind of now. If you're a professional typist, like that's what you do for your job, is you just type things, whatever, sales, receipts, whatever it is, you're like in more like the 50 to 80, maybe getting up above 100. The world record for QWERTY, I believe, is still 216 word per minute. So that's extraordinarily fast. But this isn't an even distribution. It's a long curve. You are probably, up here, if you guys type again, you are not like the user. You are experts. You just don't realize how good you are at QWERTY. But the fact that you can jump on someone's machine and just sort of type without looking at your computer, that's alright. level of expertise. And so you're probably the very high end of this curve. You see it's got a very long tail, and that's not deep. This is also old data. I'm sure now, the moment it was published, I could put the data on it, but it looks like a graph from the 80s or something. I'm sure that this has moved over as typing has become so pervasive. Now a really interesting idea, and this is sort of a bit of a controversial paper in HCI, is this team ran this idea, ran this sort of project that looked at this inviscid text entry grid. And this is that, it's basically what is the typing speed of your brain? Because we don't need to have something that we can write 500 words per minute. If your brain can only write 100 words per minute. And so they sort of toyed with this idea that if there was just a maximum level of output, while a user is doing a creative task, not just transcribing. So anytime you see these people doing like 260 words per minute, they're just copying like one page of text to the public. So they're typing, typing, typing. That's not creative thought. So you can do that incredibly fast. But if you have to actually compose an email, let's say, what is the fastest you can possibly compose that email? And it's probably something on the order of 20 to 40 words per minute. That's what they calculated. So if you ask people to do an actual task, not just regurgitate text or type as fast as they can, it's probably something around there. So 67 words per minute is what they're saying it is for mobile text entry. And right now, we kind of state the order of mobile text entry, somewhere between 20 to 40. And they argue that means that there is still innovation to be done in the mobile space. But arguably, QWERTY keyboards, probably many of you are already able to type at more than 60 words per minute, 70 words per minute, maybe even on a QWERTY keyboard. So maybe you're actually typing as fast as you can possibly type anyway. You can only code so fast or email so fast. Yeah? Is the rate at which your mind can concentrate at first as fast as, like, can you make that faster? I don't know. Caffeine? Or is there a factor of, like, how fast? Yes, probably. Well, I don't know. You'd have to read the paper. It's been a while. It's been a couple years since I read this paper originally. People obviously know how fast people talk. I think generally, you talk, you can make more mistakes in speech. Like I just stopped mid-sentence, right? But it's normal for human speech to have pauses and discontinuities and hypotheticals and so on, right? When you write an email, you can't just switch mid-sentence. Like, that'd be a really good, I really think we should do that idea. So writing text is definitely slower. There's also more context in speech. You can read people's facial expressions and so on. So it gives you a bit more extra dimensions. But I don't know. That is a good question. You can imagine, this is what makes voice entry so difficult. If you try to compose something, like if you dictate to a friend, like an email, you'll see it actually takes you a long time. Because you're like, no, scrub that. Replace that sentence. You rephrase it, right? And it takes more time because you're iterating on it. Or if you're writing it, you're doing that word processing yourself. You pause, you delete, you do that. I've done some work on text entry myself. I've really only done one paper ever on text entry. And this is our keyboard. And we made it really, really, really tiny. We sort of challenged ourselves to make the world's smallest keyboard at the time. And this is still running. So if you have an iOS device, you can try to pull this up. And you can download the paper here. But we made people do it on this penny-sized keyboard. And they were able to get 9.3 words per minute. That's actually about the middle of the path for the bake-off speeds. And the way that it worked was as follows. So here's just a demo. So you'll see it has a two-step. Spaces can be typed with the space bar or by performing a right swipe. A left swipe deletes the last character. An up swipe brings up the keyboard for symbols. Eco devices. We ran a text entry study using this penny-sized keyboard and found that users could accurately enter text at approximately 10. So you can see a very simple design, there's no auto-correct or anything like that, and we were only at about 9.3. Here is a paper that was published just a few years ago, and it's pretty much exactly the same as the winning design. So this was publishable peer-reviewed research 3-4 years ago, so what you get is a research last year and 6 months. Only 4 years earlier. So moving on, so over the past, QWERTY is not like the only game in town that has a QWERTY keyboard on a soft keyboard. You can see QWERTY keyboard on a button, small button devices, so it's basically a thumb keyboard. Does anyone own a BlackBerry device and have a thumb keyboard? A couple people. Great. I was never fancy enough to own a BlackBerry, unfortunately. So people really love these devices. There was a real cult around BlackBerry devices, because you could go surprisingly fast, so 24 o'clock at about 60 words per minute on these devices, and that's why there's still some people that cling on to BlackBerry-esque devices, BlackBerry being an Android phone, that have this keyboard layout that's super super popular. The other style of keyboard you see quite a bit is the alphanumeric keyboard, so you can do things like Netflix or iTunes or something, where you get this alphabetical arrangement. And this is a little bit confusing why they do these interfaces, but I think it's mostly for layout. If you do a 16 by 9 layout, it's really hard to have a long QWERTY keyboard and show all the search results. So I think they tend to want to put it into the corner, and then you can put a really tiny QWERTY keyboard, and if you do a sideways QWERTY keyboard, you don't know that mapping. And so they tend to say, well, we're going to do a different layout. Let's just keep it alphabetic. But it definitely breaks its own style. And you can even see they don't even follow the same grid. Like this one ends at D, this one ends at F. So they found that it's not any faster for beginners or advanced people. It's basically slow for everyone. Probably the most notable and successful one, though, well, I guess the alphabetic ones are used quite a lot in interfaces. But probably the most substantive challenger to QWERTY is the Dvorak table. So this was proposed in the 30s. It was designed explicitly using human factors knowledge to make typing much, much faster. And it used what Christopher Laughlin sort of was theorized to use, or maybe kind of like retcons to use, as they use all the letter frequencies in the physiology of people's hands. So it puts the most common letters under dominant fingers, makes a lot of sense. It biases slightly towards the right hand, because most people are right-handed. And common combinations of letters alternate between hands. And the reason you want to do this is that while one hand's kind of going in to type the H key, the other hand can be going in to type the E key. And then the L key, right, so you can type in hello. And you kind of get this motion. You want to see a lot of this. Because while one hand is in flight, the other hand can be moving kind of ballistically towards the target. And so you get this sort of like drumming effect on the keyboard. And that's what's fascinating. And you can do this sort of repeatedly on the same hand. So here are actually some stats. So on the right here is QWERTY. So they got people to enter this corpus of data. And they recorded just what all the stats are. So in this case, in this corpus, the average distance moved of a figure was, or its total distance moved was 17.8 meters. That's quite a lot of movement time. You know, if it's long, distance makes you slower. How many keys are on the same hand is 41. On the same finger was only 7.5 required using the same finger consecutively, which is bad. Because obviously, unless it's a double key, it's not going to work. going to be quite slow. The home row has 31% of keys, but the top row has 52%. That's a lot. That means you're always having to go up to that home row to type half the number of characters, which is pretty bad for you. If we look at Dvorak, 70% essentially of the keys are in the home row right under your fingers, so you have to remove, you don't have to travel nearly as much. You can see that reflected there. 10.9 meters. So much, you know, half the distance traveled. Same finger is very low. You want to have it ideally not having to move the same finger too many places, and the same hand is about 20%. You can see that in the bottom, which is actually the hardest, it's easier to reach up, it's a little bit harder to reach down, and so there is a slight preference again to the top row, and the bottom row is almost never ever used, right? So basically Dvorak did the right thing, very kind of cleverly said, well we have all this data, but the English language, why don't we get empirical data and design slang? So basically, not on like a bake-off, he ran like a super bake-off, and this was the layout that we proposed. You can see that it's unusual, or even better, it's definitely a lot of data, for example, in that middle row. And you can also see that it's biased. So Dvorak's bias was left-handed people, and Dvorak's bias was right-handed people. Okay, so, what happened is, so you invented this in 36, and then during the war, during World War II, again, we talked about like an enormous bureaucracy being spawned, every person's effort is important. You want to, you know, put people into the steel mills and onto the front lines, you don't want to be wasting time on a slow keyboard design. So the Navy thought it was really important, and so they commissioned him for a study to see if they should switch, you know, whatever the Navy type is to the Dvorak keyboard, and what Dvorak noticed at the end of the study, they were 74% faster than QWERTY speeds, and their accuracy increased 68%. So the Navy was just like, oh my gosh, it's incredible, like that's, you know, we will totally, we're willing to train people for 10,000 hours, because we'll literally just put them in a room for like three months and train them to be professional type, that's okay, the army, we can train people, right? And so then they did a follow-on investigation, because people were just like, But those numbers are really, really good. And so in the official Navy report, I actually took the actual sentence out of the Navy report. It says, unfortunately, subsequent investigation has shown that at best, the experiments in the Navy study were biased and at worst, fabricated, which is too bad because the forehand of the White House has blocked the numbers so much. And if you look at modern analyses that are done in rigorous, you know, peer-reviewed studies, they find that it's generally 2% to 10% fast, which is not insignificant. And if you're willing to do that training, I would love to just be able to type 10% faster one day. But as I asked you in the beginning of class, how many people would spend 10,000 hours to get a 2% to 10% improvement, and like three of you raised your hand. So and this is probably another reason why, it's not that there aren't better designs than QWERTY, it's that there's nothing so dramatically better than QWERTY. Yeah? Is there ever going to be studies where you're taught on this in like fourth grade, and then it goes on up and down? Uh, I don't know off the top of my head. I wouldn't be surprised. I was just wondering if you didn't have prior experience with QWERTY, if you're going to be hired. Maybe. Expert, can you tell us about how you did your training? Did anyone else type in Vorac? We just have one special person. Okay. Convince us. You have three minutes to convince us. I mean, for me, I just switched over to the keyboard on my own keyboard, and it took me maybe a week to figure out where the keys were, and then you just get faster over time. So it wasn't a huge deal because it's not 10,000 hours. Yeah. And I just like those little improvements. And also, since everything's under your fingers, you don't have to worry about it. Yeah. So how long do you think it took you before you kind of got up to where you were before QWERTY? I'm not sure. I mean, I've probably been using it for four or five years now. I can definitely still do Wordy just as well, so it's just like learning a new language where you still have the old one, but you also get a new one. Have you tried doing a word per minute benchmark to see if you're fast enough? No, I haven't. That'd be interesting, huh? If you do, let me know. I'm very curious. Anyways, there you go. Four years, and you can have good work, and also no one can ever steal your laptop if they have no idea how to log in. It is a really funny thing other people try to do with their laptops. Yes, I can imagine. Anyway, so you know the numbers, so if you want to learn it, it is possible. And there are people out there, and kudos for them to put in the time to make that included, but most people just don't want to do it. Okay, so obviously there's a lot of ergonomic designs. I'm sure you've seen all these funky keyboards that are still pretty S, but I've rearranged them in different ways to make them more ergonomic. It actually isn't great to have your hands like this all the time. At the very least, you sort of want your hands more like this, and so there's an innumerable kind of keyboard that does the various variations on those things. We talked earlier in the semester about corded keyboards. If you remember, Engelbart had the left-hand kind of key set and the right-hand mouse, and these are, you know, where you're able to basically type every character when you press multiple keys at once. So here's that key set, and again, here's the vocabulary. Your hands are always on every key. You don't have to translate them at all. The home row contains all possible keys and numbers underneath your hands. We're just getting faster than this. Maybe this will be your next challenge. This is getting decent, and then you'll be super fast, and definitely your keyboard would just be four gigantic buttons. That'd be pretty cool. They do make modern versions of this, so this is one that's produced for wearable computers. There's definitely people that are super hardcore, like the early Google Glass people. In fact, the guy who created Google Glass, Pat Starner, he's a professor at Georgia Tech, he has been wearing basically a head-mounted display. for, oh my gosh, like 20 years, ever since I ever started reading his papers, and he walks around with one of these all the time, and he's super proficient, he can just be typing, and the problem is, it's so efficient that even when you're meeting with him, it's pretty clear, you're not really meeting with him, he's just staring at you blankly, but he's really just like surfing, I don't know, Gizmodo or something, but he's really, really, really, really fast, and he will look up stuff in the meetings, which is pretty cool, and there's sort of different versions of him, and they all have function ego, and they sort of do everything, so if you put in the training, you can be really good. Probably the classic example of being really efficient if you're willing to put in the training are court stenographers, so these are the people, you know, they don't record court sessions, you know, the same with like, you don't, you always get this like paintings of like Paul Manafort, you know, sitting in the things, you don't have any photographs or video either, depends on the court, so everything is done with transcripts, like court stenographers are writing down everything that's being said to basically produce evidence, you know, testimony that's being, you know, said in court, and so because people can type, sorry, speak really fast, like me, is that you need someone who can keep up with it, and so professional stenographers that are like employed in court, they go about 180 to 225 words a minute, which is fast enough to keep up with human speech, and they use this very unusual, people are called stenographers, people who look like this, I'll show you one example, I don't actually fully understand the mapping myself, but it's a combination of a shorthand and a chorded keyboard, okay, so for example, in this line here, okay, so that's okay, the first stroke of the word example, you hold down the letters P and L, that's these two keys right here, you'll see that doesn't contain all the keys, so your first thing is you hold down P and L, which gives you an N, and then the second stroke of that word, same key combination, so I think this is, I think K, P, and E, or X, am, folk, and then, oh, I see, and if you have a double P, L, that becomes, so this one becomes like an M, and the second one becomes a PL. And each line, each one of these rows, is all the keys you're pressing simultaneously. So they're able to write the entire word example in two keystrokes. They kind of go K, P, A, P, and L, and then PL again. And that's example. And you can see that if you look at how they're writing this out, there is like a shortcut, like up is up. But what's a good example here? Well, so from is, I presume, F, R. Here's shorthand. So that must be an N, just like a machine. Now, it's very complicated. This is why you need expert training. But they're able to go extremely fast. But yeah, it's a combination of both shorthand and this chord of keyboard. You need a lot of training to be a professional. That's all you do. That's literally your whole job is to type. So here are probably some of the more exotic ones that are in the research domain. Back in especially the late 80s and 90s, there was a huge push to come up with a chord keyboard. Instead of doing it Dvorak style, where he just opened up big books and looked at letter frequentation, he's like, let's go computational. Let's have computers design the optimal keyboard and minimize the distance using a huge corpus and so on. And so probably the two earliest ones were OPTI and OPTI2. And you can see that they've condensed it so you have to travel less. And because space is the most common letter, this is four space bars. Now we can use the agility of your thumb, not just only for the space bar. It's sort of a waste for your most dexterous finger to only really ever type space. And so we can use this as a thumb for this region, and you can use it even like this for your pinky, so space, gigantic thumb. They reduced that a little bit in OPTI2. They've not only said, you can do a honeycomb kind of thing which lets you have better packing than just a regular grid of keyboards, for example. And a lot of these are computationally-ridiced. They do some sort of optimization-based approach, like simulated annealing or something, that basically shuffles around the keys to get this optima. And the arrangement basically minimizes total travel, or minimizes same thing. or whatever your optimization function is. So looking at these, okay, what do you think might be a problem with those designs? Looking at them yourself. Why don't we use those designs? I mean, I will make this your default keyboard on your phone if you want, so you have no objections. Yeah? I think more about dimensions, like it's, dexterously, it's a lot more difficult to click on, like, number keys, just because they're farther away, and you have to, like, stretch your hands more, and these make it all the more impossible. Sure, that's definitely a problem. What else? Well, I was gonna tell you, like, most of these can't really be turned on. Like, if you were trying to play this out on a keyboard, that could be a problem, you have no idea where it's going. Yep, these are definitely designed for sort of mobile settings, yeah. It's easier to go with the high up and the high, and then it's easier to come down. Mm-hmm, I've got bigger hands, which are like this. Yeah? Well, if you look at my phone, you can't click, and then you're not holding it up, and it's supposed to be on the keyboard. So it's a little confusing. Mm-hmm, all of those are true. And of course, the big one still remains, that is, who wants to learn this again? It sort of suffers from Dvorak. It's just like, yeah, these may be great, but I don't want to spend all that time, and these are even more exotic, you know, in some respects, than even Dvorak. So what people try to do, to have it so you can get improvements, but not have to learn something from scratch, are this sort of notion of quasi-QWERTY events. Like in the 2000s, people were like, we've been releasing these research papers, like some professors spent their entire research life trying to un-see QWERTY, and like not even their own PhD students would ever adopt them, right? So then they said, well, how about if we just like fudge QWERTY a little bit, and do these quasi-QWERTYs? So this is something that's been freely optimized. Here's a regular QWERTY, and a quasi-QWERTY basically has this constraint. You can move keys around all you want, but there's an extra constraint. constraint, that it can't be more than one spot from where they were originally. So it's sort of like free movement, but with an extra constraint. So you can see, like, E is here, and now it's moved diagonally to here. Z went here, and it's moved diagonally to here. So every key is within one distance of where it was before. And the idea with this is that it's a little bit easier to learn, because at least you're looking in the right spot, as opposed to having to do a linear scan. Remember when we were doing, like, find the biggest number? You didn't have to do a linear scan to find every single key. It's going to take you a really long time to learn. So that's kind of quasi-QWERTY. Another one that they did was basically pass them down into the QTBitbox, and it is QWERTY, QTBitbox QWERTY. What they've done is they've divided up the letter space into different sized buttons, such that you basically just have a one-line keyboard. Now, auto-correction can sort of take over. So this is sort of like T9, but a different arrangement of letters. And it's a very tiny, thin keyboard that can exist, like, at the bottom of your iPad without having to take up the whole screen. And this is also sort of interesting. But again, like, none of these have really been significantly adopted. I forget what this is all about. This is another one that does the one-line keyboard change. And these are the two main guys at Google that are working on the keyboard change. So if you're interested in working on Android keyboards, Shuman Tsai is the guy to talk to. Now, the other thing that you've seen over time is the transition from early kind of iOS and Android to what you see now, these sort of flat, minimal designs. And you've noticed they've lost their buttons, OK? Anyone theorize why these designs have lost their buttons over time? There's probably two good reasons. How many people have a keyboard that's like this, where there's not really defined buttons? I think most people. Let me see what I have on mine. Yeah, so mine just looks like that, where it's just like a sea of letters, but there's no buttons. So why do you think it's important to get rid of buttons? I agree, so buttons on a keyboard are basically skeuomorphic, that's basically a design accent, we don't need the buttons, there's not buttons really there, we're just rendering the shell of a button, it's almost aesthetic, people understand what this is, it's sort of a non-skeuomorphic kind of keyboard design, absolutely, so it does reduce the sort of visual complexity where you get to focus on just the letters, there's another good reason why they did this. Yeah, so if you do use swiping as a keyboard, it's sort of weird to like cross over a button, but if it's just this flat feel that sort of encourages swiping, yep, there's another effect that happens, if you ask people to type on this versus this, they actually change their, the way they type. Anyone guess how? Yeah? Yes, very much so, so when you give them buttons like this, they tend to be more precise, they'll even use the tip of their finger, if you just give them something like this, where the boundaries are sort of less distinct, they're actually sloppier, they're actually sloppier in their input, and they're faster as a consequence of being sloppier, and what this means is that even though they're sloppy, like it might fall between the H and the J, or whatever, like, you know, it's like TH, and then I land between the W and E, because I can start using computation to resolve, are they typing THW, or are they typing THE, and chances are they're typing THE, and so even if they land, even if they land a little bit more onto the W, I can still correct it as E, where if I put a button, sort of like that Windows XP example, like, if I click the W button, and it still types the key, now you sort of get this user frustration where they don't understand why something broke. But if you show it like this, even if you type more on the W but it clicks the key, there's sort of this forgiveness because there's not a rigid boundary. You can sort of, if you put a rigid boundary, don't violate the rigid boundary. So what's happening behind the scenes is essentially this, where they're morphing the actual hit areas of the keys. And again, if you have a hard boundary and you kind of lie to the user, then there can be frustration. If you just don't show it at all, you can sort of lie more efficiently. And this sort of idea goes way back. So you can label, you know, four years before the iPad ever came out, there was this paper called MidKey, and they did it sort of in a kludgy way where their mobile keyboard actually physically changed size. And actually one person in class did this design where the most frequent letter keys were just larger, so they were easier to hit. So to think of it from a Fitts' Law perspective, you have a certain number of pixels, so you give the bigger buttons, the more frequent letters, the bigger buttons. And since they're more frequent, you sort of get, it's sort of like a Huffman coding or something, like you get this like benefit of compression. In this case, they did it as you type. If I type in G-H, the next letter is going to be E, like E would be much larger than anything else potentially. But now they don't really do this, they just do it functionally by kind of morphing the boundaries behind the scenes. And they can even adapt it so that they know that you tend to hit high on letters or low on letters, so they can actually have a word model and kind of a user model that allows you to store it personalized to you. Like if you have gigantically fat thumbs or something, they can do that. So this lets people type exceptionally fast. So here's someone typing on iOS, older version of iOS. And this is just not possible, there's no way anyone could just accurately type in those small particles. Can anyone type this fast? And this video is kind of old now, so. Yeah, you can type this fast? Nice. Like, this guy's so slow. So yeah, so this is around 90 words per minute. So if you can type faster than this, it might be about 100, which means you're like almost a word snobber on the iPhone, which is pretty impressive. But anyway, all this is done with auto-correction. You can't really do this with a plain wordy keyboard. And that actually means that we might attempt to take able to have laptops that are basically all screen. I think I have one of my current highest. So you might be able to get to a laptop that's basically all screen and no keys, because unlike a physical keyboard that's hard to make fast, you could make a screen-based keyboard much more efficient. Although, I would say that typing on an iPad is still quite a bit slower than a laptop, at least for me. But that's because I haven't got a lot of training on it. Another keypad where I'm able to talk about are number keypads. They were actually the oldest kind of keypads. Well, they're contemporaneous with typewriters, but these are sort of the earliest maybe digital systems of keypads. So how many people have used a phone like this? How many people have ever dialed a number on a phone like this? Not like in a, oh, you did, wow, great, nice. How many people have never, ever dialed an actual phone number? OK, let's give it a chance. Let's just start recording the statistics. I know an answer, but I don't even know what that is because I've been there. And then I know, I'm retired. So for those of you who don't know, if you stick your little finger, like here's zero, so it's 0, 9, 8, all the way up to 1. You stick your finger in the little zero hole, and you rotate it around until it hits that little stopper, and then you let go. And it runs, it kind of winds back with a spring. And what happens is every number, there's a little pulse. It goes pop, pop, pop, pop, pop, pop, pop, pop, pop, pop. And the digital switchboard listens to the number of clicks, essentially, and it knows that you dialed a 9. Very rudimentary. It's like an electromechanical system. Works pretty well. And then in the 60s, they invented dial tones, so when I hit the number 9, it's a unique note, like a beep versus a boop, or whatever. And so the digital switchboard, instead of having to count the pulses, which is sort of error-prone and nasty, now it's just encoded in frequency, and it can be very, very short. Now, what's interesting, and you may probably never, ever notice this, is phones actually are very different from calculators. So these are common. And you'll still see these at point of sale. So the 0 is on the bottom. You have 1, 2, 3, 4, 5, 6, 7, 8, 9 at the top. So any time you get a calculator, calculator app on your phone, whatever, go to Starbucks. This is the design. Phones do the opposite. And actually, Bell Labs went out and tested this in front of consumers, and they were going to introduce the digital, the number of touch-tone phones, and they found that consumers actually much preferred this order. So, you know, they actually, not like a lot of the companies like I'm telling you about this semester, is Bell went out, actually out and did field trials with different number of keypads before releasing the phones. They made millions of, tens of millions of these phones, these number of keypad phones, that cemented, if this is the current keyboard of the phone industry, it'll probably be there until we're dead. And they actually did a little bit of user research, hung them up probably Christmas or Matthew Schultz, and came up with this design, which is kind of cool. Now, a little piece of trivia for you, does anyone know why all CMU numbers start with 412268? No? Yes. This is CMU, okay? And I have your mnemonic, so CMU is 268. But the more interesting question is, why is it 412? So I'll give you a little bit of a hint here. So Pittsburgh was 412, and New York City is 212. Does anyone know why that is? Does anyone come from, anyone know Chicago or LA's numbers? 618, that's a new one now. LA, the original LA is 214. Anyone from Chicago? Anyone from Chicago is 312. That's a big hint. So why is Pittsburgh 412? Was it like the fourth largest city or something? Yes. So, back when they were assigning zip codes, or area codes rather, they went in population and on those rotary phones, we had to dial. The shortest number you could dial was a what? It was the one that was right near the top. But I guess for various technologies. the reasons that they couldn't have duplicate numbers and get confused. So 2-1-2 was the fastest possible number that you could. You couldn't start with one also. One is how you prefix numbers, like dialing out. So 2-1-2 was basically the shortest area code you could give to a number. And so they gave it to New York City, which was the largest city at the time. Then the next largest cities in the US were Chicago and LA, so they gave it 3-1-2 and 2-1-3, which is the next fastest numbers to dial. And then it was for the next cities, 4-1-2 and 2-1-4. I don't know who 2-1-4 is. Might be like Cleveland or Dallas or something like that. And so Pittsburgh was basically the next largest city. Now, if you're from some small town in the United States, you probably have some really weird one, like 6-9-7 or something crazy. That's really hard to type, although now it's mostly blue because everything is done with a touchstone. That's how they assigned these things back then. So again, it was quite data-driven. We often think like, oh, data-driven design is like this new thing in the 2000s, but they were doing this back in 1905. I guess it's data-driven, human-centered design. It's just the same thing, just different names. So number keypads, when you had early cell phone days like this, you would do the kind of triple or multi-typing. So this would be, if you press the same key the number of times that you wanted to do it. So if you wanted to write hello, you find the H key, you press that twice, you go from G to H, so you see the H would appear here. Then if you immediately type another key, it would automatically say the H. So in this case, you type E, you go three, three, you type it to E, and so on. How many people have ever typed like this on a phone? Everyone, huh? Almost, gotta be almost everyone. Okay, so hopefully you've experienced this. Eventually, no one will have any idea what this is they copied it. So be grateful that you got to see this really amazing transformation in technology. We are the generation where mobile internet and mobile telephony happened. It happened so slowly for us, but it's incredible that this happened in our lifetime. We should be really thankful for that, to have witnessed that incredible. inflection point in human history. Very cool. T9 was sort of added on top of that. So in this case it tried to figure out 5, 4, 3, 5, 5, 6. It would try to guess that the only letters that exist in all of those buttons was hello. And that is an example of this T9 predict tech. Some guy got a super great job of inventing this sort of predictive tech scheme. There was a patent. I don't know who he is, but he's probably like sitting on the island and peeping with a margarita right now. So T9 was super successful and arguably much better than triple action. Okay. Wrapping up here a bit. So the other thing I should mention is sort of prediction versus correction. They are two different strategies. Prediction is when you type something and it's going to suggest sort of an auto-complete for you. Like this. You only type in 5 characters, but it can give you that 10 character word. Auto-correction is when you type something wrong. And both are really useful because every single time you have to hit that backspace is increasing your keystrokes for characters. So both are really important and often you can combine them for sort of a double win. I forget the question. So the other important thing here is letter and word prediction. So this is what we call a letter model or a word model. We have statistics and many of you use this. The letter T is followed by H 32% of the time. If you know that kind of statistic, you can bias your keyboard in certain ways or even suggest that key or even suggest the word to make it much more efficient, much less error. So that's like a letter level model. You can also have a word level model. So if I have ceramics collected, I type that in maybe using some kind of auto-correction. Then you can actually look at the phrase. So you have 4% of the time it's by and 3% of the time it's on. And probably like never is it collected by, I don't know, flies or something like that. Or very, very rarely, right? So if you use all this huge corpus of data, what Google does, Apple is kind of interesting because you have to opt in. keyboard settings, you have to actually explicitly opt-in to give them feedback on their keyboard because their Apple is very privacy sensitive. They want to protect your privacy by default. Google, it's like on a bashful, they just suck in all every key press. They're just sucking it into a gigantic master lock. So as new memes come out or weird spellings and stuff, they automatically ingest that and bring it back down to all your phones. So you often type in something that's a little bit edgy and it probably already has a autocomplete built in. And that's why Google sort of is like the master of the web because they're really good at sort of weaponizing all this data better than many people. Here's another example of a letter level frequency. So this is all pairings of words. So you can see like the TH, as I mentioned, very common letter pairing, and also happens to be right next to each other on the QWERTY keyboard and validating that design. There's HE. So even just having a justice in sort of memory actually gives you a lot of things to do. Sort of like an HMN, you start typing in this sequence of letters, even using some very basic things, actually can dramatically improve performance. And it's kind of silly in some respects that we still have physical keyboards that are preventing us from fully utilizing these. If I type a THZ on my keyboard, now macOS does sort of correct it, but like sometimes I just want to type that. But it is just silly that if I type in THZ that it isn't automatically done. And sometimes I have to fight with it to be able to type it up. And then finally, there's also language level models. I talked about letter models. I talked about word models. So the word sequence models. But there's also language models in general. So if I type in I am a blank, right, I type in I am a, it's very likely to be a nap, because we know that's how the syntax of the English language would be very rare for it to be, for example. Not maybe impossible, but much less likely especially. And it's very likely you're composing like haiku or something on your mobile phone. And so mobile speech kind of, you know, like a mobile keyboard should be much more aggressive in throwing autocomplete at these things. And so this would be an example of a language level model where you're taking advantage of syntax at the language level.\",\n",
       " \"All right. Thank you.  Okay, so you guys voted on a reprieve, a two-day extension to Big Ops 3, unprecedented in the history of DHCS, but I could feel from those emails coming in at Sunday at 2 a.m., there's a little bit of tension in the air, and I thought I had it off, so hopefully it doesn't be real. My team drew on track, so I apologize, keeping it going. Some of you, a quarter of you wanted the pain to be over, but taking everything into account, and I got some private emails, too, about some inter-team wrangling, I felt it was better just to give you a little bit of extra time. It will make it a little bit unusual in that I will also be, because I need to give you at least enough time to do Big Ops 4, which I would say is a little bit easier than Big Ops 3. Big Ops 3 is probably the most sophisticated in terms of the design space, because you're basically building it from scratch, but I'm going to still release Big Ops 4 on Thursday, because I want you to at least start thinking and have the idea rattling around in your head as we go into the final period. Basically, you should treat it like a final project. We're not going to have a final exam, but it will be a presentation, kind of like what we normally do, and that will be held on May 13th, which has been the final exam slot. We'll use that. I think it's scheduled to be three hours long, but we're not going to use all three hours. It will probably be two hours, so we'll get out and enjoy the nice spring weather a little bit early. Until then, we should talk about Big Ops 3. A lot of teams are in the midst of iteration. I would definitely say, because I've gotten a lot of fractured ideas, is if you want to make sure your idea is legal, send me your final bullet point list of what your design contains, so you can get the final rubber stamp. There is still some confusion about some points. Namely about not biasing towards the target phrase. Now people keep on asking me, like, if we know that the next word to type is a three-letter word, can we load only three-letter word autocomplete? Like, no. You know you're typing even and, but I'm going to show you the five most common three-letter words, like and, the, whatever, and the answer is no. You should basically pretend, like I keep kind of saying every day, just pretend like you don't know what the answer is. Like, you don't know what button they're clicking. You don't know what the correct rotation is. Or very like, in this case, we're making it more absolute than the previous two, I guess, because you were sort of like, I allowed you to sort of decorate things specially. But this one you really can't decorate specially. Like, it's only just for checking. You can sort of imagine the target phrase is in the user's mind. This is the text message that they want to write. And we're going to simulate that for purposes of a user study, sort of figure out what the Fitts' law coefficient is for our keyboards by having them enter that target phrase. But if you're biasing something, you're taking advantage of whatever's in the target phrase. Like, someone asked me, like, when they finish typing in the sentence, we just automatically hit the next button for them. That's not legal either, because you're, again, taking advantage of the knowledge in that target phrase. So if you're doing anything, if you're doing any sort of code that's checking that target phrase, it's almost certainly illegal. OK? The only time it really should ever be checked is when they hit that next button, OK? And another team was asking me about whitespace. If you look at my checking code, is I strip all the whitespace. So they have, like, 10 spaces after the end of their sentence or whatever. It's irrelevant. It all gets trimmed down. And we use Levenshtein distance. Don't worry. I'll tell you about that in the lecture after bake-off. But it's just a more correct way of basically doing string matching, so you don't get penalized for long runs of incorrect characters. OK, so with that said, are there any general questions that I can answer for people? So how many people have a working prototype? Raise your hand. Hopefully everyone, given that it was supposed to be right now. OK, good. So that means you have. have now two days to polish that and do the final iterations, run those user tests that you've probably asked your roommate to do like four times already, OK? Now you have this little bonus period where you get to really polish up your design. And the polishing does matter. I would say don't super worry about adding on new features. If it's a very rudimentary design, and I've said it to most teams, but it's possible some teams have still yet to email me, you're not allowed to duplicate just like a T9 keyboard from a flip phone. Even though that is legal by the design rules, I've made that particular design illegal because I want it to be slightly more inventive. So you have to add at least a little bit of special magic secret sauce to that design to make it legal in my eyes, OK? Just because that's kind of, I want it to be more inventive, and there's definitely better ways to do it than a T9 keyboard. And then finally, if you're like, oh, we have 48 hours, let's add in autocomplete or text correction. As I said at the beginning of the bake-off, everyone thinks that's going to be like the magic book that gets you to super high speeds. And I would say that's only maybe a 30% boost. The big boost is a good entry. You can get the letters in, that's going to help you be much faster than having autocomplete or text correction. Everyone always spends 80% of their time kind of with an elaborate algorithm for autocomplete. And I'm telling you, spend that time on the design instead. You'll find it much better. Yeah? You can't use the key again, so it doesn't work very well. The keyboard doesn't work. The keyboard is not bad enough for you to do it. Instead of, like, from somewhere, like, you can just click it. So I've read one where you click it, like, a number of times. Like, basically, if it's sort of the one position of A, B, C, D, E, F, whatever it is you have, if it's too similar to that, if you just triple-type, then that's a pick-back. If it's like a, even if it's a select and then pick three, that's like. You're out of illegality, but it's just the width of an idea beyond illegality. So I would encourage you to think a little bit more creatively. So it's not, if it's just like a triple type, like it's exact clone, that is illegal. Or some sort of click and press, or a triple click or something, yeah. Just don't make it an exact clone. You know, I think there's ways to do it. You don't have buttons. You know, the flip phone has to do it that way because it had like a grid of three by four buttons. Because we have a touch screen, you can be more clever. Like a click in the cell and maybe drag, you know, for example, just like one of the options. Okay, any other questions? So how many people are feeling more confident now they have an extra 48 hours? Raise your hand. How many people are still not confident? Okay. Keep battling. This is design. Like, you know, two and a half weeks is not a lot of time to come up with a whole implementation of a design, but it's forcing you to leverage hopefully the strengths of your team. Why is this, until I pick the bad table again, and then we'll establish that in the semester. Okay, so today we're going to switch gears again and talk about sort of a special topic in HCI, but I think a very important one. Today we're going to talk about machine learning and its applications in HCI. And they are a lovely pair, and I would say if you look at my research projects, go to my personal web page or my lab web page. Looking promising? You will find that most of my, almost all of my research projects have a machine learning component. And importantly, I'm not a machine learning researcher. I'm using it as a tool. It's like I can compile my C++ code, but I'm not a compilers researcher. So HCI, because it's so thought, we're thinking about the end user, if you want to take these awesome advances in computer science, whether that's like JavaScript compilation in real time, or just-in-time compilation with Java, or using TensorFlow and Python, or using machine learning libraries. We can take all these awesome things that people have sat in the cubicle for like ten years banging their head on, and then we can have the fun time with it, okay? This is how I feel on my job. And machine learning is a great example of a very powerful tool that has this natural complement to machine learning. And importantly, the reason why it's such a strong complement is that machine learning is basically, we can think about it both in a deep learning perspective, or like classical statistical techniques, is it can take a fuzzy input, an ambiguous input, and it resolves it with some degree of accuracy to a concrete output. And humans are the paradigm and the exemplar of this thing, where when we're gesturing, for example, it's unclear, like if I had a, let's say that thermostat on the wall is a smart thermostat, okay, and I'm moving my arms around because I like to flail a lot while I instruct. But that thermostat has to make a decision, am I pointing at it, and am I telling it to increase in temperature, or am I just flailing my arms? And the answer isn't yes or no, it's like .8 and .7, and it has to make the decision ten times a second, or once a second. And so it needs to have a reliable way, or at least a structure, some sort of formal way, to take that lossy input and translate it into action. And all human interfaces have to make this decision, right? And so we need good tools to be able to take the kind of lossy, ambiguous sac of fluid that I am into a digital signal, and again, machine learning is a perfect example of that. Okay, so just a more formal definition, we kind of went through a few tricks, we're going to kind of do a couple of these. I'll see if I can squeeze in one on computer vision as well. There's actually an individual homework assignment that I was going to assign you today after the bake-off, but now that it's so compressed, I feel like you might have escaped from an extra personal homework assignment and we'll just do a bake-off for it. But that's... So yeah, we're trying to do a special topic tonight. So machine learning is the construction and study of systems that can learn from data. You see kind of classic gesture, you know, kind of classic uses like filtering between spam and non-spam. We see this all the time with these things like Gmail, really any email client. We also see it for classifying gestures, like I just mentioned, sort of a smartphone, a staff that might be interpreting my hand motions. It's gonna have to make a decision on that. And then of course, we're seeing it increasingly for things like biometrics and recognizing. So machine learning is this incredibly powerful thing. But let's start with a really basic example. How many people have taken a class in machine learning? Raise your hand if you have some exposure to machine learning. How many people have no experience with machine learning other than like they read it online? Okay, cool. Well, this will be interesting for some and less interesting for others. But we'll go through a little exercise just to illustrate that machine learning is not a barrier to learning. We're gonna take one of these. One, two, three, four, five, six, seven, eight, nine, 10. A little fun exercise to get the brain working. Okay, so you're getting a sheet that looks like this. There's red dots, there's red circles, and there's blue crosses. And what I want you to do is I want you to draw one straight line through that clustering of points that separates best the circles on one side and the crosses on the other. And while you're doing this, I want you to observe. I always ask you to just reflect on your own behavior. Many times in class, I'm like, just look at what you're doing yourself. Just think about how you approach this problem, okay? Now, there are 100 of each. So after you draw your line, you can actually count which ones are on the wrong side of the line, okay? You can fill that in at the bottom of the page. Yes? It has to be a straight line. It has to be a straight line. Red is an arrow. So, if you're on top of one that's imagined as a perfectly tiny point in space, deep in the middle of the world, it has to be on one side or the other. It cannot be in two sides. There is no right or wrong answer here. Please! Just three seats. Come on down. You've won the sit-in-front-of-the-professor contest. So, calculate at the bottom. I'll give you just 30 more seconds. Okay, so let's hear some possible answers here. So, who has... Let's just get started. How many red circles do you have on the thing? And then, 19. Okay, so that's in total. So, if we think about those as errors, like they're on the kind of wrong side of the clustering, that's 31 errors, right? So, does anyone have a number lower than that? 12 and 19. Anyone have a sum that's lower than that? What do you have? 12 and 18, okay, anyone have anything better than that? So how many people have more than 35 errors total? Write down your total number of errors on your paper. Sum it up. We'll see what the distribution is. So how many people have 37 errors, raise your hand. Okay, 37, 36, raise your hand. 35. 34. 33. 32. 31. Seem like we're getting to like, we're going up, we're going up, we're doing a gradient descent in class. Awesome. 30. And that's it, right? Anyone have anything less than 30? Okay, that's probably right around the right answer. Okay, so I tried plotting this and my line was something like this, okay? But again, there's many answers. And essentially what we did is we just brute forced machine learning classifiers. If you think about this, there's like 65 people left in the class after a couple of drops throughout the semester. And basically all of you just drew a line and we got to see what had basically the optimal result. Now can anyone tell me what they did as a process? How did they go about it? This is actually quite a hard computational problem. How did someone actually figure this out? What did they do? Tell me your own behavior. Yep? I just kind of squinted on it and where it was. Okay, so you kind of squinted, like you kind of low pass go through the data set and did it by color. It's kind of interesting, yeah. For the right circle, I'll put the largest circle here and put the next circle in the largest circle area. Okay, yeah. So here's the grid. The equation might be to draw a point of that. Which I didn't find. Yeah. So you're not trying to find the best way. Yeah. Okay, interesting. And there's an ordering too. The blues are drawn on top. Yeah, okay. What else do people do? Yeah? I focus generally on the intersection of two points and not on the others. And then basically. How did you do that? The points which are grouped together, near the intersection, I try to put them on the right side, so that on the other side they will look more dense. The more dense the points are, it's better to keep them on the right side. So you sort of drew a course line, and then you sort of try to kind of squeeze it a little bit around to see if you can minimize it. Okay, that's interesting. What else do people do? Yeah? Just sort of looking at sort of the major orientation, and then trying to sort of... Okay. I saw people using tools, too. Who used something to draw with? Yeah? A little ruler? So what did you do with your ruler? Well, actually, I sort of started off more vertically, and then not using rulers, really, but I used a ruler to draw. But I adjusted my line kind of upwards a bit, because there's a lot of trees up in the middle, and then when I was making a cross, I realized the line was going to stay close to the next tree. Yep. And I saw someone using their ID card, and what they were doing is sort of just sliding it around to see if they could try lots of things. And essentially, that is what a lot of machine learning algorithms are doing, is they'll often find sort of the course that they would call this as a decision boundary, which is a very simple classifier. You decide which class is what based on this line. And what people are doing is finding sort of the course kind of quick fit, and then they're iterating, and maybe they're brute forcing, but they're sort of trying to find an optimal path by doing these kind of iterative and potentially smaller decisions. So you can actually kind of do big movements, and you kind of compress down to very small kind of tiny movements, and you can try to find an ideal one. And so what you've done is basically all the machine learning classifiers, you just basically are a machine learning training algorithm. You just trained a lot. If I took all your answers now, I could even mean all the vectors potentially. and I could find probably something that's pretty close to the optimal, or I could just take the max. Now, this is a very abstract example, but you can imagine this being real data. So imagine that we were trying to use two different features, so two different values that characterize a number. Imagine this with two different species of fruit, oak and maple. And you have the surface area of the leaves, potentially, is one dimension, so you go out and you measure 100 oaks, and you kind of draw a little shape around them, and you calculate the surface area. And the other thing you do is you measure the thickness of the trunk. So you get a tape measure, and you measure on the trunk, and you record that data. And then, of course, the first time you're out with someone who's like an arborist or whatever, and they tell you, this is an oak, this is a maple, and so you get your data. You get 100 of each. And you can plot them on this scatter plot like this, just like you did, but now we have to get a label, so it's a little bit red and blue. And the idea here is that when you come in with a tree that you do not recognize, and this could obviously be more complicated than oak and maple. Probably most of us would tell oak and maple apart. But you probably couldn't tell me the difference between like a hickory and an ash, or something like that, right? Or even two different species of maple, potentially, right? And so when you come in here and I drop a dot into this area, an unknown tree, I go out and I measure it, I measure the surface area of the leaves, I have no idea what it is, and the dot ends up in here. You would guess, based on the precision boundary, that it must be a maple. If a dot ends up in here, I would guess that it is a maple. And that's essentially what machine learning algorithm is doing, is once you've trained a decision boundary, or multiple decision boundaries, is that when an unknown item comes in, if you can measure the things of interest, you can get an answer, at least a prediction. That may be incorrect. I mean, there certainly are incorrect values in here where if we measured this maple, it would guess it's an oak, and that would be wrong. Maybe it was a horned one, or a young tree, or whatever it might be. Now, what you essentially calculated is what they would call a confusion matrix, right? So you filled out how many red circles are on the blue side of the line. So if you just ran this data through that decision boundary, if it was actually a red, but you classified it as blue, it would be an error, so it would go in this red box. 12 would go in here. And then if it was actually blue, but it got classified as red, you put your 12 or 13 or whatever in this box. And then of course the correct answers would go in the two corresponding boxes. If you sum all these boxes up, it would equal 200. Basically everything has to live in one of those boxes. If it was red and you classified it as red, it would be here. If it was blue and you classified it as blue, it would live in here. And what this confusion matrix does is it characterizes how errorful you were and your accuracy simultaneously. So you take the numbers, you put them in here, and then you take the opposite numbers, essentially, and you put them in here. So if we sum this up now, you can see we've got 82% of maple is correct and 87% of blue is correct. If we mean these two numbers, whatever, 84 or whatever, that would be your mean accuracy of this classifier. So it's not particularly accurate. Training versus classification. So again, just to make it clear, is that you need to train machine learning before. You can't just make a machine learning classifier that knows out of the box. You have to get the data in order to find that decision boundary. If I just gave you all those 200 dots as just gray circles with no differentiation, and I said draw a line between them, you'd say, line between what? It would just look like a gigantic cloud. So in almost all cases, pretty much, I think, in all cases, machine learning has to have the learning component. You have to train it first, and so that's the training component. And then after you've trained, basically, your algorithm, in this case, you've trained a line, then you can use that to guess something, and that's the classification component. Training and classification are two separate processes. And importantly, machine learning doesn't have to be just two classes, and it doesn't have to be two-dimensional. For simplicity, I rendered, if you have a piece of paper that's 2D, and I gave you a two-dimensional problem. But it could also be one-dimensional. So, for example, let's see if I can find a driver's marker. Do you need driver's markers sitting on desks? No? Hmm. Where is it? Oh, God. There's only Sharpies, which is the most dangerous thing that could happen. So, if you had a number of it, let's say it wasn't 2D but 1D, let's say this is just thickness of the trunk of this tree. And it's continuous like this. And so you can have, like, every tree that lives somewhere on the top. You hear it all day. You hear it all every night. You hear it all every night. You hear it all every night. You hear it all every night. It doesn't have to be black, it doesn't have to be red. The decision boundary would just be like here. It doesn't have to be a diagonal line. It just can be a single line. You say, if tree is greater than 22 inches, then maple, whatever it may be. And importantly, not only can it be one-dimensional, but it can also be 10-dimensional. Right? So, you can go up. And actually, generally it is multi-dimensional. So if we just take that exact same data set and we now add up third dimensions, not just thickness of the trunk or the surface of the leaf, then you can measure the depth of the root, like ground penetrating radar, or how far the roots go out. Now we actually have two clusters in 3D space. And instead of drawing a line through it, we draw what's called a hyperplane. Sorry, a plane, not a hyperplane. And now you can imagine your decision boundary, which would have been far too difficult for you to do in class, slicing through those two things. And it has the exact same function. If you're on one side of the plane, you're an oak. If you're on the other side of the plane, you're a maple. Now the reason why this gets super complicated, and why we use computers for this, is that as you go above three dimensions, humans sort of lose the ability to even visualize it in their mind. Like, I can't show you the fifth-dimensional version of this. And if I told you also that it doesn't have to be a plane, it could be an arbitrary shape. So if we just go back to 2D for a second, this is what I made you do, a straight line that bisects these two different data sets. But we can actually, if I said you could draw any line, as long as it follows a mathematical function. It could be log, it could be exponential, it could be a sine wave, it could be anything crazy. We've got an n-th dimensional plane. polynomial. Then your decision splits basically explode. Or you could draw an arbitrary line and then you sort of try to fit, so it kind of sort of fits into a Bezier curve or something to it. That's crazy. But of course, you know, linear lines, as we learned about in law, it's just AX plus B. You know, you have to find basically two coefficients, right? You can brute force it. We can have an algorithm take this data set and really just try all possible A's and all possible B's, like plus or minus a thousand at 0.1 increments, and we probably would have gotten a pretty good classifier. It'd be a stupid way to do it, but you could do it. The problem is you would look at like second, fourth, eighth order polynomials, because your number of coefficients explodes, right? So now you have eight numbers that you need to find in a huge space. Now your grid search, we would have called it, starts becoming too complicated. Now it's increasing sort of exponentially, and it would take you millions of years to try every single value. But the nice thing is you start getting really interesting shapes that let you do a much better job. So if you're looking at your sheet of paper right now, and you imagine trying to draw a curve line, you would probably be able to boost accuracy by 5 or 10%. Now it can't be something too crazy, right? I mean, it can get exotic, but it has to still be defined mathematically. And again, you can't just go to, you know, you can make it, you can just go to an nth order polynomial, like 50th order polynomial. You can probably fit a line to any arbitrary curve, or a reasonable approximation of it. But of course, that's going to be really difficult to find. So as you add complexity, you can do more complex shapes. But keep in mind that, basically, a real machine learning algorithm will find complicated decision boundaries at a high level. Even like neural networks are essentially just doing these, you know, when you have basically activation functions like a sigmoid, for people that have already studied this, you're basically getting these nonlinear behaviors. So essentially, you're always carving up, even in a decoding classifier, and sort of carving up this decision space. So this is a 2D example, but imagine how complicated this gets in 3D, where now you don't just have a plane cutting through three dimensions space, but it's some sort of like a manifold that's got this weird shape. So again, I can show you it in three dimensions, imagine how we have five... species of tree with these arbitrarily punctuating planes, you might see something like this, where all these kind of things are bisecting. So if you're greater than this on tree thickness but less than this on something and greater than this on something else, then I know that you're a hickory or an ash or an oak or a maple. But again, it becomes intractable for humans to do this extremely quickly. And so machine learning is really the science of having computers learn these complicated functions automatically for us, given the training. That's really the essence of it. So what you did is essentially all that machine learning does, but we're just taking it to the billionth power. So the other important thing here is thinking about making good features. Machine learning thrives on good features. You can't just... There are obviously some algorithms where I can just give it pictures of oak trees or give it pictures of maples. Much, much harder to train a classifier just given that kind of data. There are a class, obviously, at the forefront of deep learning research. You can train on all these different breeds of dog, and it'll tell you if it's a German shepherd and things. But those actually look quite different. A poodle and a chihuahua and a German shepherd look pretty... They're almost like different animals. They're as different as a tiger and a lion in my opinion. Maybe even more so. But if you're just like, here's an ash, or here's a northern great maple, and here's a southern red maple, whatever, you know, fake. I obviously know very little about trees. Then it's going to be very hard for the classifier to figure that out. So typically what you do is you try to find numbers that very nicely characterize something. So it's kind of a digested format. This is what the notion of featurization is. You take something that's complex and you turn it into some sort of representation. So for example, we could have a set of features like age, height, gender, if they wear plaids, their transport mode, and they have like 21 or 71 centimeters of mail. If they don't wear plaids, they'll have a bicycle. Therefore, we think that they might be an age. Or whatever. They've labeled themselves at a distance. But just as valid, it's just like a string of numbers. 1's and 0's, like, have they taken DECS, yes or no, right? Are they born in Pennsylvania, yes or no? And you're going to have a number from a questionnaire, and that defines maybe your own personality. So you have some sort of a recognition algorithm that recognizes that. And likewise, even for very basic gesture recognition, if you have a joystick, and they go left, right, down, that's your sequence of maybe course directions or towing my hand, and you basically are bucketing the hand into gross movements, maybe left, down, right is what a C is for you. And then likewise, if you have a sound classifier, you could run that at the peak, find the peak frequency and what the amplitude is, and you can say, well, that's an example of yelling, and this is an example of whispering. And again, you're not just typing in the raw on your waveforms. You're sort of digesting it down to something that's a singular number. And it's always featurization when you featurize something, all the kind of things in that set of things, yelling, whispering, talking, conversing, screaming, singing, whatever it may be. The vector, the feature vector, all the numbers that make up the features should be consistent. It can't be different lengths, because then you can't have like your, imagine sort of that three-dimensional scatterplot. You can't have for some classes it's three-dimensional and for other classes it's five-dimensional. They all should be in a common decision space. So let's actually try this. We're gonna do a class challenge. I want you to work in your groups. Imagine these are input signals for a smartwatch. Let's say, sorry, a smartphone. Imagine you have a smartphone that you can hold in your hand, okay, I gotta say it's on the phone, and it can recognize one of three gestures. So you can smack it like this, okay, you can wiggle it like this, and you can wave it like this, okay? And you're gonna, this is the accelerometer data plotted over time, so here's time, and this is basically the amplitude of the acceleration. Don't worry about all the particular details. Looking at those signals, they look different to your eyes. I want you to write down, brainstorm in your groups, angle, size, roughly four, how you would generate features, things that. characterize these things, okay? So, a couple minutes to figure this out. You're not gonna turn this in, just talk in your groups, now I wanna hear some answers. You're not gonna turn this in, just talk in your groups, now I wanna hear some answers. Okay, so, you're not gonna turn this in, just talk in your groups, now I wanna hear some answers. Okay, so, you're not gonna turn this in, just talk in your groups, now I wanna hear some answers. I'm going to try to talk to the kids. Yeah, that's a good idea. Yeah, exactly. I was going to say, I'm going to act like I don't know. All right. Oh. Oh. Yeah. Yeah. That's fine. That's fine. I didn't pick this. It's actually really good. It's really good. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Oh. 34 seconds. Oh. Oh. Oh. Oh. Oh. Yeah. Yeah. Oh. Yeah. Oh. Yeah. Yeah. Oh. Oh. Oh. Yeah. Yeah. Yeah. Oh. Oh. Oh. Oh. Oh. Like, the number of spikes in a particular second, suppose, like in one particular second there's one spike, in bigger there's three spikes, and in bigger there's like one, I think, 0.5. So what I would say is, the algorithm for this is you find all the peaks, like here, here, here, here, and then you find out the distance, let's say, between the first two peaks. Okay, so it takes me, kind of like, peak to peak time of the first one. The algorithm, yeah. Number of peaks is a great one. Okay. Okay, so the mid and max. Those would be two numbers. Right? Anything else? Yeah. Okay, so, what would that be, standard deviation? Mean, yes. Deviation from the mean, which is standard deviation. So, basically, the standard deviation is how different is it from the mean. That's a really good one. So, in this case, the mean would be sort of here, if I eyeballed it, and I have a very low standard deviation, because it's not very far from the mean. And these two would definitely have higher standard deviations. I don't know which one would have more. So, all those are good ideas. And I think if you really just took those five features, you'd probably get just a 90% accuracy on these signals. It would be my intuition. Okay, so now you have a sense of it. And, again, some people are talking about, like, the height of all the peaks. Or, like, you could just imagine generating the number. Like, this is, you know, whatever, 110 and 20. This one's, like, 50, 49, 52, 22. The problem is, you can't just give the list of the number of peaks, because the number changes. And so you have to have something that's consistent. So you could do the mean height of the peak, or the number of peaks is fine, because that's always a singular number. You could do, like, the variance of the peaks. So you have to always find a way to... Let's try another example. I will take you back to when I was a PhD student and I was doing that technology that I think I showed you two classes ago. We had a touch screen and we were trying to differentiate between knuckles and fingers. Basically expand the input on touch screens. And these are the signals. I will literally give you, you can steal my startup idea, please don't. Basically this is the idea that we had. Here is the example data. Here is the IMU data from a finger pad. And here is the IMU data, the hemicellarometer data from a knuckle screen. They look pretty different. And there is an added dimension here that I will also give you, the FFT, the frequency spectrum of that signal. So again, I want you to think. Come back into my shoes four years ago and see if you can think about what features could characterize these two signals. Same deal, think about it for two or three minutes. It's not a lot of talking for a brainstorming session. If you have similar ideas, you can add onto it. Any other questions? Thank you. Okay, does anyone have any new features they want to add to that list? All those features there would also apply here, certainly. But there's more features. So what can people tell me? Yeah? So you're saying you look at the frequency domain and you find the peak? A little bit. This one's more like a 5.5 and this one's more like a 7. They're a little bit different. They're related, but they are a little bit different. This peak and this peak are probably different. They're drummed up, right? I guess you're right. Maybe they are the same frequency, but they're different in intensity. So what would you do? So you define the value of the peak, and then you can imagine you have a polygon on a number line. That's what I thought was going on. Okay, yeah. So at a particular frequency, like 5,500 hertz, you look at the power. I like that one. Yeah? Okay, so you get some under the area of the frequency curve, and do some statistics on that. Yeah, absolutely. That's a good number. What else? Yeah? The second derivative of the frequency. The first one is almost constant, like the top one is constant. In the beginning, for instance. You're talking about the time or the frequency? Frequency. Okay, the frequency. So you use the derivative of the FFT. Oh, and the second derivative. Second derivative. Interesting. This one's got more change, you're saying. Interesting. So the acceleration of that thing. You can imagine the slope or the change in slope. Yeah. What else? Anyone have any other ideas? Maybe my startup is safe? We've learned a lot of tricks over the years to make this reliable. Okay. Well, you guys. So yes, we do a lot of the things that you mentioned. Another common one that's inexpensive to compute. It's often very expensive because we don't want to add to the latency on the touch system. Because, again, we already went through how long it takes to go up and down on Android to be able to classify an event. So we don't want to add much latency on. And so we try to avoid having to do FFTs or kind of frequency domain things because it takes time to compute an FFT. So another cheap one we use is what they call zero crossing. So how often does the line cross over the zero axis? That gives you a sense of sort of frequency. And we use that as well, I will tell you. I'm not revealing any concrete secrets. I don't think I showed you this, but you can also do letters on the screen. So W for web and C for camera. So it lets you basically do quick toggling. You don't have to go back to the home screen and back in. And, again, just devices on the market. Okay. Last class challenge. So here's another technology we did. This is a hand gesture recognition. So basically it's looking at the touch points on the screen. And it's guessing what your grasp is and, therefore, what tool you might be holding. So sort of iconographic grasps. So how you hold a tape measure, how you hold a camera. Very distinctive. No toolbars here. I hate toolbars. It's a waste of time. You already know how to use a mouse or how you hold an eraser. My favorite one is the dry erase marker, though. It feels so satisfying. We have a big version. That's not my favorite. Anyway, so all we have here, this is just a piece of software. It's just a stock iPad. There's no special things. There's no IMU. We actually only use touch data for this project. Here are examples of what those touch points look like. a single finger, lo and behold, it's a single dot on the screen. Nothing too exciting there. Two-finger pinch. We also just, anytime you see two fingers, we always assume it's a pinch. We don't do any interpretation. If we see more than two dots, our machine learning runs. And here are, for example, here are examples, and here are example clusters. Okay, so how you hold a mouse is sort of like this. One way you can hold a camera is like this. Also, four fingers, potentially. How you hold a white eraser marker, you can see it's sort of related to the mouse, but because it's a square, your fingers tend to, you can't hold a white erase marker like this, your fingers, you grasp it, they tend to align on one axis of the white erase marker. For example, so given this set, so what you have in your data set is a list, an array of all of the touch points, and each touch point has an x, a y, and a size, which I'm representing here as diameter. That's all you get. On an iPad, you get this for free. x, y, and the size of the finger, okay, the size of the contact area, okay? What I want to do is now think about, and I'm going to tell you what features we actually use to publish this research, is I want you to come up with a list. How you adjust these points and kick out a feature. Okay, so three minutes, brainstorming your groups, let's see how close you get to publish peer-reviewed research in four minutes. So this is not time-delaying data now. Just imagine this is a single snapshot. All you get is that. Okay? Okay.  So, who seems to have a good featurization idea? I already heard a bunch, so let's hear it. Okay, tell me a terrible idea. I heard a lot of good ideas, but I don't know if I can answer all of them. You could draw a box that would end in fingers, and then all the points would be drawn. And then basically the final box is sort of taking into the position of a magnitude somehow, somewhere where the average density sort of is. I guess like, where the point is with the amplitude, and where the magnitude is. Draw it to the box. Okay, so you're proposing, like, here's your dot. You put a box, and let's say there's like a dot up here. So you put a box around it, the center of the box is there. And then what you do is you compute the center of mass of all the things. It's probably like here, and you compute like the distance. between those two things. We don't do that, but that is an interesting way of characterizing it. Number of clusters, and what you're going to do is you're going to clusterize it, and then you're going to add the number to it. That's a nice, easy number. The other one I will add is number of points. You're never going to hold a marker with all five fingers, and it's going to be very difficult. I guess it's possible, but it's unlikely. Nor are you going to hold a camera with one finger. So, number of cluster points, you definitely use. What else? Yeah? How do you get one number? It's got to be a singular number. You can do maximum pressure, or no, or e, or some variable pressure. We'll do all those things next. We can do math, we can do map, but it sucks. We kind of go to town. You basically hit all the points. All the sides, so that's 2, 8, 12, 14. Again, doesn't matter if you have one finger on the screen or 20 fingers on the screen, you can always find that the maximum average of the sides is only one. That is your average. So you always get one number irrespective of the number of points. So, yeah, these are interesting numbers. Well, I know you have a lot more ideas. How would you describe that as unmixed? Okay, so you're saying you find sort of the center. And then do you cluster or do you don't cluster? Okay, so there's a cluster here and there's a cluster here. And then you're saying you find the distances between the clusters. Okay, and what would you? Mean? Distance? Okay, so mean. Mean! Mean clusters. Okay. You guys are getting more complicated than you need to. What's the simple feature you have? Just a little area. Total area is beautiful. And that's just the sum of the sizes. Okay, what else? Yeah? So how would you characterize that as one number? Would you sum the distances? Because there's something here that you think about, like, when you have two fingers, those are one distance. But if you have three fingers, there's three distances. So you need to take an arbitrary number of distances and compress them into one number. Average. I love average. Okay, the reason average is because you find, if you knew all distances, right, all pairs, all possible pairs, you need to find the average of them. And that would be an enormously valuable feature. I'll show you one example. So if you compute all the distances in marker, so there's this one, there's this one, there's this one, it's really small, because you can't hold a marker like this, it would just fall out of your hand. Where a camera, because it's inherently big, if you take all the distances, there are some tiny distances, but if you take the mean of those distances, it's going to tend to be a lot larger than a marker. It's a really simple method to compute. Turns out it's extremely, extremely useful. Does anyone else have any other ideas? Yeah? Yes. So what we can do, we can take those all distances that we... So they can add on to the average of the standard deviation. Anyone else want to add on some features? Awesome. The magic is gone. Vega 3 is burning out. That's it. All ideas have been sucked out of you for this semester. Yeah? But can you apply it? You could, but keep it simple. Yes. You could take a look at movement, but just imagine it's instantaneous. Final call. You guys only got about a fourth of the features that we ended up using. No final guesses last call? I'm going to ruin the surprise. OK. So here's what we actually used in this research. And you could have guessed all these. None of these are really that complicated. So, number of touch points. Easy one. Really distinctive. OK? The total touch area. That's the sum of all the sizes. Really useful. As you put more fingers on the screen, you cover more of the screen. Very distinctive how you hold the tool. This one's a little bit fancier. The principal orientation of the point cloud. So what it turns out is that you can basically assume that you fit an ellipse. I'm not even drawing a circle or an oval around any of these. What is the orientation like? So basically, sorry. It's the width of the oval versus the height. So the principal axis of the oval is the long one, and then there's a minimal axis. So this one has a very high ratio. The oval, you kind of fit an oval like this if you drew it around, and it's much longer than it is wide. So it has a very high ratio, like a 5 to 1 ratio. Versus like this, you put an oval around this, it's going to be mostly sort of circular. It's going to be more like 1 to 1. Inherent. OK? So that's one of the features. OK. Then, we do four sets of data. So we compute four lists of numbers. One is distances between all pairs of points you guys came up with. See this computing? It doesn't matter if there's only two fingers on the screen or 25 fingers on the screen. You can find all the distances between all pairs of points. This is like AC. n-squared algorithm, and you can calculate all the distances. We can also do the distance from each point to the center of a point. This is actually the first feature that you guys sort of came up with. What you do is you have like an array that sort of looks like this, and what you do is you basically do the, kind of look like a boundary box where you take all the x, y's and you zoom them, and you put the center of mass sort of here. And then we do the same and calculate all the distances to the different points. So very similar to the distances across the different points, okay? Then we do angles between consecutive clockwise points as measured in the center. What we do is we start off with an angle, and we basically look at the angle. So we look at the angle of this one, let's say it's zero, then we look at the angle of this one, we look at the angle of this one, we look at the angle of this one, we look at the deltas, okay? So we just calculate, it's like 45 degrees, 20 degrees, 12 degrees, 0 degrees, okay? So the angle between. So it tells us basically the angular path. And then finally the sizes of all the points, so 2, 12, 8, 10, as many points as you have. And for each of those lists, those four lists of just numbers, we calculate seven statistical features. The mean, the median, the min, the max, the standard deviation, not 7, 5, 6, 7, 5, 5. So if I have a list of sizes, 2, 8, 12, 42, 96, whatever, I just have to use mean, I just have to use deviation, find the minimum size, find the maximum size, and I just make it into a gigantic list of things. And this is what we used, and in the end it brought us up to something like high 90s accuracy, by just taking single frames of temperature. Every single time on the screen, if there was a point greater than 2, we would just run our machine learning algorithm, and it would classify the instantaneous gesture on the screen. And it turned out to be pretty reliable just for this. And I think if I gave you more time, and you really, if I made this a homework assignment, I'm pretty sure a lot of you would come up with very similar sort of features, and some new ones. And again, it doesn't matter how many points on the screen, we should get. So let me actually just show you, in a very rudimentary piece of software, one of the ways that you can do this. So in this case, we're going to... I'm going to be using the bane of my existence because it's an HCI trainwreck. I'll show you in Weka. How many people have used Weka? Okay, good. I'm glad that so few have had to live through the agony of Weka because it is an atrocious piece of software. Let me see if I can boot it up and I'll show you this software. Okay, let me mirror my screen to make it easier. Close all my salacious emails. Okay, so here is Weka. I apologize it's so small. Okay, so you have this little interface here. It's actually made in New Zealand. That's why the Weka is up on the slide. We're going to go to Explore. We're going to look actually at the data that we collected for the thing that I just showed you. I'm going to go back to my file system. Choose. We're going to open a file. Again, don't worry about this interface. I'm just going to take this TouchTools example. It's a CSV file. Okay, so let me just actually open this up to demystify what's inside of this data set so you can actually see how this is not complicated. I'll open this up in Excel. Okay, so every row of this data set is data that we collected by having people basically touch an iPad. And every column is a feature. So let me just zoom in here. Okay, so here is all paired distance. So this is the all paired distance. I guess I can't see what it is. Oh, sorry, all paired distance mean, median. So this first column, A, is all those distance pairs, the median of them. And for whatever this one was, it was 130.2. The next one was 122. Then we have, for example, all-pair distance mean, standard deviation, minimum, max, all the things that I talked about. Here's the centroid distance, so this is all the distances between the centroid and all of them. And you can see they just all have different numbers. Now the important stuff is at the end. Don't worry about the zeros. At the end, we actually get to the class layer. So this is the ground truth, this is like the folk versus the native people. And so this particular example, it was the whiteboard eraser. So those numbers are trying to characterize the whiteboard eraser. And if we scroll down here, and I'll post this data if you want to see, here's the camera, here is the mouse, here's a magnifying glass. Actually, not a huge amount of data. Here's the pen. And again, if we kind of scroll over, you'll see it has these different numbers. I can already see, but remember before it was like in the 120s? If you do the pen, the minimum, the mean distance going there, that's way smaller than the white erase marker, because this is a big gesture. Tiny gestures, okay? But basically, you can open up this file. I'm not actually sure why these are all zeros. That's kind of weird. Probably some sort of error. Let me actually see, if I go to my all.csv, is it different? All.csv just seems like it's a blank thing. Okay, in fact, what I'll do to make this even easier, is I will just delete all of these columns, so that we don't have that data. Okay, save it, continue, close. Okay, now what I'm going to do, is I'm going to open up this CSV, which only has like, it goes up to, oh, so how many features is that? Not many features, okay? We're going to open this up, if you already opened it up. Okay, so let's zoom into here. So here is the WEC interface, which sadly is just like, a tour de force of widgets everywhere. It's not, it's sort of like I should put this in the first lecture of classes, because all this interface is not designed for, no grid system. It's just like, this is like a tab, like big slide or something like that, kind of crazy. But anyway, the important thing here is, if you recall, it's very nice because there's a safety button. It has all my features, and what I have is all the different classes are labeled in different colors. So if I go down to the very bottom. I go here, the different class labels, again it's a very terrible interface, but these are my different class labels, so this is, so number one with blue is the whiteboard eraser, red is kind of camera, mouse, magnifying glass, and small eraser. So let's just remember too, that pen is in pink, and the whiteboard eraser is in blue. So now, if I go and I look at a feature, like let's go to all pairs mean, so this is the mean of all those pairwise things. So we know that the pen is tiny, and so if you look at the number, this is just one feature, so it's like a line. It's just a line, just like a number of them. You can see that the pens, the pink ones all tend to cluster towards the left, and the blue, so in this case it's a white erase marker, tends to be kind of here. I forget what red was. Camera or something probably tends to be bigger. So you can see that if we drew a line, sort of right here, most of the pink would be on that side, and most of the blue would be on that side. But you wouldn't be able to build a cluster that's 100% predictive of pen versus white erase marker. So we have to find, oh, so we could still draw the line here, and that would get us to like 80% accuracy, but then we have to find another feature that pulls them apart. But one thing you can see here is that green, whatever green is, is like right between two. If we put another line here, it wouldn't help us differentiate green from pink or blue. So we have to go to another pair to find that out. So here, pink and blue are sort of the same. Here, pink and blue are totally stacked, so the minimum distance is identical for both. Now let's go to centroid. Still, pink is sort of on the low side and blue is on the high side. So let's find one where they're totally independent. So here's actually a pretty good one for green and blue. You can see that most of green is over here, and a little bit of blue is over here, and pink is nicely separated. If we drew a line here, just that one line feature alone would probably get us like 95% accuracy, just separating pink. distributions of these different, how the different classes align on the number line. So here what's interesting is green now is offset, it's actually quite away from blue and it's sort of on the right side of pink, that's kind of interesting. And what you want to hope to see is that in some pairings, that there, even though you make them one, there may be confusion, is once you stack enough of these features together and you draw that sort of plane through like 10-dimensional hyperspace, that it will separate them all, right? So you kind of build up, each of them are kind of individually a weak classifier, but when you use 20 of them, they all basically give you high accuracy. So here, you can see pink is still sort of overlapping a little bit with blue, you can find a really good example. So pink is very separate from blue here. So in this particular case, the numbers are, I don't have all the features in here, and you can see it's not particularly sort of amazing. But anyway, you get the idea that even with just these, like 20 values, there is some separation between different classes. Like red and blue, so in this case, red and blue are totally stacked on one another, so this feature, all distance pair standard deviations, would be no help for differentiating red and blue, but in other cases, red and blue are quite separate. So okay there, really good whatever this is. Centroid distance median is a really good feature for separating red and blue. Now what we want to do is have machine learning figure this all out for us already. We only have a thousand pieces of data, that's actually pretty reasonable. So what we're going to do is go over to this next tab, which is classifying, and I'll just select a really basic algorithm. We'll use a tree, which is sort of similar to what you did. We'll use this one called J48, sort of run-of-the-mill. And what I'll do is here, I'll just hit start, and now it's going to figure it all out for me, okay? So I just ran the algorithm and it's built a decision tree. In fact, if I scroll up here, you can actually see the decision tree that's created. So the very first split in this tree, so basically everything is gifted, okay? every number line is treated separately and you go down the leaf. So the very first script it does, which is often the biggest, it says the centroid distance mean. If you're less than or equal to 80.9, you go down the left-hand side of the tree, unless you're down the right-hand side of the tree. That's the first script. Comes in, does no idea what this thing is, it's going to go down the left or right. And what happens is as you work your way down, the if statements get basically bigger and bigger and bigger. So then the next thing you have is the angle of difference mean. If you're less than 1.5 radians, then you go down this range, and eventually what happens is you've worked out enough to where you get to the end and actually predict it. So here is the final innermost leaf. It says all characteristics mean. If it's greater than 127.3, it probably says white erase marker. So it actually outputs a prediction. So if you think about visualizing this is that it takes a number line, chops it, says go left or right. Then it takes what's left, chops it, goes left or right. And it can use different features until you basically end up in a bucket, where hopefully your prediction is correct. And it's figured out all these sort of breakpoints for us automatically. And if we scroll down, it actually did what you guys did in the beginning of the class, which is basically figure out the confusion matrix. So this is what it actually is. And this is what it's classified as. So in this case, it's destroyed itself. Oh my gosh. Okay, here we go. So in this case, whiteboard eraser, which is labeled A. It was classified correctly. This is what it actually was. So this is what it's classified as. 283 times, it was correctly labeled as itself. That's good. That's a classifier. 283 times, yes, it was whiteboard eraser, when it actually was a white erase marker. But 7 times it thought it was a camera, which is B. 11 times it thought it was a mouse. There was a lot of confusion here. 28 times it thought it was a magnifying lens. It never confused it with pen. And one time it got confused with a small eraser. And what you want to see in this confusion matrix is the diagonal line So this is B classified as B, this is mouse classified as mouse, magnifying graphs as magnifying graphs, and as head, small ratio. And you don't want to see too many numbers here. Overall, you can see the classifier is able to correctly predict the mouse as themselves. Because again, we're using that training set. We know what the label is, it sort of hides the label from the classifier, then compares it afterwards. I'm not going to get into all those details. But you can see that we did pretty good. So if you actually scroll up here and look at what our accuracy is, it is 91.8%. So with those A through O columns in Excel, and just using those very basic features, it was able to differentiate them at about 92% accuracy. I think if I had the full list of things, those columns of zeros that I deleted, it would probably be in the high 90s. So this gives you a sense of this. Now the other kind of interesting thing about Weka, it's kind of horrendous, but if we go under more options, we can actually go down here and say output source code. There's a little checkbox here for output source code right here. So I'll hit OK. And if I run this now, you'll actually see it's going to kick out all the Java code for this entire classifier. So you can really copy and paste this into your code, and it'll actually output at the end your prediction. So if you give it the feature, if you give it the vector, if I go to the top here, if I look at the first thing, you give it an instance, which is basically just an array of values. And then it'll actually return 0 through 5 based on the prediction. So it's kind of crazy. And if you actually look at how the functions are, they're literally, inside of each one is just a GIF statement. So it recalls a function to a function to a function, and you kind of go down this tree of branches, and then it returns kind of recursively up the stack your answer. Very simple. It's a gigantic blob of Java code, but you can really copy and paste this into your code. We're now going to look at kind of clustering. There's also some interesting little visualization tools that basically give you scatter plots and everything. So we can kind of go in here and look at, for example, why is it so terrible, so terrible. So here's some interesting ones, let me see if I can zoom in. So you can see, what you want to see here is that things are differentiated. This is sort of like that graph, so you can see, again, whatever these two columns are, this is like, leaning to this angle. You know, blue sort of lifts here, and red sort of up here, and you can imagine doing the same sort of task of fitting that line to carve out all of them. So again, on the left-hand side over here, are all the dimensions, all paired to the max, and so on here, that would be nice to see. All features cross all features. And if you can see with your eyes that the clusters are sort of distinct, then you know it's a good feature. If you go to things where they're all overlapped, you know it's not going to be very useful as a feature, because it doesn't give you any discriminative power. Okay, let's keep marching along here. So that is a very, very, very, very kind of whirlwind tour of Weka, you can download it, it's free, it's a horrendous interface, but I'd really like you to play around with some descriptions so you can skip algorithms. I chose that J48 tree algorithm, but you can also choose, like, multi-layer perceptron, which is sort of like a very basic deep learning. You can choose, like, so for a vector machine, which is very common, there's like a million different algorithms you can try, and you can just run them instantaneously. Let me see if I can actually do a multi-layer perceptron really quick. So if I go to classify, I'll choose under function, multi-layer perceptron, it's going to hit start, it's now going to train this little net, this little kind of network, it's much slower, there's many more parameters, so it's much, much slower than the decision tree, which is pretty fast, but that outcomes all of the weights, sort of different, the same way that's sort of different than MetaX8, which is the other one. You can see that neural network was 94.4% accurate, so that's better than our 91.8, so it turns out a little tiny neural net does do better than a decision tree, and it was as easy as clicking that button and running it. Let's see if the really kind of workhorse, especially in my lab, is SMO. We'll see how SMO does, 92.4%. Ah, so deep learning wins again. So, pretty good to see. And if we look back, actually I go back to this number, we go down to the, this is the confusion matrix for the multi-layer perceptron, and you can see that it is indeed, but there are a few numbers that become, there's occasional misclassification of ones and zeros. There's still a lot of confusion with wide-array smarkers. And actually, sometimes this helps you do, design your gesture settings. If you see wide-array smarker often being confused, in this case with D, which is magnifying glass, is you may say, maybe we should pick a different type of tool or different gesture so that we know there's confusion in the way people perform it, maybe we try to find an alternative gesture to do something in. You can do sort of an iterative gesture design in loop with your machine learning. Yep? Like I just said, so we thought about tools that would have a practical use in a touch screen, and then sort of try to find ones that were unique to them. Because there are other things that you grab like this, like how we have the pen gesture, this is just a precision grab. You would hold like a little eraser like this, you might even hold like a compass like this, so we did have to be a little bit selective in kind of curating our gesture set. So actually the funny thing about this thing is that it makes you a really good demo, like it's very visual when you see it on screen, but if I just brought you this iPad and I told you nothing about it, it'd actually be very difficult to figure out how it works. So they would say that in HCI that it's discoverability is really low. Once I show you how to do a dry erase marker, you sort of know it's there, but if I'm like, oh, this is a touch interface, you just kind of do crazy stuff, you see things popping up, but it'd actually be very hard to learn the mapping. And that's actually a major downside of that technology. For that very good reason, it hasn't shipped on any devices. We try to convince people, but they're just like, ah. The only thing we may have convinced people is that just include one gesture, which is white erase marker. Still use a single finger for drawing, but it's very annoying to have to, you have like a smart whiteboard and you go to like pen mode and you draw, and then when you want to erase on your $10,000 smartboard, you go back over to erase, and then you erase and you're like draw, when you couldn't have a gesture to do it. And we think that the dry erase gesture is actually pretty iconic. Okay, so that's Weka, I already pointed to that. So that was a decision tree, kind of interesting. You can use it in Eclipse very easily, you can even use it in processing relatively easily. It's basically, I'm gonna give you a code again in class, I'm gonna post it online, but basically you can just make this new Weka classifier, like a helper function, and you basically give it training data, so in this case, like training data.csv again, it has all your features, it has headings, your features, and the last, rightmost column is just the gesture, or whatever you wanna classify, and then it'll actually train a machine learning model automatically for you, in this case, I think the default is SMO, and then you can just pass it new distances.inco. So let me actually just show you what this looks like really quick, if we have enough time in class, I'll just show you. So here is Eclipse, I apologize for my screen just a tiny bit on purpose. So here is just some very basic code, this is written in processing, but I've done it in Eclipse. So in this case, I have class, so it's a quiet, whistle, talking, class, clacking, class, cheering, I have minimum, which I showed you before, for audio processing, in this case, I'm writing all these features as a file, and I'm doing it in FFT. So this is no machine learning here, this is just basically creating a file output right there, and I'm writing these things to disk. In this case, I'm just writing the FFT to disk. So let's actually try this, every time I hold down the space bar, every time I hit, hold on, a key, it'll collect data, so let's just record some data. So first I'll record quiet. And it's 100 instances of quiet. Okay, so let's have everyone whistle, or I can whistle. ♪ Whistling ♪ Okay, everyone talk, talk. Okay, talking, yeah, clapping, talking, talking, clapping, yeah, yeah, yeah. Okay, clapping, let's mess this room up a little bit. ♪ Clapping ♪ Cheering. Big up, Bree! Yay! Wow. Brutal. Okay. So that just saved all the data. I'll post the CSV. Now let's go to the classify lock. So what you'll see here is I'm going to make this new classifier. I've called it, I've outputted this training data to CSV. Then I'm going to open up Minimum as Usual. I'm going to do 10 of these for it. And the last thing I do is I go to classifier on Classify Cluster, and then we're going to just pass in the 10th key raw, and it's going to output against. So we'll see it. We trained that super fast. Let's see. So it's going to take the raw FFT, it's just going to shove it into the classifier we just made. And we'll see how accurate it is. So quiet. It's going to be talking. It says talking. Here, I'll bring that. Talking, talking. You guys talking? Clapping? Cheering? Yeah! So we've built an example, and it's already machine learning. We're out of time. I'm going to post all this code. So if you want to run exactly this, you can. It's literally like 200 codes to start playing with machine learning. I'll see you on Thursday for the bake-off. If you need a phone, I have a couple phones. I don't need your sheets. Just trash them on the way out. Sorry. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Thanks again. Bye. Bye.\",\n",
       " \"So, does everyone like to class early on spring break? I should have been off this today. I should have relished in after class being missed. Alas. Well, it'll be a more intimate setting. news is that because I'm releasing Bake Off 2 today, you're going to get to meet your team, but almost certainly every team is going to be missing one or two people. But that's okay. That's why I'm going to give you three weeks to do it so you can nag on them during spring break. So just some quick items is I'm going to send out Pierogy 1 today. So this will encompass the work that you did for Bake Off 1 and also for our homework 6, I think is what it is, the fifth law homework, and also your video prototype. So you're going to evaluate your team on those three things. This will be the longest time you'll spend on a team because we're going to pick up the pace for the rest of the semester as we do four Bake Offs. There'll be a couple group homeworks as well, but Pierogy 1, the longest, will be with a set of people. The peer review is pretty straightforward. I don't need to explain. It's pretty self-explanatory. It's a Google form. Answer the questions honestly, frankly. There's an open comment field where you have to type in some details. I do look at all the scores. So if you just write, you know, it's like one to five scale or one to four scale, if it is like four, four, four, four, four, four, four, you instantly get flagged. Like, you just come up to my spreadsheet like immediately with like suspicious numbers, and I will email you with clarifications about everything. Importantly, I'm not expecting everyone on the team. You're going to do, you're going to submit the form once for every team member, including yourself. Don't forget to submit once for yourself. You're going to praise your own work on that team. And the key is to be honest. And I'm going to look at how you evaluate yourself relative to what your peers thought. And you're not meant to be a four in every call. If you did a hundred percent of the coding and a hundred percent of the video and a hundred percent of the user testing and a hundred percent of the ideation, that means that your team was totally dysfunctional, right? I will not believe it. And everyone will get like basically points off. The key is to be honest and show me how you distributed the work. And if you did all code, that's great. But I very much doubt that you worked at a hundred percent, like significant carried the team level contribution in every category. So the key is to show me where you did contribute and where you didn't contribute. And you're going to, you can get full points if you normally contribute to sort of like two or three aspects of. the assignment and it's like six or seven different aspects. So again, it'll make more sense when you see it, but just answer it honestly as possible. There will be new teams out. In fact, the teams are already formed on campus. I'll show you what those teams are. I'll show you all at the end of the lecture. And we're going to talk about Bake Off 2. Just very briefly, just to get the ideas percolating, I don't want you to spend all of your spring break working frantically on Bake Off 2. I want you to enjoy spring break. I'm going to enjoy spring break. You guys deserve a little bit of relaxation, but you might as well plant the seed. So if you're just sitting there, you're sipping on some tea, and you're watching the snow drip by, I've got a brilliant idea for Bake Off 2. That's the sort of relaxing ideation that I want to see. Here are your new teams. If you're on a team, that is, we randomly shuffled it, so if it's the same people, then it's fatal. I think you guys should consult your horoscopes. There's an attraction that's let us happen because the universe is speaking to you. If you're with one other person, that's probability, I guess, reasonably, but the same three people end up in the same team. It's definitely something very special. So I will put this back up at the end of class, and you'll get to meet your team. It looks like a couple more people are here. I also found an interesting example. It's a terrible photo on my phone, but I came across this elevator, and it really just like aided my HCI core. So this is what happens when you're in the elevator. This is what happens when you look like from the inside, but typically you'll go into the elevator and sort of lean against the back wall. The first thing you do in the elevator is you kind of reach for the button, right, and then you walk into the elevator. This is what you see when you first walk into the elevator, and again, sort of like in that email address in Home Depot, right, like the most common symbol is the at sign in every email, right, or maybe like period or something, or a vowel maybe. The most common button in the elevator is going to be 1, right, like everyone probably, you can go to a lot of floors, but most people are going to go to 1. most important button. And they very conveniently hid it behind this card reader. So when you're looking at it as you walk the elevator, there is no 1 key. It looks like it starts at 420, which is here. The 1, 2, and 3 key are sort of opposite, especially 1, which is like under here. You can't even see it. I was like, what? This only goes to 20 to 40 or wherever? And then they kind of like reach around and it looked like I was only like 3 feet tall before I actually saw the 1 button there. You actually basically have to slide up against the wall of the elevator before you can actually see the 1 key. Again, their whole industry, their entire business is making elevators and they can't even do that well. I don't have any knowledge about elevators, but I would have designed them very differently. So it just shows you that again, it's not common sense. These people that have probably installed hundreds of elevators, they're like ruining people's lives one little second at a time because they can't actually execute their profession well. And it's not because they're evil or even lazy. It's that they just don't think about the design process. So this is another great example of a design fail. If you can't see what it looks like in the lecture notes, you can zoom in and see it. No, it's horrible detail. Before we get into today's lecture, I also wanted to go back and revisit. I didn't quite finish my slides from last lecture, but I wanted to talk about two other pitfalls that can happen that you should be cognizant of when you're doing sort of need finding and sort of like survey research and so on. When you're talking with people, there's other things that can happen. So first, this setup, which was done back in the 20s at this basically like a Western electric, like I think, what's the weather? Chicago. And they ran this study in the 20s to basically see if they change the level of illumination on the factory floor, if that would make workers more productive. So this is sort of at the beginning of electric lighting. So it's sort of like, oh, we're going to move you from a 40 watt incandescent to like 100 watt incandescent above your work surface at the factory. Is that going to improve workers' efficiency, right? So we can give this to our 1,000 workers, but it's going to cost them light-bulb drops and more expensive back in the 20s. But if it makes them 5% more efficient, is it going to be worthwhile? Very logical question. Definitely good. These factories back in the 20s were all very dinty and smoky and so on, so it'd be nice to have this. And what happened is, when they did this experiment on just whatever, 20 workers, 100 workers, it didn't look like it played for the entire factory, is the productivity seemed to improve when changes were made. It's like they swapped out light bulbs, put in a brighter light bulb, but then after the experimenters left, basically productivity returned back to normal. So what do you think might be happening here? So now they've put in a 100-watt light bulb, they're 5% faster, and two weeks later, the factory is back down to regular speed. Well, there's some theories. Yeah? Did they know? Well, they know that their light bulb changed, right? Right. Did they know that it was like a cat? I mean, they knew that it was like, even if it was flip boards hovering around them, but they probably would have guessed that, yeah, it was a cat. Yeah? Getting an outfit. Say again? Getting an outfit. Getting an outfit. So they probably would have guessed that, yeah, it was a cat. Yeah? Getting an outfit. Getting an outfit. Getting an outfit. Getting an outfit. Say again? Getting an outfit. Maybe, so you're saying that after they've adapted to the bright light, they sort of fall back into routines? Yeah, possible. Yeah? Maybe they saw this tremendous taking out of them that they thought it was like this and then after they... So how would that change their behavior? So I want to see how well you're doing on your bake-off, I'm saying like shadow your bake-off team, and if a professor is going to follow you, how would you change your behavior if I did that? The color of your lighting. Yeah, right. So basically what's happened here is, this is called the observer effect, or sometimes it's called the Hawthorne effect, maybe after the factory where they discovered it. And it's basically that there's a motivational effect that happens, not because of what you've changed, like the light bulb, but because they know that they're taking part in an experiment. So if you're a factory floor worker... And you know that they care about things like how many doodads you can assemble in an hour. If there's people buzzing around you and they're like changing your workstation and changing out light bulbs, it doesn't matter if you change it even to like a 20-watt light bulb. If you make it darker, probably their efficiency would go up, because they're under scrutiny. So by active observing something, you're going to change it. Sort of like a, whatever, like Schrodinger's cat or whatever. It's very impossible to know if the effect you're measuring was induced by you simply measuring it versus the thing that you've implemented, right? So you show people a new version of the Google homepage and they do more searches on it. Is that because it's a more efficient search engine or because, you know, they know that they're part of some sort of study and it's a beta test and they want to see how the new Google version works, right? Very hard. So there's no real way to solve this other than trying to be minimally invasive. Like, you know, swap the light bulb when they're not there or do it over a long period of time. Like the ways you can see if the observed effects go up is don't run that study for two weeks. Run it for one year and see if it sustains at a 5% improvement over time. So again, you just have to think about your protocol, like your study design, how to combat the Hawthorne effect or the observer effect. In the same sort of note, another effect that's really important to notice that's kind of related is the novelty effect. So, can someone tell me what a novelty is? Like the noun, a novelty. Or the adjective, novel. Yes? Something different or strange. Something different or strange? What else? What else is sort of like a connotation if you give someone a novelty? How many people have heard the word novelty before? Okay, so you speak English. Give me, what would be an example of a novelty that I could give someone or if I took them to a place where there's lots of novelty? Like a carnival maybe? Like a weird teddy bear and stuff? So novelty normally has this playful effect. There's something new and strange, but it tends to be sort of like, not comedic, but sort of playful often. Right? It's like a little toy. But when something is novel, it means that it's new. Right? So it's just a fancy word for new, essentially. And so can anyone guess what the new effect might be in a study, or when you're talking to other participants? And how might it affect your results? Yeah? Does it have to be something like a new way of getting more like, OK, it's just a new process. They may be more focused. How about if it's a thing? Like I give someone a new thing, like an iPhone 12 that I've secretly stolen from Apple. What do you think their impressions are going to be? Yeah? They're going to have more positive reactions, just by like, the ritual of taking a few of these and then buying it. Yeah. So partly they're feeling special. But just because it's new. You give them something weird, strange, and new, like you just, oh, wow, it's a new product. Doesn't matter if it's like really that much better. It's just a new, shiny thing. They're going to tend to rate it better than they would if it was something very commonplace. So for like, in my research, I did a bunch, many years of research on on-screen projected interfaces. And these have like a very kind of sci-fi, kind of flashy look to them. And I asked participants at the end of my studies, you know, in this technology, you can't quite see it in this clip. But basically there's like this big armband with this stick coming up. And on that stick is like a projector and a computer. So it's like a really super gangster prototype. Like no one should ever want to use this. But I asked my participants, would you trade your smartphone for this technology? And everyone's answer should be no. This ran no apps except little toy apps like this, which was written in processing. And like couldn't make phone calls or anything. But like half my participants were like, yeah, I think I really would. That's a bogus answer. And it should have been a bogus answer. They were like, ooh, this is just so cool. I want to take it home and show it to my friends. But it's a totally useless research prototype. And so we asked them even on like a bike. scale, like would you want this right now, highly yes, very much so, to not at all, it's going to be very skewed. Not because it's not useful or useless, but just because it's like, ooh, like sci-fi, like there's sort of a cool thing about it, right? So you have to be careful that when you're showing something that's futuristic, especially, that it's going to suffer from novelty effects. And what you probably need to do is not ask them qualitative things. Don't ask them, do you like this experience, or how much do you want this, because you're going to get false answers. You should stick to quantitative measures like how accurate it is, and how fast it is, and how heavy it is, and all the sort of computing metrics that we can measure in a quantitative way. Are those buttons easier to touch than a typical, like an iPad, right? That would be a good A-B test to make sure that the technology works, not an evaluation of how much people want it. That's a different question entirely. Okay, so novelty effect, problem effect. Okay, but that is not what we're talking about today. We're talking about our first lecture on visual design. We're going to cover some of the kind of big topics in visual design. We're going to end with our first sort of little bit more of a deep dive into color and color perception, which is obviously a big part of visual design. We're going to have a couple lectures on visual design. Okay, so when you look at a scene like this, okay, I think this is like Hong Kong or Shanghai or something, but what is happening? What are your eyes having to do to parse this scene? You're doing it right now, so let me just describe what your eyeballs are doing. How are you scanning this? Right, let's color first. Okay. Is this an effective way to lay out information? How many people think this is like a pinnacle? How many people think this is the worst? How many people think it's not great? Yeah. So I mean, it's very messy, right? Like you're looking all over the place. There's a lot of information. There's a lot of clutter. It's unclear where to look, right? It's not ordered in any particular way. Everything is competing to your attention, right? So it isn't like a hierarchy of information. Every little business on this street. wants to grab your attention so you know to go to like this restaurant versus another restaurant. And so this happens all the time in the real world and it's sort of competition, but it also happens in the digital world. So how many people have seen a friend's desktop or your desktop that looks like this? I've definitely seen faculty desktops that look like this. And this has the same effect, right? It's like, it's not sorted in any way. It's just like a blob of icons. I guess it's sorted by, is it alphabetical? No, it's just like a blob of files that probably came in in time order. So how would you find the file that you're looking for here? You may do like a text entry, so it's like you hit the letter on your key to try to get closer. But most people, like I know, like my kind of parents, they would just scan this, right? They would literally just look over the whole thing and try to find where their file was, right? And that's a very inefficient way to do it. So this brings us to a very famous sort of HCI quote, you know, Herb Simon, after which Newell Simon Hall is named, you know, CMU faculty, I forget if he won the Nobel or the Turing. See the Turing guy? I think it was Newell won, maybe Simon won that. No, I don't know. No, Newell won the Nobel and Simon won the Turing. Anyway, but he made this very, very nice prediction, very, very far back, right? Back in the 60s, where everyone at that time, you think about computing, remember, think back to our early lectures. Computing in the 60s were like these gigantic machines, you know, very few people could touch them. It was like batch processing era. And everyone was like, we need faster processors, more memory, you know, faster printers, blah, blah, blah. And basically Simon said, you're all wrong, right? That's all going to fix itself over the next 10, 20, 100 years. The scarce resource, when you're talking about information, is actually human attention. So he sort of figured out Moore's law really early on, and basically said, I know that computers are just going to be able to solve this. The really scarce resource in a computing system, or an information system, is going to be human attention, which is really kind of the... a clever thing back, especially back in the 60s when computers were really slow. So you have to find ways to manage human attention. You want to build a good computing interface that's easy to use and efficient to use, not like that elevator I showed you, and you have to design it well, and you want to triage and stage information in effective ways, and you want to draw the human attention to the right things. You don't want to have every pixel on the screen be equally salient because there's like a billion pixels on the screen, like render resolution. I want to draw you to the right pixels to get your job done. That's basically the kind of high-level principles of a GUI. So today we're going to cover, very quickly, grid systems, hierarchical size, grouping, small multiples, and finally end with color, and then we'll pick it up in another two lectures on this. So let's start with grid systems since they're, I think, the easiest to understand. How many people have heard of grid systems or even used grid systems in their web page design? Okay. So it's basically, you think of grid systems as basically multiples, sort of like the kind of Lego building blocks in order to put together information onto a 2D plane, like not only this website, but also a user interface. So here's kind of a schematic view where you have just the two-by-two blocks and also one-by-one blocks, and you just sort of put them together on this grid, kind of like city streets in a grid. And you just sort of lay it out in a uniform way. And often you pick the size of element that's necessary to convey your information. So if it's like a big hero image of two people high-fiving in the desert or something, you might put that up in it. Now let's talk about grid systems. What are grid systems? Well, a grid system is basically a grid system where you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system, and you have a grid system. the same layout. And then there's even like a grid where they try to capture things. These are multiples. These aren't quite multiples, but you can see that they're laying out in these different boxes that are separated for the tree alchemy. This is in its own kind of like bar space. And then you have the checkboxes here in their own thing. And then we were able to basically separate all these things. And if you look in webpages, you know, modern webpages, they have grid systems all over the place, but here with the bottom, I think it's BBC. So you can see, this is like a 1x4 element, and then here are 1x1s. All piled in here. Here you have sort of like the 1x2 element here. And of course they do have a little bit of like a sidebar that violates the grid structure, but the main content area. So then here's like the old CME webpage. You know, it has again like that kind of big hero image. And then it has these kind of little 1x1s. So you see this kind of design pattern everywhere. And we'll talk about design patterns in a later lecture. And so when you violate that, this is this example I think from Lecture 1, where like there's just no grid system at all applied. It's like let's just keep it together at maximum density with no alignment. And consequently, you sort of have to do that thing where you look at like all the different things. You have to scan it in order to find out what you want to do. There's nothing that's really drawing your eye except for that gigantic like these buttons at the top. So no grid system, and therefore it's very hard to know where to access information. And of course there's lots of webpages that I think are atrocious in terms of grid systems. Let me just show you these. I believe those are still online. Here's one of my favorites. It is just a beast of a design. Oh yeah. Look at that beauty. So again, like if you think about it, from a computer science perspective, it's like maximum pixel density. Everything. It's like opening up like a diner restaurant menu. It's like just stuff everywhere. It's like why are there pancakes in yellow and like you have no idea why anything is like colorized the way it is. This is sort of that thing. This is like how AI would lay it out. I want my objective functions, like most information in smaller space, like an AI would sort of lay it out like this. But obviously, it's not effective because, I mean, other than the icons, I'd be very happy to find whatever we want to buy. Great prices though. Let's go to the art department at Yale, which is also, it is, to be fair, even though I'm going to make fun of Yale a little bit, it is like a Wikipedia editable document. And so people come on here and do all sorts of crazy stuff. Let's go look at some websites here. Maybe facilities. Nice. So someone clearly came in with all the sheep. Someone came in and loves animals. So they don't really follow a great grid system on this. They do sort of have some elements, but it's sort of haphazard. I love how there's three COs right now. They really wanted to go see other things. So this is, I think, a little bit, the design is a little bit, not drawing your eyes to anything, but drawing your eyes to the background, which, again, it's art, so we can be provocative here. But it's one of those websites that doesn't really get you to what you want to do, like admit to the program. There's an admissions page or financial aid, but I would say it's hard to get it to the right page. OK. The other pages are worth it. Now, you can also do grid systems for icons. So iOS 7, one of the things when Johnny Ive took over design of Mac OS, it was a very, we'll get into skeuomorphism later as well, but there's a very sort of heavily textured sort of barring elements in the real world. Johnny Ive came in and basically said, we're going to clean slate iOS's design. We're going to make it minimal and flat. They also introduced this grid system for all their icons. And the idea was to give this balance to all the icons, because icons are just like a soup of different things. Every web developer is publishing their own icons, and it was getting really messy. So I will let him explain it. Like refining the typography. To much larger ones, like redesigning all the icons. It's a little hard to see the grid system. And developing a grid system allowed us to achieve a much more harmonious relationship between individual elements. We've also incorporated a whole new palette of colors. So, you know, it's a little hard because the projectors all need to conform to a color palette and a grid system. And it tries to make it so that a lot of the elements are sort of matched, right? The size of this bubble has a relationship with where the sun is and so on. They both probably crest at the same point. So, you know, here are some of the other elements on there. So they have these, like, inscribed circles. They try to have all these, like, golden ratios so it feels, like, you know, extra nice. And they try to have all the icons fit. So here's actually an example. So this is with the grid system overlay. So, like, you know, all the kind of circular elements like Safari and the radar one and this one are all the same size circle. Like, you can never know that the trunk is centered on the table. The trunk kind of crests against that element. You'll see that Chrome sort of violated it and just went for its own thing because people don't really care. But it does, they do try to lay it out as much as possible. So, you know, kudos in iOS 7 for trying to push it forward. At the same time, Android was, like, horrendous. And, in fact, if you get an Android phone today, sometimes it really does make you want to scratch your eyes out. Especially if they try to customize it. But I took a screenshot when, like, a year after iOS had basically revamped everything. So this is actually not that old. This is maybe, like, Android 5 or 6 or something like that. Can anyone see what's wrong with this or, you know, what are the design elements that you would correct? What are some things that are not harmonious? Or does it just look amazing and I'm nitpicking? Yeah. What are some examples? So this is sort of its own style, right? Yeah. Okay, one thing. What else? That's it? The photos will bother you on this one? Yeah? Like the shooting, like, if you look at Gmail versus, like, email, it's pretty much, like, bottom-ish. The email is, like, 3-ish. Yeah, a little bit more. There's, like, yes, there's, like, a little bit of, like, a height variation on there. Yeah. What else? Like, photos, like, you know, there's, like, I didn't look at them on Gmail, and I didn't look at them on Facebook. Okay. Yeah. So these are almost the same shape, but this one's sort of, like, coming off. Okay. Gmail's its own square. Email's... By the way, these are all Android stock icons. These aren't, like, a manufactured... These are Google-approved icons. Like, I'm not, like, pulling random apps off. These are all ones that ship with a native Android stock, Android phone. What else? Yeah? The color selection is not the way to go. Definitely. Anything else? Yeah? That'd be nice. But I can see that being helpful for some people, certainly. The thing that really hurts me is just the alignment. So if I draw... If I add some lines on here, you'll see that they don't match. They're actually not all uniform. So they're all, like, small pixel variations. Like, this is a line. This sort of violates... Like, go over, obviously, Gmail's totally different. This one, like, if we run a line to the bottom of email, people are sent below. This one to the top, not the bottom, and that screen is just so wonky. So it's, like, given that, you know, Google controls all these apps, they can literally just email all the people that are inside these icons and be like, please please, center it on the right, left, center it on the left. And it's just, like, a lot of people are just like, oh, I'm gonna send this to you. I'm gonna send this to you. I'm gonna send this to you. I'm gonna send this to you. And to be fair, Google has corrected a lot of these things. And I think in some respects, they were actually able to surpass Apple's performance. Okay, another one that's an obvious one, hierarchy of size, super obvious. Basically, webpages descend, or good information descends, and you have different... You use size, basically, to denote the headings. So when you go into a webpage like the New York Times, the New York Times logo is big. Very common, Facebook logo, first thing you see tends to be a bigger typeface. And then as you descend down, you get these kind of smaller headings. So this is the top story when I took the screenshot. And you can see it's the next biggest font on the screen. There's actually no other font that's that big. And then you descend into these kind of smaller categories. So you have, you know, tiny little links or comments that hang on the article or the time list. Here's like little, you want to read more things. So you see, even though these are in sort of a similar grid system, even inside of the grid system, they are actually this kind of reducing in size. This is just salience. This is sort of like that streetscape. It's drawing you to the biggest sign. And then these kind of smaller vendors might have kind of smaller signs. So you see this repeated all over the place. Grouping is another common one. Again, when you see another elevator example, if you look at this, can you guess what these buttons map to? Like what does this button map to? How many people think that maps to 21? Raise your hand if you think it maps. How many people think it maps to 22? So it is 21. And the way you would know that is that it's slightly closer to 21. But of course, when you're frantically walking into an elevator to get to class, you're not like, oh, it's like three millimeters closer to this one. So you're just like pounding it to get to class. And so this is a very fast and difficult example. And what you want to do is you want to group things. You want to attach things together. So the more common design is like this, where you signify somehow visually that the numbers attach to the button. And that's why most elevators, good elevators, you have designs like this. But it's really clear. It doesn't actually matter about proximity. It's about what you're attached to. So grouping is a very common thing. And that's why you often see grid systems where they're sort of formed into cards. where you know that the information contained within this box is all related and even it doesn't matter about distance. So grouping is actually one of several distal grouping principles. How many people have heard of distal grouping principles? Classically cited, I think. A couple people. So these are psychologically kind of derived. People study human perception of how information is displayed or how objects are displayed and your visual channel basically applies things like heuristics to your vision to be able to group things together. So we can, and I'll show you a bunch of examples. I think there's six or seven in total. I'm going to cover them all, but I won't cover half of them. So if you take a field of just random dots. Psychologists like to go simple. They don't like to have elevator buttons. It's too complicated. They do the simplest thing because it extrapolates to more complex cases. So they'll present things like this, like a field of dots, and then they'll just introduce chains. So one of those for grouping is called similarity. So that's the distal grouping principle of similarity. So if I presented this to you and I said I want you just to circle the things that are grouped together, a valid answer could be you just circle every single one independently. But because there's extra information in here, if you gave this to just random people on the street, most people would circle the five columns. How many people would circle five columns here? It's not the only answer, but it is logical. How many people would circle the five rows? So most humans, unless you're kind of weird, you would not circle those, because you're containing a lot of entropy. The whole point of grouping is that you put like things together. You want to reduce the entropy inside the group. Like there are motorcycles and there are cars. It would be weird if you're like, there's a cluster that somehow contains motorcycles and cars. Like does the group divide the color of the gas tank or something off? You put the functional thing first. So most people do this at home. So this is similarity. In this case, it's similarity of color, but you can also do similarity of shape. So most people would circle maybe this as one group. This is one group, this is one group, and this is another group. So you get three groupings here. Here's another one. Again, similarity can be color, shading, iconography, whatever it may be. So the other Bristol grouping principle is proximity, and this again is just the same kind of blobs of information, just all circles, but if we modify their proximity, you start to have groups that emerge. So then we kind of have these entities. So, what would be a valid solution? Well, how would you group these? Give me an example. Mike, your poll is great. Yeah. So you do this in one group, and this in one group. Then we'll have a different interpretation. Yeah. And both are valid. There's no right or wrong answer. And you can imagine if I, there's probably a threshold. If you run this experiment right, slowly compress this space, and eventually you would probably say, okay, now that is a group. And you can do this, you can put this like on Mechanical Turk and vary these parameters and know exactly the point where 95% of humans basically say it's it, and I can simultaneously string. So here, you know, most people probably say three columns, or maybe there's like little kind of groups of two. Totally fine. Another one, and this is what that elevator example used, is this Bristol grouping principle of common region. And in this case, you would just encapsulate an inner region that is common to all of them. So in this case, we're using kind of a background color, but you could also just have an outline or whatever you wanted to do. And in this case, you would probably say, it would be very weird, truly insane, I think, the circle moves this way, you're probably going to do it this way. Pretty obvious. Now, what's interesting is that the psychologists have also tested the intersection of these things. So, you know, what's a stronger effect for humans? Common region or proximity? So, if given this example, what do people think is the solution? How would you group these? ... ... ... ... ... goes this way, okay? Anyone else have a different? Yeah, that's two rows. You would do rows. So how many people would do the rows? Raise your hand. And how many people would go, kind of, the columns? Okay, so they're competing and there's like two different kind of perceptions based on like your training data in your life you've interpreted as two different ways. Most people would say, I think my understanding of it is that common reason most people would actually do the rows, not the columns. But, I guess there's no right or wrong answer. Another common one is connectedness. And so if you've actually, and you see this all the time in like information visualizations, and you see these kind of lines connecting the sort of web of information, is if you connect something, literally with like a line, is it tends to make it into a group. You literally kind of connect it and dodge the thing. So in this case, most people would put it into a row. And you can do it in different ways, so you know, people might attach these different groups differently. And it can be very strong. So again, if I gave you this, and I said, make these into groups. So this is basically using proximity. It's kind of hard. It'd be a tough challenge. You probably could answer this 20 different ways. But as soon as I draw lines through it, it's overwhelming. Your visual perception system would want to put them back into columns. So this comes into play again, sort of common reason, proximity, connectedness. If you see these little blocks all the time, this is a functional unit for these radio buttons. This is a functional unit for these check boxes. These buttons are kind of put together as a little match pair. So again, if you kind of just look at an interface, and if there's functional blocks, and if you just treat them as like this balls in space, and you were to like lasso them, you know, it's a good design when functional units end up together, right? If you're like, hold on, why is like these check boxes closer to the print button, and like not near these other buttons, that shows you that something's sort of broken. So logical elements should naturally cluster together visually. So you shouldn't have to read it to know they cluster together. You should be able to convey it just in terms of connectedness, common region, proximity, and similarity. Also very popular on especially online with small multiples. This is the notion that you replicate a piece of information many times over and over again in sort of a kind of an iconic way. So, you know, if you're going through like what to eat on Yelp, it's like this kind of star rating would be considered a small multiple. I don't need to read all the information, I can just compare, ooh, one has more stars than the other. Here's some other examples of small multiples. So you can even do it for like different colors, and if you go on Amazon and you want to buy like shoes or something, and it might show you like here's the brown, black, white, gray, and you basically get a selection. You can almost treat the different options as a small multiple. And same with like, you know, Hitmonkey is a great example of a small multiple. So it's kind of a compressed, iconic view, where you, it's the same, you don't want to have different visualizations. One's like hearts and one's stars, and it's like how do I compare against this? It's the same design, and you're just replicating it many times over and over again. The variables reach up and run tremendously. Okay. Talk about color. Color's really interesting. It's a very powerful tool to improve the user interface, you know, you want to communicate key information, you're going to be careful, and appropriate use of color can really draw your eye to the right elements. But you need to be careful of how you incorporate it, because if you do it wrong, it can be really detrimental, especially for people that like colorblind people. So color is, you know, totally manipulatable. Here's a nice example. Can anyone tell me what this is? This is something that you know. I've colorized it. And it's a, you can give me the noun, it isn't a type of thing. You can actually tell me what this is. Like, this is Justin Bieber. So anyone have any guess what this is? Tell me what type of thing it is. Is it a human? Is it a car? No guesses at all? This is perfect data representation, yes. It is a heat map, sure. No, not really. Yeah. No. This is, again, it's like an AI designed this. This is perfect. I thought, I'm going to maximize your visual business with a whole rainbow here. You know, from red to blue. Every color. Maximum detail. But the thing I'm going to show you next is way worse in terms of information processing. In terms of density of information. Like a machine learning algorithm would gobble this up. It's so perfect for it. No guesses? Okay. You're on the right track. So this is a map. Can anyone guess what this is a map of? Pittsburgh? No. Some people may be going there for spring break. Maybe. Yeah. Actually, Cuba might be on here. Is this Cuba? I don't know. Yes. Yes, that is. So what is this then? This is Cuba. And what is this? Florida. Florida. Anyone going to Florida for spring break? Wait. Okay. So just look at this graph. Here is what this data is. Okay? So this is it. I've just colorized it differently. And I've colorized it in a much worse information way. So we compare these side by side. What's the difference? Why is this? Because it's like Florida. Right? So this took you, you didn't really guess this. This is what it guessed immediately. So why is this better? Here's the scale. There's like maybe three or four reasons why it's dramatically better for humans. Yeah? So like we've made a distinction here that green is for land. It would be weird if blue was land and green. Then you might have been hard to recognize. Right? So it's not like green or blue really necessarily have to match. This is a very pure way of representing it. but there's a human connotation, like land tends to be green. So what else do they do though? There's another clever trick that we do at the border. So green and blue. We could have made this green and this blue, and it's fully gradiated. But that wouldn't work out very well. So the gap between where water is and where land is, is important for humans. We do cross that boundary, but it's an important boundary. It isn't like 10 meters in the air and 15 meters in the air is that important. But if you're 10 meters underwater, it's going to probably be a problem. So we've actually made it, this again is not good for the data. I could have had way more gradiations, like purple and yellow or something. But we've actually made this notion that at zero, there is a change. And that's purely for humans. That's a good thing for humans. It's like a discontinuity in the data. Unlike a best use lawyer value. So the other thing is that as it gets deeper, we go to a darker blue. As it gets taller, we go to a whiter white. So you can see the bottom of the Appalachian Mountains are forming there. So this again, if you think about the information, the RGB values, this is a much higher density. We have much finer detail. I can see a little tiny weak of how the shore drops off. Here we're using less color range, so like less bits. But yet, this is much easier to recognize. So we know that color is important for humans. And so here's the same different thing, colorized in different ways. We can colorize or even shade maps in different ways. We have MRI scans take place, thermal take place. How you present that information makes a big difference to perception. Now, humans don't think in RGB. RGB is sort of like, we make additive colors in red, green, and blue like the pixels that we make, but our perception is much more attuned to these three values. So there's the intensity or the luminance or brightness. the intensity of the light, that would be the hue of the light, which is actually what we think of as color. There's hue going around this ring, and then there's saturation, like how much of that color is there, right? So I can have a very, very weak blue source, so blue would be the hue, but it's a very dark blue, but it's very low intensity, and that'd be like RGB. Low intensity, blue, and, you know, very dark, it'd be like ATSR. So this is just like RGB, it's just a different color space, but this is how perceptually we think about how we kind of encode color in our minds. So some things are really good, so here's three dots of different, they're all the same hue, and they're all the same saturation, but I varied the intensity. So can someone put this in order for me? I'm not going to tell you which way, but this is two possible ways to order these, but give me one ordering to put these in order. BAC, right? It's pretty obvious, so in this case, we went from sort of most intense to least intense. It's not very hard to put things in order in intensity. How about saturation? So here I've just left, I've got hue and intensity the same, but I'm changing the saturation of the color. So someone put this in order for me. BAC? Yes, BAC. And you could also, you could do it the other way, because there's no necessary ordering, okay? So now, here, put these in order for me. So there's not really an ordering. If you're trying to put an order, you're probably like, okay, what is it like, it's like red, it's like green, like, joy, Jesus, like, violence, like, you could try to like reverse engineering from the rainbow, but it's just not how humans think. Like blue and yellow, like not once first and once after, but it's different, right? So humans do not put things in hue order, like we do for intensity and saturation, okay? Very hard to know, you know, what the right ordering is. And so with this in mind, it means that some things are naturally attuned to be in order, and some things aren't. And if you get it wrong, it's kind of bad. So here is actually a good example. This is the daily vegetable intake of one out of three metrics. And so the greener it gets, like the more veggies you eat, the darker it gets. I'll give one bonus point to California for this one. But you know, Pennsylvania, we're not eating enough green stuff, right? This is obvious because it's a linear chain. The more saturation of green, it goes better. And we can order that, so I can see where people aren't eating so many veggies and where they are. Here's another great example of using Q. So, languages. This is a really interesting map. It's the most common spoken language, other than English and Spanish, for every state. Now, you wouldn't want to use intensity or saturation for this, because it's not something that's orderable. It isn't like Italian goes ahead of Portuguese or Filipino or something, right? It's like, there's no innate ordering in languages, right? And so for this, you want to use the hue, the color, to do it well. So in this case, it's all color-coded. So, you know, Italians and Pennsylvania, really interesting. So, this is a really good use of Q. Now, if you get it wrong, it gets really bad. This is the mismatch here. So, this is ultra-high net worth population in their states of where, basically, rich people live. And they've decided to do it on this sort of rainbow scale, where they're primarily modifying hue. When I look at this, is blue ahead of yellow or green ahead of red? You basically have to go to the key to really understand unless you get really good at the matching. They should have done this, like, blue to red, or to minimize the kind of cognitive look of hot to cold, or just on this with saturation or intensity. It's like, you know, it's only blues, and the bluer it is, the more rich people there are there. Same for this one. This is another example. You know, best businesses, best states for small business. And they've done it with this very complicated color Q. And so, again, like, it's orange ahead of tan. You know, they had to basically, because it's not an obvious scale, they actually had to put the three inside on both of these charts. They had to literally put the number in there because it was so hard to understand. So, these are bad. You don't want to use Q for things that are going to be continuous. Now, what they often tell you to do is the best way to design your UI is to start in grayscale. Because if it works in grayscale, that means it's going to work if you colorize it, or with people that have blue eyes. colorblindness. The icons are so good that they kind of stand alone in just intensity, then it's going to work really well when you colorize it as well. So you can use like a photograph or a photocopier to make sure that it works. And then what you do is when you do colorize, you basically just keep the intensity value the same. You just basically keep the grayscale value the same. You're all too young really to know, but as you get older, you actually get harder to see some things. So as you get older, your lens is actually yellow and you tend to not be able to have some of the sensitivities like blue. And also human sensitivity isn't that great. Like actually human eyes are really, really good at blues and also greens. This is often what you see like on a submarine, they're often like red lights, like on the bridge, like during battle, is that red is actually pretty good at red perception, even in low conditions, like low lighting conditions. And it turns out that in a renal center where we have really good perception, it's really hard to see blue. Even though like blue LEDs are quite bright, but we just don't perceive them as bright as things like green and red. And you can see that blue taxis should be a little harder to read. The other thing that happens is color blindness. So hopefully no one's going to discover that they're colorblind for the first time right now. So you should be able to read all of the numbers in these. Okay. Yeah. Okay. The projector is a little bit wonky. This is a six here. That's the hardest one to see, at least for me. Does anyone who is colorblind and doesn't mind sharing with the class, does anyone not see everything here? About one in 10 men are colorblind. So if someone is colorblind, probably this would be a big slap. It doesn't matter. But if you are colorblind, in this red-green color blindness, this is what you see. Like this 45 right here, this is what you'd see if you're colorblind. You just wouldn't see the number at all. Okay. And so if you design interfaces that are like green-red, it may be totally invisible. So, again, if you can't see every number in here, congratulations, you're colorblind. And of course, there's different colorblindness. I think there's a red-blue colorblindness. There's other ones where basically this will disappear and this will not. I can still see the 56 and the 25 here. But other colorblindness means that I'll be invisible. So women suffer from this a lot less. It's about 20 times less likely for a woman to have colorblindness. So if you're a woman with colorblindness, that's pretty exceptional. A really interesting story, a little bit of a diversion, but it's really interesting, is people have proposed, and there's a lot of research going back and forth on this, that actually not all colors are uniform in humans, that there's actually a strong cultural component. This first came up with Gladwell doing a review of... I'm sorry. He went through the Odyssey, which is a gigantic book. Maybe you read it in a gigantic epic poem or something like that. And they noticed that the way they described things is very odd. This guy, William Gladwell. And if you've ever read the Odyssey, it goes through pages and pages of really small details. There's five-page descriptions of just someone's armor, every little riveted facet. It's not like they're cutting on details. And what they found is that there's very strange color descriptions. Again, they've been many pages talking about clothing and armor and weaponry and facial features, animals. But the colors are really strange. So iron and sheep are violet, and honey is green. Okay, that's kind of weird. They give all this precision, and they have to really sloppy with their colors. So Gladwell just counted all the colors in the book. And white and black is mentioned over 200 times. Sorry, white is mentioned 100 times. Black is mentioned 200 times. All other colors are really rare. So red is mentioned fewer than 15. Yellow is mentioned fewer than 10. And he never found the word blue. It's like a thousand-page document. Never found the word blue. I mean, it's set in the Mediterranean. It's got blue skies and blue oceans. There's no blue. And in fact, they describe, the famous one is, they describe the Mediterranean as a wine dark. Like think about wine, it's like very dark red. It's like imagine a dark red. Would you ever describe the Mediterranean as a dark red? Probably not. So this sort of caught people's attention and it's actually a great radio lab episode where they go through this story of Gladstone discovering this. So people have speculated that like the ancient Greeks just could not see blue, or they didn't see blue in the same way that we see blue today. So here's a great clip, also controversial, but it's a very interesting study in this BBC documentary. What is the color of water? This is the Himba tribe, I think in Namibia. For me, you see where I come from, we say the water is blue and the sky is blue. And you say the sky is black, the water is white. So we have different words to talk about the same thing. Well, we have 11 words to describe color. The Himba have half the amount. Here's their color. They include Zuzu, which is most dark colors and includes reds, blues, greens, and purples. Pretty different colors. Vapa, which is mainly white, but includes some yellow. Buru, which includes some greens and blues. And Dambu, which includes different greens, but also reds and browns. Obviously. So here's the study that they read. They clearly describe color differently, but do they see the same way? Serge has been running experiments to find out. Okay, now you look at these new 12 squares. One of them again has a different color. He's testing how long it takes them to spot a color which is different from the others. Now, I don't know. Can you do the same thing again? This is what they're looking at. For us, it's quite hard to spot the odd one out. Okay, can you point one more time towards the different color? Can you see the different color? Very good. But for the Himba, it's easy to see the green, which is different. That one was a little different. Just a little different. And it's pretty easy for them. So you see, in this particular trial, this green patch looks very much like the other ones, at least to me, and I think to most other Westerners. Whereas for the Himba, this is a different color. They have a different word for this type of green compared to the other types of green. And that allows them to more easily distinguish between these two colors when they're next to each other, whereas for hard, it's very hard. So when Westerners do this exact same trial, they will spend much longer and they will be much more likely to make a mistake than the Himba. The next experiment is crickets of Himba. In this one, they are shown a circle of green squares, which includes one blue square. So again, 12 colors, and you point towards the one that is different from the other 11 colors. For us, we have separate words for green and blue, but as the Himba have the same word for both, it takes them longer to spot the blue. Can you see the difference in the colors there? Can't see it. two categories of colors very close to each other. For us, it's quite clear the one that is different, but for them, no. So here's the actual test. For example, so this is easy for the himbo. They're like, dude, it's that one. It's so friggin' obvious. It's basically the equivalent for this. If you show this, they're like, are you trying to screw with me? I don't see any difference at all. Pretty crazy, if you think about it. So they just divide up the color space in a different way. So you can divide up the color space. This is older. So this is English. Here's the green, the big area. And we have weird things, too. It's not like our color space is nicely partitioned. Like, why is that dark reds are called brown? There isn't a special word for dark green. It's called dark green, right? Or why is that bright reds are pink? That's just arbitrary. There's no reason. They could just be called red in the same way. We don't have a special word for light blue. This is generally assumed to be a uniform. So weird stuff going on here. Green and blue are sort of gigantic. Purple. So this is just how English language, these boundaries are how English language has basically evolved over time. And of course, because of Western influence, a lot of cultures have adhered to these boundaries. Fashion and trade and so on. And then here's the himbo, right? So here's like Baffa that's like, you know, most whites and a little bit of like yellow maybe down here. Here's Peru. And as you can see, the mapping is also kind of contiguous in that they have different regions. It's just that their boundaries are just in different places. And because they're so good at that test, it says one of those greens is a jumbo and one of those greens is a burro. And so they're really good at separating the two colors because they have a boundary there. Where here, we don't have a great, if it's in here, it's like, oh, those are both greens. And I haven't been basically trained over my life to disambiguate those two colors. So XKCD ran a great experiment. And you can go look at where he had tens of thousands of people basically fill out a survey and was able to build like a modern day map for this. And some people do have like special effects like magenta and dark purple. But generally, this is how we cover. the space. But what's important here is that this is totally arbitrary and it's totally cultural, and other cultures have basically different ways of carving up this space, especially if they've been isolated from sort of Western trade and product. Here's the blue chair versus the green chair. All those are reinforcing basically sort of an international version of the color space. But back in the day, like with the Greeks, they probably didn't even have blue. It was a very weird color. Blue only existed like right here, and like the sea was a part of this blue-red area, and they just named it the same thing. Really interesting, really interesting design. Okay, so on color, if we're looking at icon design, you know, here are two versions of icons. Why do you think, which one do you like better? How many people think the top one they like better visually? How many people prefer the bottom? Okay, going against the flow. So why do you like the top one better? Yeah, I like the brighter colors. Okay, so who's there? For everyone else, what do you like about the bottom one better? Yeah? Yes. You want your icons to be relatively simple. Obviously, these aren't kind of apples to apples comparisons, but yes. There's a simplicity to the bottom icons that make them iconographic, right? Yeah? Is that a good hint? Yeah. Okay. So what is the color scheme? It's sort of like blue-white, brown-y. So there's probably only about four colors in these icons, and they use the colors in very clever ways to make it so it feels sort of homogeneous. Where the top one has got a really eclectic color palette, there's probably a dozen or so colors in there. What else is nice about the bottom bar? Going back to some more historical principles, yeah? So there's sort of padding, yeah? That makes it so it's a bit more breathing room, but it doesn't feel so jammed, yeah? Yeah, so there's a nice... of the proximity, there's no color or connection, it's really proximity or maybe kind of common region but it glues the functional items together which is also sort of nice. You have to just look at functional groups if you know you want to do some text manipulation like copy paste, you just have to recommend any icons here and you know that this is sort of a functional group for that. So I think all those things are definitely true. Of course the great thing is like how many people have ever used that, when's the last time someone has used that disk? How many people have never held a disk like this before? Raise your hand if you have held a disk like this before. I'm going to ask that question again in five years and they're like what even is that? I know the symbol but I've never even seen one before. It'll just be like a USB thumb drive in like 2050 or something. And I'll be like I've never seen one of those either. But yeah, so that's definitely super antiquated. And same with copy paste. As we saw in that Xerox Star commercial, it used to be like cutting out with scissors and stuff and we've kept those words but all these things are getting antiquated. How many people have seen a blind printer that looks like that? Not me in many, many years. Okay, so icon design. Lots of advice on icon design but generally you want to kind of avoid this sort of Las Vegas effect, sort of like an opening thing where there's lots of icons competing for your attention. You want icons to be readily accessible so they're iconographic and you know what they do, you don't need text underneath them ideally, but you want to make it so that they're still recognized. You don't want to distract, they're kind of kept up in distracting you and drawing your eye when you need it. If you kind of go crazy and that's sort of what the top row was, a lot of colors, lots of details, you can have an unprofessional appearance and it starts to become distracting. So you want to rely more on shape. So again, use intensity. Design your icons in grayscale and then pepper in some nice colors. And let's rely on some nice colors and fine details. When you're staring at something quickly, you're going to proceed for the rough shape much faster than you are going to read all the little tiny weird details inside of it. Well-designed icons can save screen lists. If it's a really good icon for a control panel, then I'll have to write... So you can actually save a lot of screen space and make it more easily recognized and remembered. Quickly found in a busy digital environment. And good designs help with internationalization as well. You don't want to rely on things that are culturally specific. And I'll show you some examples in a second. You want to represent objects or icons in a familiar manner. So if it's a mail, like you want to mail something to someone. You don't have a postman running down the street with a satchel of mail. You probably use an envelope or something that's cross-cultural and much more familiar. Try to use concrete objects whenever possible. Your abstract concepts are much more difficult. Okay, this is like the generate the idea and upload it to a light bulb. Or up there, go into a little rain cloud or something like that. When you're going to pack too much in, it actually gets harder to remember. And all the other things you have to be aware of for the icons. Nothing too surprising there. You want to be consistent. Use good metaphors. If you have to use a metaphor, you want it to be a good one. You may be using labels, especially if it's an interface that people aren't familiar with. Or the labels can exist while you're in kind of newbie mode. But that makes it sort of fade away. Once you get good at it, as icons get smaller, you may want to remove them. So you can tell with an example. Here's actually Photoshop 1. And this is the Creative Cloud version now. You can see that the icon design has changed a lot. They've actually stripped all the color out. They're entirely relying on intensity as opposed to color. These used to be a lot more popular, this notion of icon languages. So if you have many different... Microsoft Office can produce an ungodly amount of different types of files. It gets confusing. So they were one of the first to sort of pioneer this notion of having icon languages. So you have sort of the basic types of documents with this template. Those are sort of different. But then you can also do it for text documents or scripts. You can't see it. It looks a little bit different. And then you have the application, Word or Excel or so on. And you can kind of blend these together. So you can have... Microsoft Word templates, right, and it basically puts them all together in a text template, or Excel spreadsheets, so you can mix and match these different elements together. Microsoft actually used to be better at this. Now they've sort of gone to like fancy, minimalist icons, and they're starting to lose some of these details. I have the belief that the 2011 Mac version of Ditto at least has some icon language there, but they're increasingly just going sort of minimalist. But at least they're still following good icon design, that it's simple use of colors, and they're trying to convey what it is without having to put the documents in. I know, like flow diagrams, and Word is mostly text, and Excel is a little bit of icon, so it's reasonable design. Let's pick apart another example here. So I'm sure, how many people have seen, especially an older, like Windows XP-era control panel that looks like this? It's just like a mishmash of icons. So, looking at this set, what have people noticed that probably is not good icon design? What stands out to you? Yeah? I think that the product would have security centered, like, same colors, I think. Yeah, and one sort of shield-like, and people probably should have checkboxes, which I don't know totally why, but yeah, that's true there. It's sharing an iconography, but it's quite different, yeah. I think there are a lot of icons that are, like, the same, but there are three separate icons, and then two world icons. So if you don't know what you're kind of looking for, like, if you check the page, and these are icons that are the same. Where is it? This one? And there are a lot of, like, progress. So, like, those are the kinds of files that you can know you're looking for. Yeah, so I think, definitely, to reuse an icon, like, there's a globe here, there's a globe here, there's a globe here, there's a shield on some of these, so there's a shield here, there's, like, a little beanie shield there, I guess it's a protective surface. Yeah, and then I get a folder, but if I'm looking for a folder, why are the fonts in a folder? I don't know, like, that's where you store them, I guess. What else do you see in here? Doesn't drive with you, yeah? I feel like in some icons, they have labels and stuff like that, so it's like, you just access things you don't tend to see. Oh yeah, it's like you go in and out of a pizza pie. Yeah, I don't know what that means. It's like a metaphor. It's not too fancy of a metaphor. Does anyone know what that metaphor means? You go in and out, I guess, because it's easy to access? Okay, yeah, I agree. Yeah, I agree. Some of these can be very confusing. Like, that network map and that network list. But if you're just giving me that icon, I just feel like it's window controls or window... Yeah, it'd be very... It's actually a great test, like, strip it of a label and see if it can match. What else? That's the tip of the iceberg. Yeah? There are a lot of instances where there are two colors next to each other that have very similar saturations and noxious values, but I think if you were to use, like, land versus water on the globe, this is pretty... Yeah, not a huge contrast there. Yes. And it's unclear, like, why, when you put this one on a globe base, that becomes pretty small value. But, like, that one, it's just like you're floating, and that one, I guess, you close your feet and you're able to get into it. Yeah. What else? Yeah? There's not a light. No. And they're sorted alphabetically, which I think is meant to be helpful. But instead of... It would be more helpful if it was, like, very split in categories. Yeah, and that's actually what they did in the emergence of Windows, because they got... They're starting to get too bad, right? So just some other ones to point out. So there's... Not even just different folders, but there's different folder designs. Like, these are, like, opening this way, and then this one's, like, a sideways one, and there's, you know, various things going in and out. Different flat versus 3D. You know, there's a lot of these, like, here's a flat one, here's a 3D one, here's, like, a 3D here, that's a flat one. 3D. And also the 3D go different, say a tablet going this way, here's a tablet going this way, these are going this way, that one's going this way. It's just like, if you're going to go 3D, at least like standardize all the perspectives. It shouldn't just be like a free-for-all of all different, you know, angles and so on. So yes, they have a lot going in here, and to Microsoft's credit, they did actually fix this, because it was just getting obscene. And a lot of these are non-standard icons. They're developers alike. I'm sure that I said the initiator is not a standard Windows one. So they probably like stole the globe from some other icon, like mashed it up, and then dropped it back in. And in that version of Windows where you can sort of let people go loose and do everything, it gets pretty bad. There's also a lot of kind of cultural issues. Same with color. You have to realize that culture does have a big factor. You are sort of got this expert blindness that you live, you know, you live in the U.S., you're studying in the U.S., you may have even grown up in the U.S., and so you have certain expectations. But someone who is not from the United States, do you have these mailboxes in your country? So if you're not from the United States, you also not, there's a complicated double negative, enough of that double negative. Who does not have mailboxes like this in their own country? So that's almost half the question. Now you probably know what that is, because you've seen it so many times, it's an icon that we keep using. But in most countries, they don't have people come around and put mail into your own mailbox, like we do a lot in the U.S., where you put a little flag and so on. It's not like a communal mailbox in your apartment building, or even at the end of the block for like 50 houses, there's one postbox for everything. And so this is an American-centered icon that we've co-opted to use for like generic email. And of course, you know, this is true for even metaphor as well, certainly for hand symbols and facial symbols. So there's this great study that was run where they look at this, okay, so they gave people, and I've blown up these icons tremendously, but if you look at this icon, which has a lot going on there, what does it convey to you? Just yell it out loud, what does it mean? What does this icon contain? Multicultural party. Multicultural party, okay, that's an interesting one, yeah. What else? chat room. Okay. Lying? Okay, interesting. Gossip and rumors? Okay, interesting, yeah. What else? Those eyes are pretty intense. So they gave these kind of icons to different parties. So the U.S. interpretation is gossip, joking, happy group party friends. So that's a little bit different. And in the Japanese one, it's liar, double dealing, deceit, scheming. People tend to talk more on sort of the Japanese side than the kind of typical American. There is a little bit of, it's dark, it's gossip, but there's no like backstabbing or so on. So they did this for a lot of these different icons. And again, these are not quite computer icons, just sort of iconographic kind of symbols. But you know, they're quite different. So here's like, you know, people in the U.S. are like exercising, jump rope, yoga. Sometimes they're like, it's okay, or correct. Here's mad, angry, no. It's like, no, good, wrong, which is a little bit different. Talking, praying, and thinking versus thank you, pleased to speak. Even these kind of, you know, just colorization, the same icon. This is woman, 29% of the time, but a woman, woman, mother. There is no dad and parent, which is 7% of respondents. And this is a man, dad, woman, adult. So you see the percentages get mixed up, and they have different sort of cultural distributions. And again, this is just, you know, the U.S. and Japan, but you can imagine this is going to be different in China and the Philippines and Vietnam and India, Pakistan, whatever it wants to be. So, you know, these are examples of very Western-centric ones. You know, they have a woman that has to wear this skirt. This culture is where it's getting reversed, you know, but we've adopted these sort of international symbology. These sort of houses where you have a chimney because you live in a cold climate, so you need a fireplace inside, and you have a pitch group where snow falls on, and you want it to not crush you inside. You want it to fall off a roof. That's very, like, European, and not even true in all the United States. We see that as sort of a home icon. a phone like this. How many people have picked up a phone on a cord that looks like this in the last month? Not in your life. No one is that old or young. I haven't done that. How many people have picked up a phone like that in the last month? Last year? I think there are a few on campus. Okay, fine. That counts, that counts. Including the campus phones, has no one picked up a phone like that in the last year? Last five years? Most people I would imagine at some point in some life would pick up a phone like that in the last five years. But yeah, eventually that's going to be relegated to history. You'll be telling your grandkids, like there used to be these weird cables and you'd pick it up and talk through it. Okay, so because I can't show you western-centric ones, I'll just show you icons that you're not familiar with. So, let's go through these. These are, I won't tell you what they are. They are road signs, so they should work pretty well. They're European. If you go to Europe, you're expected to know these road signs if you're renting a car. What does this blue circle being crossed out in red mean? Do not enter. Huh? Nope. Yeah? Sort of, it's no parking. That's the sign for no parking. You're going to see these everywhere. So you're like, oh, there's one waving everywhere, I can't get out of the city. Because there's no parking. Okay, how about this one? A blue circle would be bicycling. What does it mean? Nope. Yeah? Sort of. It means bicycles only. So if you see that, don't drive your car in that alleyway. Okay, how about the next one? Where you have these little two motorcycles and cars in it. in this red circle? Yeah? Shared lane? No. It means no motor vehicles allowed. So definitely don't share the lane, just like pedestrians do. How about this empty white circle? Nobody allowed. Yes, that's right, the road is totally closed in both directions. And then how about this one? This is also a really important one, especially in Ireland and South America. You have one red arrow, one black arrow. Sort of, but now you can go both ways on these roads. So it means you have to give way to oncoming traffic. So you're like the red arrow, which means you're worse than the black arrow. So not in tiny roads in Europe, if these two cars are coming, if you're the red one, you have to pull over so the black one has priority. You can also see the sign with the black and you have the black and you would get priority. So basically, these are actually not that exotic and now I really hope none of you are going to Europe for spring break because it's going to be like a total disaster. Here's some other quick ones. So this one, I don't know if you remember yet, this is no through roads, sort of like that. And this one's really complicated, it has the same black, red terminology. So this means you were allowed to pass. Now hold on, I even have to read part of my notes on this one. When you see just this, so forget the end, pretend the end accessory comes up there, this means no overtaking. Because I guess in England, you're going to overtake on the other side, yes. But when it's accompanied with an end sign, it means now that if you're ending the prohibition, I'm not overtaking. Which is really complicated. Yeah, so I like, and again, there was probably a lot of designers that have spent years and years and years and wrote fantastic additions. They've got to be instantly recognized, go to people. who may not even be familiar with them, and yet even still, and then you have a lot of experience with road signs, you probably have not gotten really even one of these correct, so it'd be like a total disaster. Does anyone know what the last one is here? Yes, it is pharmacy in Germany. That is the pharmacy sign in Germany. Okay, so you have icons, like all designs, you really want to iterate on them. It's like survey designs, wireframes, whatever it is, you want to iterate, iterate, iterate, show them to your international friends, post them on a website, make sure they don't have unusual kind of cosmetics, because a metaphor that you think is brilliant, like the light bulb flying up into the cloud or something, might have a totally different connotation, like even an offensive connotation in some kind of culture, right? So you just want to make sure that it's really well-designed and is ideally universal, especially if you're gonna leave the text way locked, so it's just like a gear with a crack in it, you know, or like the in and out arrow, like what does that mean? Is it like broken settings or something? So you just want to make sure that it's... Okay, with our last five minutes here, let's talk about big up to. So a new team's been created, as you know. The old teams will exist, the old groups will exist on Canvas for grading purposes, so you'll be able to see your old teammates. The new groups are called, confusingly, Teams 2, Group X, okay? So you're gonna be in Teams 2, Group 1, 2, 3, 4, and so on like that, and you'll see those on Canvas, okay? This is gonna be due April 2nd, so that gives you three, almost, I guess like, it's Tuesday, two weeks after you get back from spring break, so you can look at that data. I think that gives you basically three weeks, essentially. And here is this new play box that you're getting put in, okay? So again, you're gonna be given a source code for a simple application, and it displays a destination square and a cursor square. Now, this is an analogy like Photoshop or PowerPoint or Keynote. You have a logo that you want to put in the top left-hand corner at a certain rotation, all right? So basically, this is where you want to put it, and this is where it is right now. And you can think of it as a destination area as an X, Y position, as well as a rotation, and as well as a scale. So you can think of it almost. So it's like an X, Y, Z and a rotation. So there's four parameters that you have to manipulate to get your logo into the right part of the screen. So this is a four degree of freedom method. Now what you have to do is you're going to do it on your laptops and you're not allowed to use any buttons at all this time. So again, golden rule, very similar to the last one is that you have to have equal treatment of all possible X, Y, Z and rotations. So there should be no X, Y, Z, R rotation or whatever. That's easier than selecting up, you know, 12.3 to be just as easy as 45. So all X's, all Y's, all rotations, all scales have to be the same accessibility. There's no snapping of any type allowed in this bank. I'm making it more stringent than before. Let me be able to actually show you what this is. So this is a single touch application. There's no multi-touch allowed. There's code that verifies that the two squares are matched. You shouldn't have to do that. We're going to do that as before. Deliverables are pretty similar, but let me show you what this looks like. So here is the code. So here is what you see. So this is my cursor square, and this is where I want to take my logo, and I want to have my logo in PowerPoint. So this is the scaffold design, which is awful, and you want to do a better job. So here I'm going to go down. I'm going to go right. I'm going to make it bigger. I'm going to go a little kind of clockwise here. I'm going to go back up a little bit. Bigger. Counterclockwise a little. When I'm ready, in this case, I click the center of the screen. Now I can go back here. my mouse cursor. So if we look in the processing output here, you'll see that it outputs, so it says was I close enough? False. Was I good enough my rotation? True. Was I close enough in scale? Also false. That means I actually got that trial wrong. So let me see if I can actually do a trial correctly if I work that out. So you can see all the other targets are on the screen. They have to be pretty precise, so it's going to make it difficult. Okay, and click. Let's see if I got it correct. True. So I got the rotation was good enough, the Z was close enough, and the X was close enough, but I got that trial correct. So what I'll do here is I'll just click through. So in this case, I racked up massive errors. On average, I took six seconds, but I clicked the right, so it gave me 11.6 seconds at the end, and we're going to do the, as before, with penalty increases. So that's it. So you have to think about a way to make dragging that square onto, you know, again, think about those other squares like a destination in your mind, like when you open up a blank Photoshop file, and I'm going to put my logo here in my magazine article. It's like a destination in your mind. That's what we're simulating here. So you need to have a way, have the user very efficiently change those four parameters to set sort of like their logo in their mind, okay? So we're not, last bake was with like desktop, and now it's going to be, so you can, I'll post with all the details to Canvas. If you want to download the bake up code, I would encourage you to at least open it up and run it on your laptop before spring break, just so you can get a sense of what it looks like. It is a bit more complicated. This is going to take more time than bake-off one. We're going to increase the complexity of bake-off, and then the final bake-off will be total insanity. So with basically no time left, you can see if you can meet your new group mates before the end of class, and everything will be posted to Canvas, and to Canvas, and to Canvas.\",\n",
       " \"Can you guys, in the middle row, can you go this way, can you join with that table? Can you slide down so they can have that table? Can you join? I like the way you kind of go... You're like... You're kind of like... You're like... I don't know. I don't know what's happening. I don't know. I don't know. I don't want to talk about it. I don't want to talk about it. Okay. I'm sorry. I thought that was a good idea. I don't want to talk about it. I don't want to talk about it. I don't want to talk about it. I don't want to talk about it. I don't want to talk about it. Okay. Let's talk about Bake-Off 3 quickly. Um... There's a couple teams that emailed me like late last night and this morning that I have not yet responded to, but I will get to that today, so just hang tight on my reply. I think pretty much at this point all teams, or maybe minus one or two have emailed me with their ideas. That's great. I would still say try to battle by Friday evening to have a version working, a implementation of your idea so that you have sort of like three, four days to polish that up. It'll take time, as you know, at the very end to wrap it up and submit the video and so on. Um... I'm trying to think of any other general questions. I keep on getting questions about can I, you know, light up keys in red if it doesn't match the target phrase and the answer is no. So like pretend like you don't like you don't really know what the target phrase is even though you know what the target phrase is. You can't bias in any way to the target phrase. You can't bias in any way to the phrase set. That's sort of the general rule for this day job. So if you're doing something that's like making the next key to type three times larger, that's definitely not legal. You're under no obligation as well to provide a delete button. You could just, you can see what the error kind of penalty is in the code. And if you're like, we're just gonna plow forward no matter what, that's totally fine. Likewise, I mean, theoretically, if you're like, I'm just gonna leave out all vowels, like that's fine too. You'll get a huge error penalty, but like that's totally legal. It'd be a very interesting way to do text entry, but all power, or maybe like if I thought of the algorithm that fills in the vowels, whatever you want. Any general questions? Yes. How do you make sure that the code is presented in the correct way? Okay, so definitely contact the TA as soon as possible. You can come at the end of class and I'll see if I can help you debug it. But, do you see it under the device list? So in, you see it under devices. Yeah, yeah. And does it say anything on the phone? Did it say, do you authorize this computer to access it? Google it. You can show me at the end of class, but yeah, that's fine. So if you have not deployed to the phone at all, you're way behind. Like the first thing you wanna do is deploy anything to the phone. Make sure you also try deploying just like an example code, like go under examples, basics, arrays. Deploy that so you can verify that it's not the scaffold code, that it is indeed the phone. Also try deleting the manifest file. So if you go into the scaffold code, delete manifest.xml or whatever, often you have to do that to kind of clear it out. A couple of teams came to me and that was the trick that solved it for them. But yeah. The TA is your help here. Don't email me Sunday at like two in the morning and expect like a super detailed reply. I wish that I could help every team debunk their stuff, but the rigors of junior faculty life prevent me from doing that. So I'll either be sleeping or out at a club. No faculty senior at a club. Okay, another thing that I wanted to mention is I splurged and I bought you some more burner phones and they cost $29, which is normally higher than my, these are Amazon Prime phones, Amazon has not banned me yet. And so for teams that I gave really crappy Android phones to, I have exactly three. Actually, this is the last three on Amazon at $28.99. And so if you're one of those teams that's really struggling with a very crappy phone with a very small screen, you can come get this. If you are a team with a decent phone, don't take these because there's literally only three that I'm gonna get before the bake-off. So come talk to me after class and you can grab a phone. Hopefully the other ones are pretty reasonable. But it is amazing what you can get for $29. It's actually running like Android 8. It's, you know, cameras on the front and back and it makes calls and what else can you possibly ask for? Any last, so next time I see you, it will be the bake-off. So like, is that right? It's Tuesday, right? So like, B, you guys should be sweating it a little bit. And then after bake-off three, we'll be bake-off four. So now you guys should be in full swing, you know? So, any final questions? Okay, laptops are going away, I presume? No laptops in Professor Harrison's class? Okay. Okay, good morning. I've got nine more. My thumbs are telling me to do it. I'll play the cards. You've got to get three. There is a chair at the very, two chairs at the very end if you want. You've got to get three. No questions on the back this time, four easy-peasy questions. Eleven minutes past. You should be able to get from anywhere on campus if your class gets out at 11.50. It shouldn't take 21 minutes to get to class. Okay, ten seconds. How many people are done? Raise your hand nice and tall if you're done. Okay, fifteen seconds. Okay, start to coalesce them. If you're finishing, that's okay. Start to group them as you can for you. So there's a chair here, if you want to sit down here and just follow the table, it's a nice mount. If you're too close... Okay. Anymore? Can you grab that behind you please? Last ball... I have them all. Okay. On the answer side of things, here we go. So the fovea, part of the eye, and that is the area of vision with the highest resolution. And the answer is... So the fovea, part of the eye, and that is the area of vision with the highest resolution. Where you read that one or two degree arc in the middle, where you have the highest resolution. And everything else is sort of just a blur that you interpolate. Finding the highest number below takes significant time, because you can't unfortunately see all those numbers at once, and so what we do is we have to scan, linearly scan often with our eyes. How many people put, a user must scan with their eyes? Okay. Yes. You probably had to literally do it yourself to do this, so that probably wasn't as obvious. Okay. The P, in peak and end effects, refers to, how many people put A, the most prominent point in an experience? How many people put B, the most painful point in an experience? C, the lowest level of cognitive load in an experience? And D, the end of a recent Nicolas Cage film? Okay. Definitely B. All of them are way down peaks. When was the last film? I don't even know. So it should be really A. I'm actually surprised that none of you picked B. B was supposed to be my trick question. But it doesn't have to be painful. It could be painful, or it could be, it's sort of like, it's basically valence undetermined. It could be really painful, it could be the most euphoric experience, so that's why you keep picking B. is considered the most prominent point, the most salient point in an experience. That peak can be negative or positive. And then finally, in Professor Harris' work on progress photos, he demonstrated length could influence the perceived duration. And the correct answer here is visual manipulation. So I was changing how they moved, and like they're flashing, and like strobes. This was just a graphical manipulation. I wasn't changing the time of day, or the number of files, or the network speed. I was just changing how they looked. That was the visual manipulation. Going into the archive. Don't you ever wonder where all these pop quizzes go? This is like a bonfire behind my house. It's incredible. At the end of the semester, you can see it for a month. So input and output. So today, we're going to continue talking about input and output. But I'm trying to, as the semester rapidly wraps up, which is kind of insane, how few lectures we actually have left, even though we do take-off three next week. So I'm trying to shift now more towards research topics. As you can see, what researchers like myself are doing in the field of ATI. And this is another topic that's sort of near and dear to my heart. I've done a lot of research in sort of using acoustics and vibration as a sensing mode to basically interpret human inputs. So let's start with just a little bit of a brief kind of primer on acoustics, because probably none of you have taken a class in acoustics, unless you're like a mechanical engineer. But it is this interdisciplinary science that deals with the study all mechanical ways, and that's in gases, liquids, and solids. And it includes vibration, sound, ultrasound, and ultrasound. So when we think of sound, we often think of the kind of sound that humans receive, which is the vibrations that go through air. But vibrations go through anything. It's this mechanical propagation of basically kinetic energy that encompasses sound. And for this reason, it's often called vibroacoustics. It's sort of they combine it together. So sound are pressure waves. So in the air, there's basically this kind of like this gas in front of us. And basically, the molecules get squeezed. And then they squeeze other molecules and sort of ripple, sort of kind of go through it. And then there's two types of hearing that are very... So there's ultrasound, which is above human hearing, which is what bats and whales and many other animals use. And there's also infrasound, which is below human hearing, and we can't really hear thunderstorms coming in per se. They're very low frequency. You can think of it like a cloud being like a gigantic diaphragm, and that might be fractions of a hertz, way below human hearing, but we all know that when earthquakes are coming, or at least the meme is all the birds start flying away when the volcano is about to explode, because they know. And it's sometimes actually true. A lot of those animals can hear these very low frequency vibrations of the earth, and they know some sort of tsunami or volcano is about to come. And then there's also a vibration, which is a mechanical oscillation, and again, just in the same way that gas can compress and push that compression out, there can be compressive waves in solids and liquids as well, and then there also can be these transverse waves, which is like the ripples on a pond. And both of those are considered waves. So sound and vibration are super closely related. Generally, you know, and they sort of, they're almost like, it's like EM, it's like electricity and magnetism. They sort of interact. So if you vibrate something, like if I take this kind of thing and I start to induce vibrations on it, it makes a sound. It's basically generating a sound by actuating kind of sound molecules, and likewise, if I put a speaker in here, like a big subwoofer, and I started to run it, things on your desk would start to vibrate. So basically, because they're all the same form of energy, it's just mechanical energy, you can induce these two different sort of vibes. And so because they're so interrelated and they're kind of hard to separate, we tend to call them vibroacoustics. And then, of course, there's all these specializations of bioacoustics and structural acoustics, so you listen to sort of the creaking of an old house or the sway of a steel structure, and all those are, you know, can be used for productive uses and are all specializations of basically acoustic monitoring. So just some quick units and terms. Hertz is the frequency of sound. So if you have 1 Hertz, that means there's 1 oscillation per second. If you do 10 Hertz, it means there's 10 oscillations within a one-second period. Hertz is just the number of times a repeat within that one second. Amplitude is the magnitude of that. So you can have something that's 1 Hertz and very loud. You can have something that's 1 Hertz and very small. And those are totally independent. Other units of measure are basically amplitudes. You can think of it as if it's units of pressure in a liquid or in a gas, you typically don't think of it as units of pressure in a solid. You can think of it in pascals, which is just atmospheres of pressure, essentially. And because human sound perception has this incredible dynamic range, we don't actually think about things linearly, but we have a logarithmic, kind of compressive, we sort of, we do a log transform on sound. So when you hear like a cricket in a forest at night, very, very weak in terms of the number of actual, like, you know, like the amount of force hitting your ear, all the way to like a jumbo jet taking off, those are, I don't even know, ten orders of magnitude apart. We don't hear them as it being a million times louder. It's like maybe ten times louder. So we know that humans hear on a log scale. And then finally, another one that comes up a lot, especially in AI, is this notion of ambient noise or background noise, is that even if you're totally silent, there's still things going on. So there's the creaking of the floor, there's the HVAC that's blowing, you know, there's like other little thing, people fiddling with a pen or something like that, that you have to filter out. So if you're going to build sort of like machine learning or deep learning that's going to process sound, the interesting signal, the thing that's produced by the human, like my voice, has to be on top of, to filter that out from ambient noise. And it gets, you know, in this classroom it's pretty controlled, but out on the street with Google Voice having to translate your voice, and you're like, this car is going by, this police car is going by, there's people honking, you have to compete with that existing noise. So that's called ambient noise. noise or background noise. It's basically the unwanted noise that's different from the signal that you care about. So human hearing, we have an incredible span to go from a 20 hertz of 20,000 times per second to 20,000 times per second. The age range varies. So you're hearing range varies based on age. So when you're younger, like you guys, you can hear a much broader range. I'll actually do a little test later in the class to see how high people can hear some sounds and to the ultrasound. For older people like myself, we lose it around 16 kilohertz. It gets really hard for us to hear. But most of you should be able to hear up to about 18 or 20. Maybe a few people can hear about 20. We'll find out. But that's also very human-centered. In fact, we just submitted a paper for publication called Exoacoustics, like exoplanets, where we actually, all these speakers that are out there, like Alexa and Google Home, even the microphones that are in those products, they're all tuned to the human hearing range. They presume they only need to listen to human voices. And we did research like a year ago called Yugacoustics, which is basically using microphone data to recognize the seeds. Like your smart speaker knows it's in the kitchen and it knows you're making dinner. We're using sound and deep learning to recognize all these different sounds. So when you're chopping or when you're mixing something or when you're stir-frying something, your smart speaker would know and maybe could actually automatically advance a set of recipe instructions for you automatically. It doesn't have to be like, hey, Alexa, what's the next step? And it's like, add the onions. And you're like, hey, Alexa, what's the next step? It's like, stir-fry the onions. And it's like very ridiculous conversation. If you had a master chef in your kitchen, a lot of this would be done by gesture and gaze and other human-to-human channels. And so we want to make things like Alexa much more intuitive, because as soon as you're done stir-frying, it's like, that's enough stir-frying. Now add the egg. And at the end of the crack game, it's like, OK, now I'm going to have to do this ridiculous kind of back and forth. But we found that all those microphones are tuned to basically human hearing, and that actually makes recognition a lot harder, because if you can suck in in front of a microphone, you can actually hear and ultrasound components, you can actually improve your machine learning accuracy a lot. And we show that in quantified in the paper. And interestingly, another result that one of my PhD students found is that we can actually build a microphone where you can actually can't hear human voice at all. So you actually filter out the human voice, so it's innately privacy preserving. And even if you subtract out that ring, it's a human voice, so if you yell at this thing, it won't recover your voice. You can still keep machine learning accuracy really high because we're using these other exo-frequencies, which is kind of an interesting result. It's in review, so we'll see when it gets published. So if we gotta get away from this sort of anthropocentric view of sound, we look at something called the animal frequencies that have this incredible range of frequencies, we do know that a lot of whales communicate at very low frequencies. And it turns out that low frequencies propagate really well, much better than high frequencies. So if you've ever gone to a house party at Pitt or walked through Oakland on a Friday night, you can always hear the bass coming out of houses. We don't hear like the whatever, T-Swizzle or whatever lyrics, what is it, T-Swift? What is it? T-Swizzle? There you go. I'm in the in crowd. So you don't hear that because the high frequencies basically get absorbed, and they tend to be quite in much more direction. But whales, because they know that low frequencies really propagate super far, especially during the ocean, and so they found that whale species were just like communicating for no reason, and it turned out they're actually communicating with whales like 1,000 miles away. Like even sometimes between the Pacific and Atlantic Oceans, right down in sort of the south, like here near Patagonia in Chile and Argentina. And that's because low frequencies in water can go fantastically far. And they're even diving to depths and particular densities and salinities that were perfect for a particular frequency range, which is awesome, it's like super, super cool that whales have sort of figured this out. I'll give you one other example that I thought was very, very cool, just reminding me, is there's this great. example of sort of natural selection and sounds. They went to this forest, I think it was in Ecuador, this cloud forest, they had these little tree frogs, and it had the same species of tree frog, but they lived at two different altitudes. There was one of those who were kind of cloud forest dwellings, sort of on the slopes, and there was one that lived down sort of more like in the lagoon area. But they were the same species, so they get, you know, mate and basically have fertile offspring, right? So, you know, sometimes you can, like a, whatever, a horse and a donkey can make a mule, but it's too fertile. Now, these two are still the same species, but they're sort of two different breeds or races of these species of tree frogs. But they noticed that these two different mating calls, okay? And what they did is they went up to the forest, they went up to these two locations, they set up a speaker at one end, and then they went like a hundred yards into the forest and set up a microphone. And what they did on the speaker is they did a frequency sweep, so it went like, all the way up to a hundred, every frequency from one to, let's say, 30,000. And then, at the microphone, they basically saw how much of that sound made it through the forest. And because, you know, basically the forest acts like a gigantic filter, it's like a high-pass filter, low-pass filter, whatever it wants to be. And so what they found when they did this plot, you know, from like zero to, let's say, oh my gosh, is, you know, it wasn't certainly going to make output of just a linear response. And so if it was just like in a perfect football field and there was nothing in the way, you might get something like this. But in the forest, in the ferns, in trees, it was sort of like non-linear. There was parts where the sound got through, and there was parts where the sound got really kind of inward. Never went above, sorry, never went above to get amplified. So that was like the response. Sometimes it was low, and then sometimes it almost made it entirely through sort of unscathed, and then it went low. And then for species B or whatever, like the other kind of subspecies, because they were in a different habitat, they were like in these low-lying kind of lagoon areas, they had different sort of areas. where the frequency went down. And they then compared the mating call frequencies to the environmental frequencies, and lo and behold, it's like right at this optimal point. So that's why the two species used two different frequencies of sound for their mating calls. But there was this selective pressure, evolutionary pressure, that they had slowly in their own environment been nudged towards these two different frequencies that happened to be the perfect local, like perfect maxima for their response. And you can imagine from a natural selection standpoint, is that every little frog that had a little bit of variance, and every frog that gets born is like plus minus 10 hertz, right? But the one that's just a little bit, no, so one frog, this is like the old frog, but then he adopted it as plus minus 10. This one does a little bit better, over a million years, and this one has plus minus 10. And eventually you just basically just walk up this local maxima until all the frogs are perfectly at this one fundamental frequency of the forest. It's like insane. It's a beautiful example of natural selection and evolutionary work. Again, you run that simulation a billion times. It doesn't help for any one single frog, but you run it for 10 million years with a billion frogs, and yeah, everyone ends up in this perfect little frequency band. And the other species that lived sort of lower down went into another little maxima. Anyway, so here is just a plot of all these little frequencies that I picked up. So here we are as humans here in purple. And so this is our frequency range. As you can see, we're nothing super special. Actually, it's reasonably broad, but there's certainly other animals that have wider frequency ranges. And again, it's based on what's useful. It takes more effort, evolutionary, to have a wider field. That means you have to attend to more frequencies of sound. And so there's probably a very good reason why humans are in this range. And it is highly tuned to our vocal range and sort of environments that we've probably evolved in. But other thing is especially like a prey animal. So sometimes you see like gerbils here. Here's like a ferret and a guinea pig. Those are always getting like eaten by like snakes and vultures and bigger carnivorous animals. They're very sensitive to low frequencies because they're often underground. So they can hear the little pitter patter of predator is coming and they can hide underground. So they're very tuned to these low frequencies. We're humans, we're not hiding underground from predators, animals, yet, given a million years. And then maybe we'll evolve to, you know, or there'll be a pressure to basically sense lower frequencies too. So we already talked about dynamic range in vision. You know, we can see like all the way from a candle like a kilometer away to like in the Sahara with full sun. Incredible dynamic range, like, I don't know, eight orders of magnitude. The same is true with hearing, about 120 dB of dynamic range. Every 3 dB is basically doubled because it's a logarithmic scale. So we can actually see over a trillion, a factor of a trillion hearing, which is incredibly impressive. So here is a scale, so all the way down to a pin falling. Like if this room was perfectly soundless, drop a single pin, probably most of you would be able to hear. But how much energy did one little pin make and make it to your ears? It's incredible. All the way to like someone's barred out shotgun, like really, really, really, really loud. And we can hear that entire range and actually process it. For most of you, think about like a speaker, like, you know, everyone has like a little sound control on their laptop, right? And if you set it too high, everything kind of clips, right? You see it go into that red zone, it sort of clips if you set it too low. So our ears are amazing. Like this laptop, there's no way this laptop can hear all the way from a pin dropping to me shooting like a shotgun at it. And I want to do that with my touch button, my knack of this, not so much. I always want to shoot this thing. So here is our actual threshold of hearing. So this outside area, so this is on here is the decibels of sound. So obviously the louder something is, the more sensitive we are to it. And then here's the frequency. And so this outer perimeter is basically the maximum that any human can really hear. And you'll see that our vocal range sits in here. And you'll also see it's non-linear. So you know, we're, even though it's very quiet here, we're basically in like a kind of zero maybe a few beats here, but around 3k, we're the most sensitive. That's, you know, the pin dropping probably land somewhere like in the three kilograms. But if you go down to something that's low, like 20 Hz here, it has to really be quite loud for us to detect it. So it isn't just a straight line. It's an envelope around this area. So here's where humans speak. This is the vocal range. You can see some of the mics and nets in here. If you look at an orchestra, like music, this is where all the range of instrumentation you can make in an orchestra. It would be weird to make instruments that are entirely in the infrasound or ultrasound because there would be lots of people playing on stage and it would be totally silent. So most of the musical instruments that we've made tend to end up in our auditory range. The other thing that happens a lot in acoustics are resonant frequencies. So when you strike something with mechanical force, it tends to want to oscillate at a particular level. You know this. Like if I take a brick and I hit it with a fork, it's going to sound very different than if I take a wine glass and hit it with a fork. And obviously, you're probably all familiar with tuning forks. They're all made of metal. And when you hit them, you can basically produce different notes. And that's because when you impact something, it tends to want to wobble at a very particular frequency. Like if I slap a pile of gel, it's going to go like this. But if I slap a pile of steel, it isn't going to go like this. And so then when that mechanical energy interfaces with the air, it's going to emit a sound that's going to be based on its resonant frequency. You can also do this with a pendulum. So if you have a string with a weight and you let it go, it basically follows a particular period. So we know that resonant frequencies exist. And the resonant frequencies are useful. And I'll show you some examples where we can take advantage of these resonant notes. This is probably the most famous example of resonant frequency gone wrong. So during World War II, that's actually before the US was in the war, but during wartime, they built a brand new bridge in Seattle, south of Seattle in Tacoma. And they called it the Tacoma-Navarro Bridge. And it was this great new thing. It was basically a suspension bridge. And it was pretty substantial undertaking. And I think at that point, they might have even started to ration steel. And what they found is that the wind, because it's quite windy there, kind of whistling through the... the cables started to induce this oscillation and eventually the bridge actually fell apart. It was a brand new, tens of millions, hundreds of millions of dollars bridge and it only lasted about six months. It was a classic example of like an engineering fail because they didn't really fully model or understand the interaction of wind on this structure. And it's not that people often attribute it to like resonant frequencies, sort of like the wineglass example, but it's actually not true. It wasn't really making a particular sound. It's this other effect called aeroelastic flutter, sort of like what you see if you see a leaf in the wind, it vibrates a lot. And basically you start shedding these vortices on all the opposite sides and it induces this kind of frequency. But it is a resonant phenomenon because it's very much like a swing. Like the wind is not enough at any given moment to cause this oscillation. So a lot of things in resonance are sort of like a swing. So if you have someone, how many people have been on a swing in their life? Raise your hand. How many people have never been on a swing in their life? Okay. Other than the people that are on their laptops, everyone has been on a swing. So with a swing, everyone knows pushing on a swing is that you don't just, you don't just grab them like, hey man, you don't just push them up to maximum height right away unless you're really strong, right? What you do is you push them, they come back, then they come back, and then at the high point you push them again. And what you're able to do is even though you only have like 20 newtons of force, is every single time you can put 20 newtons in, 20 newtons in, and eventually it's sort of simulating sort of like 100 newtons, right? And that's the key with resonance. When you're out of resonance, like if you've ever tried to push someone who's at the bottom at sort of the maximum velocity and then you try to push them there, then what happens is you counteract your forces and they actually go less high, right? So in order to get someone tying the swing, you have to time it just right. So the frequency, the hertz, has to be sort of right in mesh with the period of the person. And that's what happens here. The instantaneous force on the wind is very low, but the wind was at just the right frequency that every single time sort of the bridge turned back over, it was ready to go again and it built these oscillations from small to big. And that's true for anything. And that's also why I mentioned even a few lectures ago, I think, that sort of the wine glass example, right? Like, no one has enough power in their voice to just shatter a wine glass just by emitting sound for a tenth of a second. The way that that effect works is that you get to the perfect resonance of the glass, and it builds and builds and builds and builds just like this until it shatters. So again, it's about resonance. It's hitting that perfect resonant note. So here's a great example of a project that does this. Let's see if I can sound like this. This is one of my favorite little, like, acoustic app projects done by some Japanese authors. So this is a person tapping, and you can hear that these different objects sound different, right? We all know this effect really well. They have different resonant properties. Now, what's interesting is if you touch it, it also changes, right? There's nothing special about this object. It's just like a toy. You're adding to the mass, and therefore you're changing its resonant frequency, just like if you held a wine glass. And so the idea here is that you stick two things on it. One is a little vibration speaker, like a little piezo, basically a vibration motor, and then one's a little guitar microphone. And what happens is, now, when you touch it, it makes a very, so this is the, oh, I'm already getting rid of this, frequency sounds like an FFT, Fourier frequency, sorry, Fourier transform. And you can see that when they touch it, it's performing. So there's ultrasound being put into this object, and when you leave it alone, it just has a characteristic resonant mode. But when you touch it, like when you grab its hand, it's altering the resonant property of the object, and that can be detected in machine learning. They're training it, and again, it's just a regular toy that they stuck a microphone in the speaker on. So after they've captured some training data, they can train it, and now you'll see it can recognize. So if you recognize it's being put on the left, it's recognizing that you're touching the head, or that you're covering. gears, I think. So we've been able to make something that was not interactive, just this like plastic toy, ceramic toy, interact. So it's a really nice example of being able to bring interactivity to something that would otherwise be totally static and passive. So here's actually a table. They've attached, again, their speaker and their microphone to this table. And they can actually perform their different gestures on this table. There's nothing special about this table. It's just a regular old table. Here's another interesting one. This is like a Lego set. So there's, again, this really interesting thing you can do with it. And they've shown that you can do even different buttons on top of the Lego. Here's an interesting one. So when you plug on different Lego blocks, again, it changes the resonant frequency of those Legos. And so each one of these is slightly different. They're slightly different heads. And now you can turn it. And when you put these on the Legos, again, they just break. Play some sound effects. So it's a very clever use. It's a big acoustic hat, but it's a really nice example of what you can do if you understand acoustics. Okay, so we should also talk about the propagation of sound. So, you know, travel to different speeds, different densities, different temperatures. Here's an example of the sound of a sound under water. So based on pressure, it propagates at different speeds because the molecules are closer together. The speed of sound in air, which we know is Mach, is about 340 meters per second. That's like 600 miles an hour, 650 miles an hour, something like that. And we call that Mach 1. Mach 1 is only unique in sound. Mach in water, we don't have that in water. It's somewhat faster, less than 1,500 meters per second. But the speed of sound in water is much, much faster. And if you look at it in things like iron, or like people have used it sometimes to figure out like, like espionage reasons, like if a train is coming on a track, you can actually put your ear onto the track. Not that I recommend this. Here's a train from many, many miles away. And you can even sometimes infer how far it is away. And so people have done all these calculations for all these different sort of materials. Like if you want to test what something is, and that's a very interesting project that I'm advising in computational architecture, where they're basically just tapping on walls, like on old buildings. They want to know about the thermal constraints of the building. And so they're tapping on it. And by how long it takes for the tap to get to the other side of the wall, they can say, oh, this is like 2 3rds brick, 1 5th drywall. And there's like some insulation, and there's a little bit of wood. Like it has to be decomposed. They know how long it should take. And they can sort of reverse engineer the time of flight through the wall to figure out what the materials must be, and therefore, give a guess about what the insulation quality might be in the wall, which is a really cool idea. It turns out it's really complicated. It's a whole PhD thesis on this stuff. It's a really cool idea. So again, here's all these different speed of sound and so on, all these different interesting materials. As I mentioned, high frequency sound tends to be much more directional. So in air, it's a very sort of squishy medium. Like we do it like I can move my hands through it. It isn't like a block of steel. And so it tends to absorb sounds, where low frequencies tend to travel a lot less far, a lot less direction. That's why you can hear it with that face. So here is one of my projects. This is taking advantage of that propagation delay. So this has anything to do really about resonant frequencies. But it's taking advantage of, for input, the delay it takes for sound to get to a microphone. So what you're seeing here, let me get this to play, where in error, the four dots in the center represent the sound now. So what I'm doing, I'm just tapping on this table. Again, very much a prototype. And what you're seeing here, it's actually really impressive, is we find the time stamp. So there's four microphones under this laptop. And what we do is we find the time of the first one. So here are the four microphone signals. And we find the one that gets actuated first. So always one of them is going to be zero. And then what we do is we calculate how long it takes to get to the first one. square to error. The four dots in the center represent the four dots in the center. So again, one of these is always zero, and then we basically iterate along the wave on all paths synchronously until we find a peak that exceeds some threshold, and we calculate how many, let's say microseconds it is delayed. So we know the delay of how far it gets in each one of these four items. And so then, now the laptop is trying to guess, in this case we have a projector, so it's trying to guess where you notice angular distance. So here are the four microphones that we use. Let me actually ask you guys a question on this because it's sort of interesting. So if I give you the following problem, I hope everyone can see it. So if I have a microphone here, and a microphone here, and I tell you that the sound arrives at them at exactly the same distance in time, so there's no difference. Can everyone tell me where the sound must have originated? Here are my two microphones, and they arrive at the same time. Yeah? Right. And so it could be here, it could be here, it could be here, it could be anywhere. As long as these distances are identical, the sound will reach them at exactly the same time. So let's say I got triggered at time, so anywhere along this line. It's right down here. It's not a perfect line, but it's right down the middle. Okay, now what happens if I tell you that I got to this one at time zero, and that I got to this one at time 21, let's say. Okay, so it's slightly delayed getting to here. Anyone tell me where it might be? Yeah. Your guess is another line here. Not quite. It's more complicated. Yeah? Well, I think I know. You tell me. To the right. Sorry. This way? Not that far. Yes. Well, in this case, it's getting closer. That doesn't quite work out. Would it be like a hyperbola? Can you describe how that should look to me? You know, make coefficients, dude. Yeah, we've got to fix that. So how would it roughly look? Like... Which way would it arc? This way? Yeah, just like make points so that, like, the... that the 2, 2 is aligned. Yeah. I'm not sure if it's a hyperbola or parabola, but it's basically a curve function. And that is that it's always sort of constant time here, but you're always exactly, you know, some fixed angle, one centimeter away from this. You know, whatever that may be. So it is some sort of a curve. And that is a deep curve. That's what you get if you only have two microphones. It's not super helpful, because all you know is that you tap somewhere along, you know, this curve. But that isn't until you touch it. So what we do is we use four microphones. So here, this is sort of a simulation. So you have a tap that occurs right there. This wave propagates out. Thank you, Keynote. It's, like, amazing all the animation you can do in it. You get this sort of effect. And then what happens is you can... For each parallel, you get actually six lines, right? And you can basically do it between here. This gives you a curve. And you do it between here. It gives you a curve. You do it between here. It gives you a curve. You do it between... Yeah, all four of these. And then across the diagonal. So you actually get six curves with guesses about where they are. And so here are those curves. And what happens is you find where they all agree. Because you know it must lie along... One of the... It must lie on all those curves. And so you find the one that basically... It never works quite perfect, but you basically find one with a minimal error. And that's where you saw that kind of crazy rainbow pattern on the previous one. That was basically doing sort of a gradient descent style guess of where it must lie. And so that gives you this angle towards the thing. It also does give you a distance, but the distance is very inaccurate. So just by using time of flight, the whole point of this project was, even though it's a very sort of goofy prototype, is you could have, for example, this MacBook on the little blackboard. that are on the bottom of Apple laptops, those could be little acoustic sensors. And so when I go to Starbucks and I put my laptop down, why can't I use up the whole table for touching, why can't I control Spotify just by tapping on the surface? Like this is hard to control. You get it for free, right? It's not like you don't have to do anything to that table. You literally just put your laptop down, your smartphone down on the table, and now you can actually use that whole table as an input surface. So it's sort of this idea of why don't we just steal surface area from the environment? Okay, so another thing that we should talk about is a Fourier transform. So it's often just kind of shorthanded to FFT, a fast Fourier transform is the algorithm that we tend to use. And the idea is that when you typically look at sound, is that you get a representation like this, which is the amplitude over time. But you don't have to always have these particular actions. You can recast this exact same data as not frequency over time, but rather frequency over amplitude and time. So this would be called from a time domain signal to a frequency domain signal. How many people have heard of an FFT or a Fourier transform, raise your hand. Okay, good, sorry. I won't go into too much detail. I'm not gonna quiz you on Fourier transform, so don't worry about that. But basically, these two things, the key thing is that these things are interchangeable. You can actually go from one to the other and back. And it's basically just a frequency representation of the exact same thing. You've actually all seen FFTs. Ever seen an old, like, stereo in a car? Or if you have a nice stereo at home, you'll often see sort of the EQ bars, right? Where it shows you where sort of the bass is and where the high frequency is, you sort of see it playing along with the music. That's just basically an FFT. It shows you where different frequencies have different patterns. So if it's only got bass, or it's only got like a cello or something, you'll only see sort of bars at the low end. If it's someone screaming or they're only playing the triangle or something, you'll only see frequency at the top. Just to give an EQ bar on a stereo system is basically what a Fourier transform is. You don't see it moving in time, right? You don't see the EQ bar sort of scrolling by. You instead see sort of the frequency broken down. Okay, so let me actually show you, I wanted to actually show you, because hopefully everyone in this class is going to be at least a mini expert in processing, and processing is still a really fantastic tool. And so I want to actually show you some things that we can do in processing that make playing with sound very, very fun. So let me, maybe I'll actually just share my monitor here to make this a little bit easier. Okay, here we go. So here are some basic items, so let's open this up, and I'm going to share all this code, if you want to play with it after class you can. Okay, make this nice and big. Okay, so in this case, all I'm going to show is just the waveform, so just to show you how easy it is to do all the lines of code, is there's a great little library called Minim. You can just download the tool, Minim. I'm setting this up to show it looks pretty familiar with you. I make this Minim object, and I get a line in, which is a microphone in channel. And then here in the draw function, what I do is, for the current buffer, I'm just iterating from zero to the size of the buffer. In this case, I think the default is like 1,024 samples. And what I do is, I'm just drawing a line over and over again. So I go from I, the I plus one, just drawing little line sections for the length of the buffer. So you can, don't worry about that. I'm just going to go ahead and do that. Okay, so I'm just going to go ahead and do that. So basically, it's just going from all the x's, from zero to 1,024, and the height, except for the height, in this case, I'm only getting the left channel. So this is exactly what my voice is, according to a computer. So it's called PCM, and it's basically sampling very, very fast. It's sampling around 44,000 times per second. At any given instant, as my sound hits the microphone, it's deflecting inwards. You can imagine if I hold a sheet of paper and I yell at it, it'll sort of bend and oscillate. And so the amount of deflection is really just digitized as a number. So when it's perfectly flat, totally silent, and the diaphragm is at 0, the value is 0. If I yell at it, it's going to swing back, and that might be the maximum. Let's say it's negative 1,024. Then it'll oscillate. So if I say something, it'll start to oscillate, and the volume of the oscillation is literally the number. It's literally just capturing the displacement of a little tiny diaphragm of metal, 44,000 times a second. And that's literally all my voice is. Now, my voice is a complex arrangement of different frequencies. So if I want to see a sinusoid, I would need to produce a constant tone. And the easiest way to do that is to whistle. So if I whistle, you'll see it's a sinusoid. It's a sinusoid. It's a sinusoid. It's a sinusoid. It's a sinusoid. It's a sinusoid. And I can change the frequency, so again, it hurts. So if I do a low frequency, you'll see it's wide. And if I go high frequency, you'll see it's a little bit tied together. Now, I'm not the most amazing whistler. So you can see that my voice is made up of many different sine waves over that. But a whistle is the most amazing. Why don't you all just whistle one note, and we'll see what the conglomeration looks like. So whistle, please. So you can actually see, it's like a superposition of lots of different sinusoids. So you can see, it's still something sinusoid-y, but they're all overlapped, where my voice has all these complicated octaves and harmonics and so on. But that's all that this is. It's just a little guy from the digital computer, and you can write it as a CSV file, and that literally is what a WAV file is. If you open up a WAV file, it's basically just a CSV with all the numbers. There's no compression. MP3s and stuff are much more confusing. So this is it. So you can play with this code pretty easily. OK. Let's do something a little bit fancier. Let's do the plot the amplitude. So in this case, I've done something a little bit more complicated. I'm still going through, and I'm plotting the line. So in this case, I'm only plotting the max. I'm moving through the buffer, and what I see is the maximum sound value. So I'm going through the buffer, there's all the displacements, and I'm just recording the max. In this case, I just draw the max over time as I'm looking at the increments. And let's see what this looks like. So now, it's just plotting at any given buffer. Every time it calls draw, it's just finding the maximum value in that audio buffer, and it's just plotting it over time. So if I talk, you can see it's loud. If I clap, you can see there's big claps. And you can imagine running something like this in the background. If you wanted to build something like a clapper for your laptop, like every time you clap twice, it reads you how many emails are in your inbox, you could totally do that. And it's actually quite distinctive, a double clap. Because people tend to clap twice, they don't clap loud and then soft. They tend to clap in a very repeated pattern that's quite consistent. And you can basically say, if I see two peaks of exactly the same height in a certain amount of time, then trigger whatever on my computer, you could totally do that. It would actually be very false positive resilient, which is nice. But same exact data again, you can play with this as well. You'll notice that nothing is going above this value, and that's because my speakers are actually capping out. So I can't actually go above that. If I open up my sound panel, you'll actually see, if I go back to here, sound. So you can see down here, oh, it doesn't show the input. So on your Mac, they have this sort of sound thing. And if I clap, you'll see it's going to the Mac. And that's basically because most of that little piece of metal that moves out of the way can't go anymore. And that's why you basically get a clipping of the value. And that's true in this as well. You have a question? Yeah. Just when I draw, what was the exact reason that time is visualization? It was time. So what happens is every time you call draw, so think of 44 times per second. It's basically giving you 1,024 values. Every time you call draw. there's always a new buffer available from the sound card. It just moves to the left. Well, you don't see it moving to the left, because I'm drawing every single frame instantaneously. If you kept a longer buffer, though, and you sort of appended it, like a Q, then you could actually see it moving on the left. Just to keep the code simple, I didn't do that. OK, let's close that down. Let's go on to something more complicated. So now let's look at an FFT. So now, here, I'll show you the code quickly. So to do an FFT in Minim, you just include this Minim.analysis. In this case, I'm setting up Minim, getting the line in before, but now I'm getting this new FFT, and it needs another buffer size in the sample in order to convert the frequencies. As before, I'm just drawing the line. And then I call this special function called FFT forward, which basically is to forward, because I'm taking in a time and signal, and I'm projecting it into the frequency domain. But I could also do an FFT backwards, where I take something from FFT and project it back into the time domain. You can go back and forth. In this case, I do an FFT forward on the left channel, and I get basically the FFT. And I can draw that FFT digest to M times I. So I'll just run this one, and now you can actually see. Zoom in again. So again, at the top is my voice, right? And then here is all the frequency components that make up my voice. And you can see it's really complicated. A human voice is composed of scores of different frequencies. But if I whistle, and it's one sinusoid, it's one frequency, then I should really get a very sharp peak. It's not going to be perfect, because it's still an estimation. It is a fast Fourier transform, not just a FT. And so you'll see it's a little bit noisy, but let me try this. And you'll see if I whistle higher, it'll shift to the higher frequency, which is down here. Here, so now I want you guys to whistle, and we'll see how many different frequencies we get. Whistle again. So see if you can find your own frequency and control it. Now it sounds like wind whistling through the Tacoma Harris Bridge or something. So this is a frequency spectrum, really easy to do in Minim. And again, you can actually look for patterns. Like if I were to track that peak over time, so if I wanted my laptop to respond to some kind of weird pattern, you can just imagine iterating through this array, finding the peak, find the index of the peak, you track it over time, pass that to a little class file, something very rudimentary, even just a couple if statements, and you can imagine unlocking your door or whatever you want to do. So it really makes it sort of fun to play. Now there's one more thing that I should try. Let's do output this time. We'll see what people's hearing is like. So right now I have it set, so here in this code I have it making a new oscill, which is an oscillator, and I have it set to 440 Hz. How many people know what 440 is in like musical scale? No one? It's an A. So I'm going to play this sound, and we'll hear what it sounds like. So in this case, I'm even going to draw the output of what I'm sending to the sound card. Oh, sorry. Sorry, sorry, sorry. I realize that I'm setting the mouse. Uncommon mouse move timing. So let's just output an A. We'll get to that crazy noise in a second. So here's an A, and here's what an A looks like getting passed to the sound card. So that's what 440 Hz looks like. Let me see if I can reduce the sound here. Where's the remote? Let me go back to playing 440. Easier than that is just turning off, going to the restaurant. So here's 440 coming out of my laptop. Okay, nothing special there. Let's go up to 1 kHz, so 1000. So now what I'll do is what I've had before, which is we'll actually control this with the mouse. And we'll see how high we can hear, okay? Okay, so in the bottom corner here is it's outputting our current frequency. So right now it's 3000. I presume everyone can hear 3000. Raise your hand if you can hear this. Okay, here we go, we're going up. So we're at 8500. Everyone can hear? 12,000. Hear it? 14,000. How many people can hear this? Now let's go into the 15500. How many people can still hear it? No one can not hear it anymore. It gives me really ear piercing. Can't hear it? It's getting old. Yes. Okay. Let me move this over so I can see. Okay, up to 16, 955 down here. How many people can not hear this anymore? And again, it's harder in the back row. Can't hear it? Some people can. Okay, let's keep going higher. I can just not hear this when I'm standing in front of you. How many people can still hear it? 17, let's go to 17. Anyone can still hear it? So that's off, that's off, and now it's coming back. Who can hear that? No one can hear it? You can sort of hear it, and it's very high-pitched. So most of your hearing is, and my speakers aren't amazing on this laptop, but it's sort of dying. It looks like we're about 17300. Let me go up a little higher so anyone can hear it. If anyone can still hear this at 1800. Turn it off. On. No one. Okay. That's good. That's kind of interesting. So you guys, most of you sort of lost around 15. Started to drop off pretty significantly. That's not atypical. You normally have to be another 10 years older, though. Okay, so now we've done that little test. One final demo, which is a bit more interactive, is we're going to combine two things, which is FFT and I'm getting output from my speakers. So if I look at this code, what I'm doing here is I'm combining a couple different things. So I'm setting up an FFT as before. I'm also setting up this output wave. In this case, I'm sending the output at 18000. So right at that high end. No one could really hear it very well. And I'm sending out this ultrasound, and no human can really hear it. And I'm also recording from the input, and I'm going to plot the same things I did before. The data that's coming in and the FFT. And let's see if anyone can guess what's going to happen here. So my laptop is emitting ultrasound. Okay, so you can actually see that ultrasound on the top. And you can actually see the ultrasound. This is actually getting modulated on top of my own voice. And so, unsurprisingly, my microphone picks up this gigantic spike at 18000. There shouldn't be any surprise there. Can you hear something? Yeah, sorry. So now, if I do this, what's happening? You can see there's a signal change. The question is, why? I'm not wearing any sensors. ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... So, again, you can play with this code. I'll turn it off so people aren't gonna bleed out of their ears. And you can play with this code. It's very, very flexible. And we'll do some other, we'll probably do a hack with it next week as well, with some machine learning added on. But anyway, this shows you that acoustics is a very nifty domain. And again, we're surrounded by air, we're surrounded by liquid. We're made of liquids ourself. There's all these sort of interesting low-hanging fruit and how to utilize and kind of harness acoustics and vibroacoustics to do interesting HCI stuff. Okay, let's go back to the presentation. Let's go back to Prestron, displays. Probably heard some frequencies you've never even heard before. Keep in mind that the speakers on your laptop, if you try to run this code, they don't really like to go much above 18 kilohertz. Because again, they've never been designed to go that high. And so, if you try to set it to like 22, you just won't, your speaker won't do anything at all. Anyway, so, processing's less minimum. If you wanna do cool, like, acts of minimum, definitely take a look at this. Very nice kind of documentation. They also did interesting things. You can do input, you can do output, you can do filters, you can actually have a nice beat detection output. So if you wanna have some crazy Arduino that's like flashing lights every single time you clap your hands or hit a drum, they have a beat detection library automatically built in. You can do audio recording, blah, blah, blah. Okay, so, HCI and acoustics, lots of cool things to be done. I've already showed you one of my projects. This is an early example that I think I also showed you a lecture or two ago, where they basically put a little microphone on their wrist and they did different actions like dialing, writing, rubbing, and clicking. I don't know if drum triggers exist. But basically, they were able to actually recognize what your hands are doing from a smartwatch. Here's an early one of my projects. In this case, I was using a stethoscope. It's a really remarkable, you know, Victorian era device that you can see. I hope this video is not a flip, I don't think so. And basically, it's listening to actually the scratches happening inside the table. So I'm using, sort of like. fingernail and where I run on the surface, where I rub on the surface is the interpreters. In this case I sort of made this gesture set that if you did like a like an N which is like that stroke, stroke, stroke. You can interpret that if I do like an A, kind of upside down triangle, like an answer phone. Here is like this little sensor attached to the wall so it's coupled with a drywall and you can walk up to it and basically interact with it. So I can tap and again it's thinking about like all the walls in your house, like all these microphones could basically live in your light switch and instead of having to like attach these screens everywhere you could just have all the walls in your house just basically be an acoustic sensor. This project I already showed you with the different sort of cantilevered microphones in here. You know that you can do this with bioacoustics. This is a later project that actually, one of the technologies that I spun out into my startup company. It also is based on acoustics. So basically every single time you touch your screen you make a little sound, right? Inside of you, especially if you had a little microphone inside of your phone, you'd hear this little sound. You can imagine that how you touch the screen makes it sound different and especially because you're composed of different things. The tip of your finger is sort of fleshy, the knuckle is sort of bony, and the fingers, the keratin in your fingers is also quite different. If I just tap on this table you can hear it. Here's my fingertip, here's my knuckle, and here's my nail. So I mean, not surprising. So if you take advantage of, not the microphone, you actually use the accelerometer for this, inside the device you can actually classify the type of touch automatically. So what might you do with a touch screen that can recognize different types of touch? So here's one example. So again, you sort of have a light clicker, so you can actually do selection with your knuckle, tap that, highlight all this, and there's no long presses or anything like that. Here's another example, so in this case like image media, so I can scroll around if I want to cut something out, I can just highlight it anywhere on the screen, I can move it around. But actually what I think is the most valuable here is this, you'll see the arrow highlighted, it's like right click, I can tap on it, and I can summon a contact name, so I can post it to Facebook or Instagram or whatever it may be. And if any of you have a Huawei phone, they don't have a Huawei phone, and they're not very popular in the US, but I have one, and so it actually is running, they ship this, so if I was on Amazon and I put down my knuckle here and I do like a lasso, I get that little cutout. And there's a couple other features, but this technology, we just passed I think 200 million phones running our software, just in December, but you can't get it if you're an iOS user. Okay, another project is SweepSense, another one of my lab projects. So in this case what we SweepSense is an existing feature to emit inaudible ultrasonic frequency sweeps. And it uses existing microphones to measure the reflected signal, which changes based on the geometry and material properties of nearby surfaces. We built two demonstration applications. Here our software emits inaudible sounds through both earbuds. When in the ear, most frequencies are attenuated before reaching the interface. However, when the earbud is removed, the sound propagates more freely. The difference in particular frequency bands can be captured, learned, and used to trigger interactive events. Here a user receives a phone call, which can be answered by taking one earbud out. When playing music or videos, the media can be automatically paused if both earbuds are removed. In this example, we use a laptop's integrated speaker located beneath the display. Just to clarify from the last example, what we were doing is we were emitting two little... frequency switch like 19 to 20 and 20 to 21. One in the left ear, one in the right ear. So when it's in your ear, the microphone on that cable doesn't hear them because it's confined to your ear. But if you pull it out, now the microphone is sort of leaking ultrasound out into the world, and by which frequency band we pick it up on the mic, we actually know which earbud must be removed. And again, nothing special about these earbuds, it sort of works for free. So here's another one where it's very similar to the demo that I'm showing you now. If we're making ultrasound out of the laptop, reflecting off the screen, I built a little regression model. My command box here is the angle of the laptop. Changing the lid angle, in turn, changes the reflectance characteristics of different acoustic frequencies. We use this data to train a regression model, which can then act like a virtual lid angle sensor. So again, you get this sort of extra little sensor in your laptop totally for free. Of course, we didn't really have a great example use application for this. You can see that our demo here is a little boring. This can be used, for example, to switch between different modes. For example, it's the best way to bring something back to trigger a dashboard, but I don't know if people would like to use that in the future. So there's also work in ultrasonic Doppler. So using that Doppler effect, people have been able to fire it at people coming down the hall. So a lot of door openers you see at supermarkets are based on ultrasound. And as you kind of are walking towards it, they can see that there's not only a Doppler shift, but there's also increased reflectivity, and they open the door. But me and Victor Raj, another researcher here at CMU, we did this project where we actually kind of fired ultrasound down the hall, looked at the reflection. And what we're able to see is not... And this is an example of someone walking. If you think about it, as you walk, your arms are going behind you, so there's actually negative Doppler shift. There's positive Doppler shift. Your legs are sort of moving, and you become this sort of big mirror of ultrasound, or of sound in general. But we were using ultrasound, so again, you couldn't hear it. And you get these really interesting patterns. And how you actually... locomote is very distinctive to you, so your gait, the patterns of your movement are very unique to you. So we were able to, yes, not only who was coming at reasonably high accuracy, like 90% out of like 10 users, but also infer like the age and gender of people as well. That's how basically they reflect an ultrasound. It's kind of interesting. That's because the biomechanics of like men and women are actually just a little bit different and we can try to pick that up. We also were using, another really cool idea was to emit ultrasound from the bottom speaker of a phone. So you hold the phone like this, right? And so you can imagine emitting not only the, well, in this case, your microphone, but you could have a speaker here as well. And we were looking at Doppler shift off of the lips to help the speech recognition model. Because again, there's a lot of ambiguity in how people talk, especially if you're outside, it's really noisy. But if you can give the machine learning model, the Doppler, the movement of the lips in addition to what's coming into the microphone, it helps you basically better processing and sort of a language model. Interesting. This is exactly the project that I showed you. So at CHI, which is sort of a big general ACM conference that happens every year, that's kind of seen because of gigantic presence apps. So whatever, seven years ago, they actually published a paper on exactly what I showed you in class, which is that they emitted ultrasound from a laptop, used a microphone, and looked at basically the frequency shift to basically do gestures above a laptop. So what, in the end, I was able to write in like 15 lines of code, turned out to be an entire research project for like five people for a year. Technology has advanced a lot lately. And so they were basically able to show that you can, don't be like a lead mouse for anything crazy, you can literally just use sound above your laptop to do all these sort of interesting gestures and control power points and stuff. Here's another really fun example, just trying to bombard you with all these cool uses. So Chris, I'm just going to ask you, my, well, he's a professor in ISR. He was a PhD student at University of Washington when he did this, and now he's a professor here at CMU. So if you're interested in this sort of research, you can go to his website. definitely talk to him for things like human-to-human studies. But he had this really nice idea that what happens if we emit a sound, or if we turn on the vibration motor inside of the phone, and we look at the vibrational response. So everyone has probably heard at some point in their life a smartphone vibrating on a desk, right, with the vibration motors on. It makes this very particular kind of characteristic chatting kind of sound. And what they found is, well if we can pulse it, just turn on the vibration motor periodically for very short periods of time, and we look at basically the response, we can guess, are you on a table? Are you being held in the hand? Are you in a pocket? And so on. And so it's this notion of basically, again, it's sort of like, you don't have to build in a special sensor every single time. Is there some sort of way to hack the existing sensors that are in these devices to enable new capabilities? And in this particular signal, they showed that when you touch the screen, you know often when you touch on the keyboard, for example, there's a little, kind of like, you know, hack this response, and they could basically steal this, appropriate that. So when you touch, that's very likely, you don't dampen the oscillations in the phone nearly as much as if you push hard. So basically back, whatever, five years ago or four years ago, they basically already had done 3D touch like with Apple chip, you know, fairly recently, and they were able to do it with actually no additional sensors on the screen. They basically had a pressure sensing solution that came for free, you know, just soft, you know, soft. It's a really, really nice example of, sort of, appropriating signal. This is another interesting problem. I don't know where this name comes from. It's some sort of weird Scottish word, the same, but they have these sort of rocks, and maybe someone knows this. It's more of like, these sort of rocks that are decorated. And so they had this idea, you could have sort of like totally passive remote control, so you could 3D print something, with like weird ridges. This is a little bit of a low resolution image. You see sort of like, kind of, kind of ridges along here, kind of little kind of bumps, and in the center part there's actually sort of this corrugation, and then there's something in the middle as well. And the idea was that as you run your finger on these sort of, essentially almost like sandpaper, it's like structured sandpaper, because you 3D printed it. is the sounds it produces can be picked up by a microphone. It can either be in the device, or you can imagine it even out in the environment. And when it hears that sound, it can, whatever, control your TV remote control. And the idea is that you can 3D print these custom objects that could be artistic, even, and yet have basic input qualities. And again, they're just like a lump of plastic. It's really, really interesting. Sort of like me running my fingers over this keyboard, right? Like, that's a very distinctive sound that probably nothing else in this room makes. And that's basically, I'm not pressing any of my keys. I'm just hitting the ridges. And that's basically what they're doing here. So really kind of interesting idea about making objects that have input capabilities, but are just like lumps of plastic. Or you can even make it out of wood, whatever, really. So I followed it up a little while later and said, well, that's a cool idea. Maybe we can do this where we can sort of take it into a more structured approach and actually make acoustic barcodes that are actually like barcode lights. Here is, I don't know, just a quick video. Pattern, physical tags that, when dragged across, produce unique binary IDs. This ID can trigger a wide range of interactive functions. Here, we've attached a microphone to a whiteboard. This is what tags sound like to a computer when using a fingernail. So this is from a microphone. And a dry erase marker. And finally, a phone. So you can see the sound of it. Our system processes these waveforms and extracts an ID. You can actually see it. This is like literally the processing code that I just showed you in class. Is we just take that buffer, and we plot the amplitude that we see at every given moment. We shortened the buffer to make it 1,024. I think we went down to like 256. But it's really the code I just showed you, where we plot the amplitude of time. And what do you know? That it basically is encoding a binary ID. If you see a peak, we know there must have been a little groove that caught the fingernail or caught the dry erase marker. That's a 1. And if it's negative, it takes a redacted error. And so that's a zero. And so we can really just, we try to find the first, we find the first peak, the last peak, we know there should be 16 binary digits, so we just really divide it up, and then in each one of those we say, is there a peak or not, one or zero. Really basically, I was like, I could easily give this to you for a homework assignment, and you could do it. It's really not that complicated, code-wise. And then we can carry, you know, basically, I think like 10-bit IDs, something like that. Oop, no, forget it. And what we did in that project, I skipped the video, but you can actually take any object and basically laser etch or even stamp on these acoustic bars. So you could have, you know, you could be at Macy's looking through the window, and on the glass is like this little pattern, basically like an acoustic QR code that you can just basically take your phone and swipe it on. And it doesn't have to be quite as intrusive as a QR code, and it's done entirely with sound. Here's a cool project at Berkeley. So they actually used, thinking about fabrication with objects. We present Lamello, the co-design technique for tangible input components that are sensed using passive audio. You know, a big missing gap with 3D printing is you can make something that embodies the physical form, but none of the physical on the device system is a prototype object. And they wanted to combine prototyping with one thing. Times of different dimensions have different sound profiles. And they wanted to combine the sound Times of different dimensions have different sound profiles. A contact microphone clipped to the input device detects the sounds and interprets them as user interactions. Lamello types can be added to 3D printed work. Any traditional, for example, sliders. Very kind of clever, again, kind of a clever hack. And that's it. So again, when you 3D print your object, you can sort of get best in both worlds. You get the form and the function, sort of in one prototype, and you can grab it and leave it for a while. Okay, here's another last project I'll show you, because you're probably getting overwhelmed with acoustic projects, but we did this project called Acoustuments. It was sort of inspired by instruments, and we basically did a survey of all these different instruments you see in the wild, like trombones, and trumpets, and guitars, and whatever, and especially wind instruments. And there's very particular things they use. They basically change, like in the trombone, you're basically changing the cavity size, you might be changing the different pathways, and we know that that changes the musical notes. And we do that in ultrasound, but with a phone. And so we're going to turn 3D printed objects into these instruments, but with acoustics. So here's the, I'll let the video do the talking. Here we go. We introduce Acoustuments, low-cost, passive, and powerless mechanisms made from plastic that can bring rich, tangible functionality to handheld devices. The operational principles were inspired by wind instruments, which can produce expressive musical output despite their simple physical design. A sustained source of sound is injected into one end, and various physical elements are altered to produce distinctive outputs. In the same manner, we can create an instrument that attaches to a smartphone. One end of an enclosed tube is connected to the speaker, which emits a continuous ultrasonic sweep. The other end is directed into the microphone, which monitors the output. Like musical instruments, we can introduce structural elements along this pathway that can alter the acoustic power sweep of the output, which we can use for interactive control.\",\n",
       " \" You got your heads, if the screen comes down on us, I'm going to give you the cross-light screen. It's probably a good way to go. Okay, so you guys can start by putting your team number on your table, and if you figure out your participant, you can wear that tag. Okay, let's go ahead and get started. Oh, yeah, I was looking here. I was like, oh, great. Oh, what? Oh, OK. No, I haven't seen this. I know a reporter in the experiment zone. Yeah. No, I haven't seen this. Of the antiviral agent. The Hale, you know I mean. Yeah. So just really quickly, same procedure as before, you're going to get one trial, where you can sit down, you can tell them how it all works, you can talk to them, you can touch them, and then you're going to do the time trial right after. So get them trained once. We're going to test once. You can't have multiple. You're going to see people where it's self-typing. Like this, that's not allowed. You have to treat it like a smartwatch. They can hold it in one hand. And they can use multiple fingers, but it should only be coming from one hand. If you're incredibly dexterous, you can send your fingers over and test your own smartwatch. I would love to see a demo, then I will allow you to do it. But you have to show me now. You're dexterous. So it's going to be holding it in the hands or leave it on the table, and then the other hand is going to be touching the back of the hand. So we're going to do a trial number. It's going to be three. We're not going to go crazy. Some designs are going to be fast, some designs are going to be slow. So three, just a little bit more than the scaffold code. And you're going to be entering words per minute plus the penalty. That's before you're entering it with the penalty. There is the URL. And remember that unlike the other Madoffs, more is better. More words per minute is better. Higher numbers are good. Finally, before we get started, we also have one of our TAs, Judy. So Toby can't be here today, but Judy is a trusted source. So we're going to recruit Judy to try to get Judy laid back. So we've got her preparation. We're going to get going in one minute, so ask your final question. Any other questions before we get started? So, note by the way, if you have any questions, please feel free to ask them in the Q&A section. So, note by the way, there is a participant 15A and 15B. They are so badass, they have two participants. Okay, I can do that to 22. Okay, let's get going. So, participants come to the middle. And you can start to recruit. Okay, rolling. Okay, let's get started.  code and actually one inch by one inch, who is the person that has a ruler on hand? Can you identify yourself in that order? I'm going to come around and measure. I'm going to come around and measure. I'm going to come around and measure. I'm going to come around and measure. I'm going to come around and measure. Yeah, that was so smart. You should get prepared for that. No. No. All right. OK. All right. All right. All right. This is really good. All right. All right. All right. All right. All right. Yeah. All right. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. And I'm going to walk around. Oh, sorry. So the other thing that data recorders need to do is type in a very brief description of what your design is. Is it alphabetical? Is it number defense? Does it have prediction? Does it have prediction of the historical relief? Yes or no, type it in there. Thank you. Thank you. All right. All right. All right. All right. All right. All right. Now, I'm going to turn this over to you. Let's see. Sorry. Now, can we come down here? Yes, you can. OK. Let's get to it. Sorry. I'll be right here. Yeah. Yeah. That's fine. But we'll go to her. So I know. Oh, my god. Oh, my god. Oh, my god. Oh, my god. Oh, my god. It's fine. It's fine. You're fine. It's fine. You're fine. Now, I'm going to bring this back to the committee. Yeah, sure. That's fine. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. 2-14, you don't have a single sign on the board. And 2-16. Oh, yeah, yeah, yeah. Yeah, yeah, yeah, yeah, yeah, yeah. Oh, yeah, yeah, yeah, yeah, yeah, yeah. Team 14, 15, and 16, I still have no descriptives. I think it's fine. I mean, this one's fine. This one looks fine, too. Yeah. Oh, yeah. What was the question? I'm sorry. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. Yeah. It's OK. It's OK. It's OK. It's OK. It's OK. It's OK. It's fine. It's OK. It's fine. It's OK. It's OK. It's OK. Don't worry about it. It's OK. It's OK. Yeah. It's okay. Are you feeling better or stronger? Still okay. It's OK. It's OK. Yeah. It's not the worst I've ever been in a wheelchair experience. It's not the worst. I'm going to go through this whole area. Judy is available. I'm going to go through this whole area. I'm going to go through this whole area. I'm going to go through this whole area. I'm going to go through this whole area. I'm going to go through this whole area. Team 8 and Team 10, you only have one time on the other board. Team 8 and Team 10, you only have one time on the other board. Team 8 and Team 10, you only have one time on the other board. Team 8 and Team 10, you only have one time on the other board. Team 8 and Team 10, you only have one time on the other board. Team 8 and Team 10, you only have one time on the other board. Yeah. Yeah. Yeah. Yeah. Yeah. Uh huh. Yeah. Yeah. Yeah. Team 16 and Team 17, you need to enter in if you use a prediction. 16 and 17. Team 17, you need to enter in if you use a prediction. Team 17, you need to enter in if you use a prediction. Team 17, you need to enter in if you use a prediction. Team 17, you need to enter in if you use a prediction. Yeah, we have a lot. We have a lot of different ideas. We have a lot of different ideas. We have a lot of different ideas. We have a lot of different ideas. I can't undo it. We did try to snap it on. Oh, no? Yeah. It's hard to understand. OK. Do you want to load all my comments? No. No, it didn't work. It didn't work. Yeah, it didn't work. Go back and look at your comments. Yeah. Oh, sorry. I'm sorry. That's why it's so hard. I'm sorry. I'm sorry. It's so hard. I'm sorry. So it's difficult to say it. So there's two teams in the 15s that seem like they're kind of leading the pack. And right now, there's about a three-times difference between the 15s and the 16s. But it's just actually after the game. It's just after the game. So I think that's where we're going to be starting out. Let's work on that. I do. I think so. Think about it. Yeah. I don't know. Oh, yeah, you are. Yeah. You have to let me know. Yeah. Yeah. Oh, yeah. It's really fine. Yeah. Yeah. I mean, it's really nice. Yeah. Yeah, I do. I do. Yeah. I do. Yeah. Yeah. Yeah. So… Hi again. Hi. I think there's a lot of people that want to talk about this. There's a group of people that are trying to do it already. I'll let them know. I don't know if we're running out of time. Maybe we should do a Q and A. Yes, we have to get going. I haven't been here in a while. Just a quick observation. The top five teams, which are above 13 words per minute, two have autocompletion, and three do not. Less than half of these have no prediction or autocorrect or anything of that nature. What the two top teams do right now is about a two per minute lead. . . . . . . . . . . . . . . .   So then I like, spiral, and then I can... You want to really do it with the... You want that kind of confidence. Right. That's what I think. I like working with people. I'm not a change-maker. I'm not a change-maker. I'm not a change-maker. I used to make my own thumbs up, but I don't like thumbs up. I don't like thumbs up. I don't like thumbs up. I don't like thumbs up. ... So we're probably going to convert in five minutes to the lightning round. So run your one or maybe two participants trying to grasp as many as you can to go through the knowledge you prepared. ... ... ... ... ... ... ... ... ... ... ... ... ... The team that just moved into the lead has no auto-correction or auto-complete. My prediction has come true that it's not going to save you necessarily. But that's because you have one crazy word for men, excuse my language. Make sure you save time to run your own participants since they are likely to be fastest. That's question six. You're right about that. Thank you. Oh So Two more minutes to wrap up your last question. There are participants that are free. Definitely make sure to run your own person, only like half the teams have run their own participants. Definitely run your own participants. Team 1, Team 3, Team 5. Run your own participants. Team 2, Team 3, Team 5. Run your own participants. I'm sorry, Team 4, Team 5. Run your own participants. Team 5. Run your own participants. Team 6. Team 7. Run your own participants. Team 8. Run your own participants. Team 9. Run your own participants. One more minute, 30 seconds, finish your last person, I'm going to pick people that are moving on. Okay, I picked six teams that are all above 13 words per minute. So, one at a time, rather than this gigantic mosh pit here, actually, sorry, yes, one of your own participants again, fill in those X's with your own time, and we'll see who All right, Hunter, I went ahead and moved us off, one more time. Was it a lucky? What? Did we have it? Yeah. Did you get it? I did. I don't think I did. Did you get it? No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. No. We still need to find the team 17, and team 18, and team 22. I don't think I can type through this. Team 18 and team 22, what do you think I'll do? I don't think I can type through this. I don't think I can type through this. Okay, three teams, so I'm gonna call you up one at a time. Everyone's gonna sit where they are. We'll try to do this a little bit more efficiently. So we'll start with team 21. Come on up and plug into the projector and show us your idea. I have a dongle for you. Can you demo it on your laptop? Yeah, I guess that's not gonna help. Okay, I'm sorry. Okay, everyone crowd around there. Come into the moss pit, come into this little freedom. And we'll try to crowd over this five and a half inch screen. So all three teams, all teams come to the middle. Everyone come to the middle. Come, come, come, come. We can talk less if you have the energy to detach yourself from the chair and stand for 10 minutes. While they're setting up, every team's got their phone ready to go. While they're setting up, what should we do for the talkout? High five. Okay, we'll do eight, we'll do eight. Eight. You guys can do three, three and a half times. Yeah. 25 can be here. 25. Okay, anyone ready to show their ideas? Oh, yeah. Okay. Okay, who's the other team? Who's the other team that made the talkout? Oh, yeah. Okay. Okay, guys, hold on for a second. If you can see this green, you'll have to explain it to us to make it to match. I'll just show you what we did. What we did was we composed ourselves having the labels on the CSV itself. We put the labels a little bit apart so people can actually see what they're typing. And then what we have is every time you start a new word, every time you start a new word, there are three defaults. New, the, and is. And then to enter a word, you click on it. And then every time you click on two letters, it'll suggest a word that you can type. Is it scrollable too or not? So this gives you three possible solutions from the first two characters that you type. And as you keep typing, it's building more and more. So it's just a QWERTY keyword. ... Halloween? What's Halloween? Halloween? Yeah, okay. Okay, cool. ... And it's just a QWERTY keyword, no prediction. No prediction. Just a string of QWERTY keywords. I mean, it's fashion, so... ... Okay, very simple. But it seems effective. Last team? Yeah, turn it around like this. Um, there's a button that toggles the size of the keyboard that you're on. And then the position is based on the word and also pairs of words, so it'll predict what your next word is. So it's also according to a very clear pattern here. It's split in two as opposed to all in one. There's a toggle button and then the basically top three auto-corrects, which gets you, you know, whatever 75% of the time. And you guys had no auto-correct, right? Two of them made an auto-correct and one. Okay. Okay, so deploy your code with a number of pieces like this. And then Cynthia, grab a seat here, and then we'll... It'll be the final showdown to see who's the fastest. Here we go. Okay. Everyone ready to go? Number? No two pieces, I think. Ready to go? Okay, go for it. Everyone else is missing out, but it's the most typing you're ever going to see on buttons this small in your phone. Load is up. Keep in mind that the phrases are different lengths, so the time they finish does not correlate with their word count. So, keep in mind that the phrases are different lengths, so the time they finish does not correlate with their word count. Okay, we'll go in reverse order of finishing. 22.11, 22.14, 21.235. Okay, we're not done with class yet, because I want to tell you about Bake Off 4. I want you to sit back down. Okay, so number one is you're going to give me back my phones. So give me back my phones, unless you really need to keep them for some reason. And I'll tell you about Bake Off 4, which is titled Sensor Madness. So here are the new teams. Don't worry about those new teams right now. If you're really dying to meet your new teammates, you can't because they're not in the class. But you're going to have all the way until the finals on May 13th to get this done. So it's going to be in this room. Just to clarify, I think the final schedule is wrong in this regard, because they put us into some really random room. So we're going to just do it here, 1 to 4, probably more like 1 to 3, because we don't want to do Bake Off 4 for three hours. So don't worry about that. So here is the challenge. And I'll show you a video of the scaffold code. We are going to be using mobile phones again. Okay? This is basically ‑‑ you know, every Bake Off is basically couched as an example in a kind of box. graphical user interfaces. It's not going to be typing, it's not going to be like rotation in PowerPoint, it's not going to be clicking on buttons. This one, I don't know why that's on. This one is going to be a hierarchical selection. So the idea is that you have four possible things that you want to access. This could be like, you're on your Apple TV and you want to switch to Hulu, Netflix, HBO Go, whatever, okay? And then inside of each of those four things, you're going to choose one of two things, like whatever. Game of Thrones, Episode I, or something like that, right? So it's going to be choose four, and then once you've chosen those four, you have to pick one of two possible options, okay? So if you look at the generation code, it generates a stage one target and a stage two target, and they're sort of net. So it could be pick item number two, action B. Pick item number four, action A, okay? That's all there really is to it. Now the trick is, you can't use the touchscreen or any buttons. You're going to have to do this entirely using the sensors that are on your phones, okay? It would include things like the light sensor, GPS. It would be kind of crazy to just like one of the more things I run into, like Martin Warren says, A, B for B, but if that's what you want to do, I mean, this is the lowest time in the history of this class in a whole hour of students. You can use a camera, you can use the accelerometer, the gyroscope, the compass sensor, the magnetometer. That's fine. Anything you want to do, but you can't touch the screen, other than to get it started. So here is a video of the scaffold coach. Let me explain. What happens is I'm using the gyroscope here to rotate that little tiny square onto the green target, and it's a little hard to see, but when I cover the proximity sensor, it says up or down, and that thing says down, and I'm going to flip the phone down. In this case, there's four circles, that's why I choose four, and then once I'm in one circle, I can go either up or down. I'm using a combination of gyroscope, accelerometer, and the light sensor to pull this off. I will tell you, this is a very horrendous piece of scaffolding code, and you never want to follow my design. I have it spinning around like this, but actually you do, it just goes around in a circle. That's it. You guys are free to go. If you want to meet with your team, here is your teams again. But really, I don't necessarily need to get started right this instant, but it would be good to start thinking about it over the weekend and play hide and seek. Come on over. Yeah. Hi. Hi. Hi. Hi. Hi. Hi. Hi. I can barely hear you. How much coffee do you have? Yeah, of course, yeah. Do you want me to go ask him? I can't hear you. I can't hear you. Okay. I don't know. I don't know if you're okay. Yeah. I don't know. I don't know if you can hear me. I can't hear you. I don't know. I don't know. I don't know if you're okay. I can't hear you. Yeah. I can't hear you. Yeah. I don't know. I don't know. Yeah.\",\n",
       " \"I'm going to jail. Oh, yeah. I'm going to jail. Yeah. I'm going to jail. I'm going to jail. This is my advisor. My advisor had to get in my way. So she went to the office, and she was like, really off. Are you kidding me? Yeah. Arlino National Museum. Yeah. I'm going to jail. I'm going to jail. I'm going to jail. I'm going to jail. I put together, like, a dozen advisors, Okay, just some quick bake-off clarifications, because I've been getting a lot of the teams sort of ramped up and started asking me interesting questions, and there's some things that I've heard three or four or five times now. So number one is, I'm trying to make this more realistic and more stringent than bake-off one, and one of the ways that we're going to do that is that you have to decorate all of the destination squares the same way. So let's say you wanted to put a little dot in the middle of the square or something like that. That has to be true for all of them, okay? The only thing that I'm going to allow you to uniquely decorate, like feedback, is going to be your kind of cursor or logo square, okay? So imagine that other than the red, the darker red, which is used for the destination that you're navigating to, that trial, everything else should be the same for all of the squares. Does that make sense? So if you want to augment them somehow, you have to augment all of them in an equal way. So if we make some changes to all of them... things that change when it's transferred because of the opacity. If you yellow it, it transfers. If you change it, then it's white. They should all be the same, but the only opacity should be the fill color. And then you should just leave it alone, so everyone should have the same fill color. So yeah, you can put your... for your logo square, I will let you decorate that in interesting ways that you should run by me to make sure that they're legal. Okay? This also means that in Bake Off 1, I let a lot of teams do like the line that went to the target. That's no longer legal because that clearly is sort of decorating the destination square in a special way. So again, in this one, more stringent that all destinations have to be treated equally, including visually, which is not true in Bake Off 1. The other thing that I sent out a clarification on, that I updated, is that you have to pick up your logo every trial. So imagine like, again, this is Keynote, or Photoshop, or Illustrator. You might have many logos on the screen, so you can't have, like, it'd be really weird if you went into Photoshop and your logo was always glued to your mouse, and you could never ever have two logos, because they're like, both are just glued onto your mouse. So imagine that. It's like in Keynote, you're going to like grab this graphical icon, you want to go and somehow select it, a click and a drag, or a click and hold, whatever it may be, and you're going to move it where you want it to go. That's what the destination is trying to be. So every trial should start off with you grabbing the graphical element and moving it to where you want it to go. So you have to somehow have a mechanism for, like, picking up the logo square. So think of it as a logo, and not a cursor. Your cursor can be something different, it can be your mouse cursor. You want to pick up the logo and drop it into the destination. So use that metaphor, it should make more sense. I also got some clarifications about, like, when they can hit, like, an enter button. So, like, you know, like, when all the parameters are correct, like an enter button will pop up on the screen. This is not illegal, because you should be always able to proceed, right? This should always be basically an enter button on the screen. on the screen, because again, the destination red square is only there so we can run a trial. And so if I told you, I wanted to always put it into the top left-hand corner. Forget the destination squares. That should be just as accessible, because every pixel on the screen should be basically equally critical. So if you only make it so the button to drop the logo is visible only on that one square, then clearly that's making that one thing much easier than the other. So you should be able to always proceed to the next trial in an errorful way so that your confirmed mechanism doesn't have to be a button. You can make a lasso gesture on the keyboard or something like that. So not on the keyboard, but on the trackpad. It doesn't have to be a button. But it should always be present so they can always go forward. I see people just go, click, click, click, click, click, click, all the way through and get all 10, or 20, or whatever trials are going to send for the big dog wrong. OK? So yes, getting more complicated. Is there any general questions that I can answer for people that isn't going to reveal too much in terms of an idea? What is the, like, on the box, what is it going to create for the current version of the code? This is actually new this year. I modified it. It's sort of based on a bake-off that I ran previously. But you are the first people to test this exact bake-off. And so I don't have any time for an RPU. I would guess, if you're very clever, and you'll see it, we're going to start pulling away. Good designs and bad designs are going to get way worse and way good. So I think that if I last a bake-off of, like, 2 and 1⁄2x, I think we're going to be more like 5x on this one, between the best and worst designs. I would say you probably should be in, like, the 1 to 2 second range for trial. Like, if I'm in Photoshop, I can definitely do it in that speed. And I want you guys to be even better than the 1,000 Adobe engineers that are working on the next version of Photoshop. You guys are going to easily be able to come up with something better than Adobe, even though they've been spending 20 years on it. OK? Any other questions? People are looking worried. There's no pop quiz today, so put away those laptops. You're safe, you're safe. Any other final questions? So how many people think they have some good ideas for their bake-off? They've already started brainstorming, they have some reasonable ideas. But I need to shoot down. Like half a person? Okay, you better start ideating. Again, this is more, this is gonna take you more time to program than bake-off one. So make sure you leave at least one a week to get a good prototype working. So by the end of, by like Monday evening, you should definitely have emailed me an initial set of ideas, otherwise you're just gonna run it like runway. It's a two-week homework assignment in groups, so I'm expecting a three-person contribution that represents two weeks of work. It is a significant fraction of your grade. It's like something like a half a letter grade of your entire grade, so you don't wanna botch it. Okay, so today we're picking up on where we left off on human factors. So we're gonna talk about some other human factors today and some basic sort of cognitive principles. And we'll start with something that we can all kind of potentially reflect on. So this is another one of these laws that's sort of like Fitts' Law that people have been using to characterize things. So if you look at a set of choices, and this is one example that I put in here, you know, these really dense kind of lunch menus versus a very simple lunch menu. Can anyone guess, eat over a large set of people, how long, do time trial, how long it took them to select an item on this menu, which menu would take longer? So how many think that this menu would take longer to make a selection from? How many people think that this menu would be harder to make a selection from? Okay, so you are correct. This would take you more time on average than this. You know, with all other things being equal, like obviously if it's like a different cuisine and stuff, it's not gonna work, okay? So what I want you to think about, and this is what I want you to work in your groups, is if you have the number of choices. of some abstract thing, and the time it's gonna take you, what does this relationship look like? So you can just draw a line. So work in groups for just a few minutes and see if you can guess what that relationship is. As the number of choices grows, this time goes down, this time goes up, and what does that line look like? So take out a sheet of paper, someone in the group take out a sheet of paper. You're gonna turn it in at the end of class and put your Android design. Give it a chance. Write up four groups of roughly four, fourish, please. You can write down multiple chances. There's not going to be one chance. Okay, let's hear some people's answers here. I saw a bunch of really interesting ones. Who wants to volunteer an example? Yeah, sir. Root of L and then N. What does it look like? N is the number of, like it's like, it increases but like it's back to something. Like this? Yeah. Okay. What else? That's sort of similar, right, like, I got it now. Okay. What else? Those crazier ones out there in the back. Okay. What else? Yeah. Just like this? Okay. In the back. Oh, it's long? Okay. Interesting. I'll take one more. Yeah. Oh, that's not that. Isn't it that one? What's that? Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. So the answer is here, it looks like this, which is that it's log, and again, you guys seem so disappointed, but log. And the reason why it is roughly log, and again, the thing with this is that it doesn't actually fit too many cases, but the reason why it's log is, you can think about on the menu. So when you do a menu with 10 items, and a menu with 100 items, it takes you five minutes to select a menu on the one with 10 items, it isn't going to take you 50 minutes to select something with 100. So that's definitely not linear. And the more, if I gave you a menu with 1,000 items on, you're not going to be there for five days. Right? So we know that if you just follow the choices out, as the number explodes, like, you know, what college do you want to go to in the United States, there's like 10,000 colleges, it didn't take you like five years to figure it out. You were able to pack down a lot of things quickly, and for that reason, it basically follows a log relationship. So here's actually the formula, and you'll see that it has these coefficients just like Fitts' law, because it turns out making different decisions takes a different amount of time. Or like, choosing a college is very different, like, what do you want to... order entropy. So you're going to put a different coefficient in front of it because it has different magnitude, different cognitive overhead, and so on. So it's more like double when you go from 10 to 100 menu items. And you see this sort of choice. Well, so one other thing I should mention is you don't see this law, which is called Hick's law, or Hick-Hyman, it was kind of co-discovered, very often. It tends not to be applicable. Because Hick's law is very mechanical. Like, it's very simple. You're not really thinking about Hick's law when you do it. And so it's mostly following sort of human motor performance and really physics. Like, you can only decelerate and accelerate your arm in a particular way. It turns out that Hick-Hyman law is very rarely applied. Because most of the interesting decisions we care about are complicated, right? Like, it isn't going to follow this beautiful relationship if you have, like, eight colleges that choose from versus two colleges versus 50 colleges, whatever. So for this reason, it's very rarely applied. And you won't see it nearly as commonly as Hick's law. But you do see some of that sort of rough estimate kind of thing. It is a nice proxy thing, but how you divide up choice. And if you know the kind of rough relationship of choice, when things are, like, roughly equal, it does give you some insight into how to distribute things. And so when you see, for example, your Craigslist does not really hold it very well, it's just sort of a big blob of options that you have to sort of scan and decide what category is the right one. If you look at well-designed, for example, menu bars, they tend to do two different strategies. One is that it's hierarchical, right? So rather than exploding this all out into 50 options here, is they nest things so that at any given time, you have, again, a 10-item tier and a 5-item tier. And every one of these have, like, five items on the menu. There'd be 50 items instead of 10 plus 5. And also, they're doing these little grouping things, too. So actually, the high-level cognitive leads are going to look like groups. It actually reduces the kind of functional number of choices. Now, this works in pun-intuitive ways, though, right? If we have a menu that's 50 items long, we could do it as one gigantic menu. So you plug this in without a coefficient, this would be log base 2 of 50 plus 1, following the formula here. And that would estimate that it would take roughly 5.7 seconds to search for your item in that list of 50. And so you might say, oh, well, that's making it more efficient by putting 5 items into 10 submenus. So it's 10 items, and each one has a little nest of 5. And so in this case, it's basically a choose 1 of 10, and then inside of there, choose 1 of 5. So there are two decisions, but they're smaller decisions. But if you actually just sum these up when you should be able to, you'll find out that it actually is predicted to take longer. So this strategy of basically having a hierarchical menu scheme, it only has sort of a payoff if you get the ratios just right, which is sort of counterintuitive, because I think we'd all agree that a menu bar with 50 items in it is not very real. Like, even the worst kind of menu bar in something like Adobe Illustrator probably only has 20 or 30 items, and a menu bar with 50 items in it is unreal. So this is where it starts to fall apart. It is not quite so simple human decision-making as it is choice-making. So it's very counterintuitive. And of course, it's not accounting for all these things that you said. This is assuming the model only works with all things being equal. Like, choose a color. You can't even choose a color. Even choosing a number between 1 and 10, if I ask you on your pop quiz to choose a number between 1 and 10, it's not going to be like 10% of the class with each number. There's going to be all these weird human kind of cognitive principles and biases that are going to affect that. And for this reason, it's sort of falsified. So Oprah simplifies very unfortunately, but again, we know that it is still roughly a log scale kind of thing, and so it's not widely applicable for today's ACFL standards. Okay, here's another one, which doesn't have a great name, but it was told to me by Sue Carr, who is a CMU alum, and helped write this seminal book on human-human interaction back in the 80s. And he has this little game that you can play. We're going to do the exact same thing that we've done previously, which is I want you just to reflect on your own behavior. As I make you do this task, I want you to listen to everyone else in the class and think about how you're doing it yourself. You are the subject of the class. I'm going to put circles on the screen, like they're blue dots, and I want you to shout out, or not scream it out, but say out loud the number of dots that are on the screen. I want you to record, essentially mentally, just keep track of any trends in your seat. Here we go. Just say how many dots there are on the screen. Okay. So you have some data. This is the next one you have to predict, okay? Number of objects versus recognition. Keep it on the same sheet of paper, as I want you to plot what that curve looks like. Okay. Okay. Okay, let's hear some guesses back. Okay. Someone give me this relationship. Okay. Okay. You guys have data on this one, so yes. I felt like it was pretty low, so like six. Okay. So if you'd like five. Linear. Linear. That's okay. Interesting. Okay, what else? I said, yeah? Yeah, we had the same shape of it flattened out. So it's sort of like a flat and then like, hmm. Yeah. That kind of thing. Okay. What else? I saw the answers. Yes? Yeah, like the same thing on the first one. That's what we were curious about. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. So those are my answers. Yeah, that's a pretty good one. So we were curious. Curve. This way or curve? Curve. This way? Yeah, I told you. Like an exponential square. Okay. Anything else? Yeah? Um, this is like really smooth even, because it's like on really sharp bottom at around six. Okay. And then like flat so. You guys are pretty close on this one, so that's good. So it actually looks something like this, and the key thing is that it's mostly linear up until around 5. Around 5, something special happens, and you become dramatically slower. And this is because humans can see up to about 5 objects, you don't have to go 1,2,3,4,5, or if there's like 2, you don't go 1,2, you just see there's 2 immediately, you know there's 2, you don't have to count, you just know there's 2. And you can do this up to about 5. After 5 things, you basically have to start counting, so 6, 6 a little bit longer, 7, 8, 9, obviously there's like 52 items, you're going to have to go up to 52, there's no way to like see, oh I can see there's 52. So humans can do this up to about 52, and they perceive them as one chunk, and it basically is one clock cycle of your mind, you go up to 5. Some people it's more like 4, some people it's up to 5. And so it's mostly flat, but the key thing here is that there's an inflection point in the graph where the relationship changes, and that's 5. So should that come up on a pop quiz in the future, the key is, there's a change in the behavior around 5, okay? Here's a little Vsauce video on this which has some interesting details. So let this play. You can tell if there are 0, 1, 2, 3, or 4 objects in a photo without even needing to count. How are you doing that? Is it some sort of sixth sense? No. Psychologists call it subitizing. We can intuitively, at a glance, determine whether there are about 4 or fewer objects in a photo. This has been part of human culture for a very long time, and it may be the reason so many tally systems from all over the world, all through history, wind up having to do something different when counting the number 5. When estimating or comparing amounts above 4, the brain uses what's called a subitizer. what's known as an approximate number system. It's a psychological ability we have that's about 15% accurate. If two amounts are at least 15% different, we can tell. So for example, 100 objects and 115, or 1,000 and 1,150 or 1,200. If you want to test the accuracy of your approximate number system, PanaMath has a pretty good test. We often take linear active counting for granted. But it's not granted to us. We aren't born with it. We are, however, born with the ability to subitize and use an approximate number system. Children younger than the age of three can tell without counting that this line of four coins contains fewer coins than this line of six, even if you spread the four coins out into a line that is physically bigger, longer than this line of six. However, mysteriously, around the age of three and a half, children lose this ability and begin saying that this line of six coins contains fewer coins than this long line of just four coins. Possibly because around this age, the physical world of objects of physical sizes is more salient in their minds. But then, when they begin to learn linear counting, they reverse back and begin again correctly saying that this line of six contains more coins than this line of four around the age of four. The smallest is a pretty interesting development in psychology, but yes, we have this sort of baked in, but it's not a totally linear thing. So this is, you know, it kind of goes by different words, but I have it as the rule of four and a half because it's around like four or five is when you have this. But now what we're going to do is we're going to play this game again, but I'm going to do some tricks on this other line, okay? So we'll again just yell out how many dots are on the screen and notice the behavior of the class. So, what you probably should have noticed, if I made that video a little bit too easy, is that once you group items into, basically below that threshold, you basically do a summation that doesn't require you to count. You can sort of, like, I see the 333, and I know that's 9. And so you tend to be, when you do this sort of chunking behavior, you tend to be much more accurate at the estimation. I would ask you to, like, screen out the number that's within plus-minus 10% as fast as possible at your grade. You'd be able to do it at a pretty high, much higher accuracy if it's grouped in some way that's below that 4.5 rule. And you do see this sort of design principle applied in many different places. So if you look at, like, tables of contents of books, they tend to, you know, they tend to have groupings that are around 4 or 5, if it's well-designed. If there's, like, 50 items under here, the suggestion that you'll get from, like, your PhD thesis advisor is, like, well, then let's put subheadings that are about, like, size of 4. It's obviously just, there's no, sort of like a finity diagram. You shouldn't have a cluster with 50 things. And if there's 50 things in there, you should do a better job and separate it into these smaller clusters that are more digestible. And you're seeing it in physical design of things, too. So no, it's not by accident that the function keys have these separations, right? Or at least on traditional keyboard, they have these separations. It's that if I said you have to jump to F9, you know how to jump. You don't have to, like, scan over from F1 until you find F9. But look around this rough region. You can generally know that it's in that grouping with F9 to F12. You see it in menu bar design, as I already sort of pointed out. So you tend to see it in well-designed interfaces. As I said earlier, there's a little, sort of like grid separations or padding, again, playing on that Gestalt grouping principle of proximity. And you tend to have sort of two packings of around 2 to 4 to maybe 5. So if you start making it like 7, 8, 9, 10, 20, it starts to break down. Here's actually the slide. So if you open up the keynote, again, you'll see the sort of groupings that tend to have, like, this one is 6, which is really good. pushing it. But again, it wouldn't be an effective grouping if it was like 7, 8, 9, 10. Here's Mac West. It's hard to see that again, because he doesn't even have a foot in the little 10 grouping, but around 5 again, 4. Okay, switching gears a little bit. We brushed on this topic previously. There's no mental model. I've used this word before. Can anyone venture to guess on what a mental model is? We'll start with how many of you have heard this term before? I'm pretty sure I said it in this class, so you probably heard it even if you don't remember it. So anyone want to give a stat on what a mental model is? A mental model is like you're in a class of five or ten, and I'm going to give you a picture and give you a mental model of what a dog is. It's like a furry creature with four legs, but all I am saying is that it's the same category as a dog. Yep, that's a good example. Anyone else want to add to that definition? Okay, very well. So it's really at a high level. It's basically how someone thinks the real world works. So you might have a mental model that all furry creatures are mechanical inside. It would be a wrong model, but it is how you rationalize the world. Or there's a soul in your dog that eats it alive. That could be a totally valid mental model as well. So it's really the representation of the world around you, and the relationship between all the various parts, and people's perceptions as well, and also how their acts have consequences. If you think your mental model is that this is all just a simulation, and all this is just holography, that's going to maybe impact how you treat other people and act on the world. what people's mental models are, what they believe is how the world works, how objects work, then you can basically help to shape their model and build good mental models, especially when it's in relation to like an interface process. People have mental models about how the internet works, most dangerously, it's like it's a series of tubes. That's a mental model, not a particularly accurate one, but it was the model that that person was using when it got to the center of Alaska, right? And so you want to foster good mental models and a good application, how Uber, like Uber's doing a lot of magic tricks behind the scenes, like pair you up with drivers and have this routing. And your mental model is probably not exactly how Uber actually schedules drivers, but it's good enough for you to use the application that only reveals certain things that sort of facilitates your mental model. Like you see the little car moving around the map and your model is probably like the closest car to me is the one that's gonna pick me up, but they might be lying about which cars, those could actually be totally fictitious, actually those cars, I don't actually know if they're even necessarily real all the time. So people build mental models of how things work. That's just how we are, we always like to come up with it, and this is why humans throughout the ages have come up with mythological reasons about why the moon rises and the moon fades and tides and the sun, we always like to explain things and build mental models about things. And sometimes they can be naive and sometimes they can be quite detailed and functional. Like the computer scientists in the room, you probably have a pretty good mental model of how a processor in your computer works. So it's not necessarily logical, like my mental map of Pittsburgh, I still discover crazy stuff I don't fully understand. Now, there's this great little clip that I'll include, but I think the reference is gonna be lost on me. Does anyone know who this is? Who is this person? Can anyone name who this is? No one can name who this is, I do not believe it. Scotty, Scotty from the original Star Trek, like back in the 70s, okay? And Scotty, not the new movie, Scotty is, well he is as well, he's the chief engineer, right, of the Star Trek Enterprise. And gosh. impossible to have any Star Trek jokes in this class. I grew up on, like, Star Trek. OK, so Scotty is, like, this brilliant, like, engineer who watched, like, any of the movies or whatever of the show. He's, like, always, like, you know, fixing things, like, with, you know, glue and stuff, right? And so there's this great clip from this movie. I think it's Star Trek 4 or 5, maybe, where they have, like, Rescue the Whales is the worst of the movies. But anyway, you'll see Scotty, who's, like, the 22nd century, interacting with a 20th century computer. He has a mental model of how computers work in his era. And you can see, in this nice example, someone who's, like, a genius totally failed using computers from, like, our time period. So here's this little cute clip. Now, suppose, just suppose, I were to show you a way to manufacture a wall that would do the same job, but be only one inch thick. And that'd be less sudden to you, eh? It's OK. Perhaps the professor could use your computer. Please. Computer? Computer? He thinks it's what's activating it. Ah, we're getting the mouse. Hello, computer. Just use the keyboard. We get a broken mental model where there are complete interactions. Oh, great. And then this is where it gets a little bit ridiculous, because he's, like, never used this operating system before. This is, like, me in my office. This is, like, what I'm doing. Transparent. aluminum? So anyway, it's a cute example of even if you're super smart, basically you can have broken mental models about things. And you should all watch Star Trek. If you're really nerdy like me, you have to watch Star Trek. It's like Manifold. It's like the Bible, okay? Okay, so you know, people have mental models of all sorts of things, like reverse Polish notation on math, you know, this kind of weird notion about like dragging files to the trash on a Mac OS with a really bizarre mental model that doesn't really hold up in reality. And so, you know, good interfaces communicate the model, right? And we'll talk about affordances later, like how to use a computer for the first time, if it has good design and good affordances. You should be able to sort of pick up and grow your mental model, understand like how a database software works, how an accounting software works, so you can understand what that model is, okay? So, of course, you as a system designer, you have way too good of a mental model. If you've built this system, you're going to have this really detailed mental model. You have to realize it's sort of a form of expert blindness, that users are going to come to me with a lacking mental model. Like, do people really understand like how Uber is actually doing things? Is it the same drive every time? They're going to have, they're going to be building it from scratch. You have to just realize that. So, back to that mantra, the user is not like me, you are an expert, they are not the expert, make sure that you do this kind of approach. And obviously, try to think about what that mental model is, how you build that interface and what data you show them is going to impact how they build that mental model over time. And so, you want to foster the right mental model and not something that's sort of broken, right? Okay, this is another topic that we talked about in last lecture briefly, the difference between recognition and recall. So, can someone tell me what the difference between those two words are? It's not special words, this is just the English words, recognition and recall. Yeah? Recognition is probably figuring out what's in front of you and understanding what's in front of you, whereas recall is understanding how to be something in front of you. Okay, sort of. No one else want to take a stab? Someone new, perhaps? Yes? Right, so in recognition, you're noticing the thing, right? It's right in front of you, just recognizing that it is the thing that you want, because it's giving you some clue, right? Like the answer in a multiple choice question is a great example. You may not have remembered the answer, but you're like, oh, this one I know. It's like I pulled it out of memory. Versus recall is when you have to pull it out of memory with no cue. So it's more like a fill-in-the-blank question on a test. You're like, oh my gosh, what was the name of that author who wrote that book in high school? You don't really have much to go on as you have to recall it from memory as opposed to recognize that it is the correct author. And that's the big difference. So just to formalize this, remembering something in your mind from memory without the cue is recall. Like what were you wearing? Like what color was your shirt on the Monday of spring break? Most of you probably don't remember, but if you remember like you went to your friend's birthday party, then you're like, oh yeah, now I remember. Because again, you're using other clues to basically steer your memory. And so recognition is when you're using some sort of a sensory cue, like you're looking at a little printer icon, and you recognize that it's a printer, that's recognition. Or that you remember something in the front. Oh, that's a really nice blue shirt. Oh yeah, a blue shirt. That's going to be a sensory clue to help you remember something. Right? So here's an example. Here's some ones that are about recall. So what's the primary color on the Pennsylvania state flag? What color is it? The majority color of the flag. Blue. Blue flag. What was Mr. McMillan's first name? Carnegie. I love that. Not Carnegie Mellon. Two different people. That were both called Andrew. Oh, that's a bad one. You're just learning that. The Year of American Independence. So, if you present in a different way, you might be able to actually figure this out. So, here. So, if I go back to recall, even if you give them the answer, you might have remembered it. 1492. 1812. 1066. Battle of Hastings. Okay. So, recognition is good. So, you always want to create, it's much easier to do recognition than it is to do recall. So, this is why you want to have these little icons that you don't have to recall what these are. You can look at it and guess what it is or recognize what it is. It facilitates recognition. I could have made this print icon but someone could have made a print icon that's like a red square. And you could remember that red square equals print. But it's so much harder than just having good recognition. So, you always want to have good recognition. You know, commands that are like about inserting something and when you walk through the insert menu, you can just recognize, oh, that's just insert. The thing I want to do is insert something, so I'm going to recognize that that is the thing I'm going to look for. So, with good labels and good text and good icons, you can pretty much make GUIs recognition-centric. And this is why the jump from command line to GUI was so monumental. In a command line interface, you have to recall everything, right? There's no affordance of what I should type to launch like a text editor, or open a browser, or print a file, or delete a file. You have to remember and recall from memory every single command. So if you get rusty at all your Unix commands, like Linux commands, it's really hard to use a command on the interface. Or if it's like Vim, Emacs, and it's like all these esoteric, crazy key commands, it's basically impossible to use. There's nothing to help you recommend how to do it. And so even if I just wheeled in a machine from the 70s, some like mainframe from the 70s, and we were tasked, like everyone's gonna die in this room, it's like an escape room, and we can't leave until we print out a file. It would take all of our collective intelligence, we'd probably get nowhere. We would just be typing in commands randomly, because there's no recognition at all. And so that's why GUI was so much easier, is you could just look around the interface and recognize the thing you wanted to do, as opposed to have to learn everything from scratch. So recognition does fail though, like these are the ones that I pointed out before, a couple lectures ago, is that even when you try your best, even though I've told you what these are, you probably would get at least one or two of these wrong. Right, you don't remember what this one is? How many people think they remember what this one was? Okay, so that's not a good sign, because that's supposed to be good iconography, and yet I've even told you the answer, and you've already forgotten it. So recognition does not always work, it's difficult. You're basically having to recall, because it's not recognizable, so you're relying on recall to figure it out. So how about this one? Another kind of example that's sort of related, so here is sort of a typical, or at least a slightly older version of Windows, they're both of the same dialog. Anyone comment on what they think they like about the design, or what's bad about the design? Yeah? I think I like the other one, because the stored save option is kept at a distance, and the save and the cancel option is grouped together. So if you basically, by the city, click no over there, and you're afraid to save your document in the first one, you will not make the same mistake again. I'm not sure I totally understand it, so what's better about this one? So like, you will not click don't save and lose the data. I see, because it's further away. I see, where? This is... Okay, yeah. So this is what you want to hit. And that's like the most destructive possible thing. And they've actually placed it next to this. Yes, that is absolutely, I would say, a design fail. In this particular instance. Not only has this grouped it far away from save, but they've actually added extra padding, so it's even more distinctive if you don't hit don't save. So that is one thing that I think is nice on this macros example, yeah? I like how it says save and don't save. It's like making sure it's easier to press instead of yes or no. And that's good, but is there anything else you can add to that? Why do you think that's a better design? I feel like a blue save, your eyes can't see. Sure, but what about the text? This says yes, no, cancel, and this says save, cancel, don't save. You think that's better or worse for the equal design? I could also make that yellow, so the yes button kind of fills blue as well. You don't have to read the whole question and explanation to get to the buttons. Right, so this one is if you're really lazy or you're moving really fast, is you don't have to read. This is actually much shorter than the Mac OS one, but basically you can infer what's going to happen, the action, because the button is facilitating that. You know what the button's going to do without having to read the whole explanation. We're here, there's nothing I can really recognize without having to read this. You know what, I'm saying yes too. It's subtle, but it does all add up. I think it's a nice design choice to put the action in the button as opposed to having to read. Let's talk about the model human processor. In some classes, we used to actually spend money. There's some classes that spend half a semester on that for the model human processor. I've included it in this lecture because I want to give you some historical background because it's really where I got started. We need to rewind time a little bit to the beginning of ATI. This is the book I mentioned before. The Psychology of Human-Computer Interaction, which was written by three CMU people, is basically what formalized and popularized the term, so the actual term was really started here at CMU. It was published in 1983, and it really is what put it in as a field. I know there's some people in this class that got to meet at least one of these people, Moran, who was here at CMU just a few weeks ago. I guess a lot of people think the same. He was a grad student under Newell at the time. And what they wanted to do in this book, and that's why it's called The Psychology of Human-Computer Interaction, is they really wanted to find ways to summarize the human as an engineering model. So it was really popular at the time to basically try to boil down human factors into a collection of rules that could basically be simulated as a processor. So some of them just called the human processor model, but I've used the term model human processor. So the idea here is that you take all these human rules, like Fick's Law, and Hick-Hyndman Law, and the four-and-a-half rule, and so on, and you formulate them, and you try to build basically a simulator of a human, but treat it like a microprocessor. So given a set of inputs, like if you have an Intel processor and you say, I want you to add these 20 numbers, you can deterministically tell how long that processor will take to do that. And what they wanted to do is say, well, if we treat the human brain, or the human body, as sort of like a microprocessor, we can say, I want you to order a car on Uber, and I want to be able to estimate how long that's going to take. And you need really good rules to do this. And that's what they tried to do. So here is an example of a computer processor. So here's like storage up here, here's some registers, and you want to put them down here, and here's the ALU, and so this is like a whole processor flow diagram. I'm sure you've seen these if you've taken a computer architecture class. And here's what the model human processor looks like. And you can see it even sort of looks like a computer processor. And here you have the task environment, you have I.O. in a kind of a computer system. and then it goes into these auditory inputs, so you have visual input, you have auditory input, and then you have all these little processors and you add a little memory and it all sort of moves it around. So there's a vocal output processor, so you get like a sound card in your computer and so on. And again, if you can make this specified enough, the goal was to be able to basically compute the length of any app. So they broke this down into these three different sort of major categories. You have different systems, your perceptual system, your motor system, and your cognitive system. Motor is how you move like your body, perceptual is basically how you sense the environment, and cognitive is how you sort of crunch the numbers. This is like your brain power, your intellect, right? And each one of those has different parameters. There's a storage capacity, it's like how many bits do you store in your cognitive system? There's a decay time, because we're not like perfect, you know, things like our memory just fade over time, short term and long term. And then there's a cycle time, how long is like a clock cycle of our motor system? How long is a clock cycle of our perceptual system? And so they're all different, and they've been tested experimentally. And so like for our perceptual system, the human brain's kind of like, you know, one plus minus one standard deviation is about 50 to 200 milliseconds. But for me, it's about 100 milliseconds. That means in order for me to see something and respond to a stimulus, it's going to take at least one clock cycle of my perceptual system, and it's going to be around 100 milliseconds. So this is like if I throw something at you really quick or something, like you're just going to basically, you know, it's going to be very difficult to react in less than 100 milliseconds, one tenth of a second. A motor is a little bit faster, has this range, cognitive capacity to compute something is about 70 milliseconds. So these are all derived, these are not like examples, these are all derived from doing a big research of all these psychology studies. So, and you can test this, like, I didn't bring a ruler today, but you can imagine, you know, if I grab something here, let's say I can grab a wand, here we'll do this test on someone. You ready? Hold your hands like this. And when I let go, we just stop it. Stand up if you're ready. Three, two. So we can do that a thousand times, right, and I can record that distance, and if you basically know, you know, whatever, 9.8 meters per second, we can basically figure out what the clock cycle time is for that entire step. The operation is, visual system has to sense that the thing is dropping, so there's going to be a perceptual cycle interval of about 100 milliseconds, then you have to basically make a decision, I'm going to close my hand, I think it's another 70, and then basically you have to get your motor system to react and close your fingers, and if you sum that all together, it's going to fall within that range of the three different, basically like little circuits that have to fire to make it. And you can, again, do this a million times and get pretty robust averages for these simple things. And again, it's sort of like Hick-Hyndman, that it works really well for sort of mechanical things, but when it's like, you know, choosing a college or like where should I go eat, like it starts to fall apart because it's not mechanical in the same way that it's logic. But nonetheless, people have gone a great lengths to try to formalize it, so here's just a very small table of all the different, you know, kind of cycle times, and the range of cycle times, all the different things. So, you know, moving your eye is not just a one-size-fits-all idea. If I have to move my vision from here to here and refocus, that takes on average like 230 milliseconds. If you're talking about visual capacity, so how many kind of words or letters you can store in your kind of visual channel at a given moment, it's up to about 17 letters at a time, right? And so people have gone, like it was exhausting, they tried all these different things to basically quantify the human. So it's like human factors on steroids, right? It isn't just like Fitts' Law, it's like every little possible thing you could do. But you can imagine if you had a long enough list and you could quantify every little drop of human performance, you could make a pretty complete model of human processing, right? Unfortunately, I sort of gave away the answer to this. If you had a really good processor, it means that for an arbitrary interface task or an input task, you could automatically, like programmatically, output how long it would take that user to do that so you can basically feed in like your uber application and you know the uber application is like you click the button to open like you find the icon you click it so there's like you know it's going to take x amount of time plus y amount of time then it's like typing an address which is going to be like bum bum bum bum bum bum bum bum bum it's going to be x amount of time then hit the submit button and then like you know the car is basically on its way and if you had a really good model how long every little click would take and how long every little eye saccade movement would take then you could say the fastest you could do that is 12.9 seconds you can automatically calculate it if you had sort of this deterministic model human process so you want to calculate how long it takes to perform a certain task not just interface tasks but even you know like assembling a car kind of tasks right not creative tasks but important and it was like I said it's super cheap and easy it's sort of like the golden gateway to never have to run a user study again because basically you could just feed it through the design and it would simulate basically like the average human. Now there's lots of things that weren't accounted for they never found a way to really actively do it so things like tactics right like your sense of touch it isn't just like obviously like moving like a I don't know like a block of ice versus like a big like cardboard box like there's different coefficients that hang on all these things as weight there's texture and so on really hard to model one-size-fits-all kind of rule. Kinesthetic memory versus motor memory this is not really a real thing like a baseball pitcher really knows how to like you know you got so much training on how to you know throw a ball like 100 miles an hour and obviously like how do you fit a curve to that we all have different skill levels. Some of us are great at playing basketball and probably most of us aren't very good at it. There's also this notion of selective attention like there's a lot of letters like I'm looking at all your t-shirts there's letters everywhere in this room but I can't remember all of them equally like if I look at someone's teacher that t-shirt like twitch right there I can remember that in my memory but I'm not going to remember everything equally so there's notion of selective attention where you focus on certain tasks. they basically emitted entirely. And then sort of like multi-threading, the attentional capacity, like being able to multitask on different things, just isn't accounted for at all. So basically, long story short, and I'm not going into too much detail, is that there was this really awesome vision in the 80s, like if only we could just have the whole list of human factors, we could build this complete model. But in reality, it turned out to be much, much harder, because humans are complicated. So probably one of the most famous uses of this is GOMS. This is Goals, Objects, Models, Systems. It's an acronym for something. Goals, Operators, Methods, and Selection. And it's basically a specialized image. Once they realized that the modular processor wasn't going to be a catch-all, they started to simplify and sort of go to the core rules that they know are going to work really well. So they're basically using a cognitive model to predict quantitative time for expert users. So they're also saying, we're going to specify an expert user. There's no one-size-fits-all criteria. What you do, if you provide in this method, it's sort of like cognitive walkwords, you're going to think about the method that people can apply. The input is a detailed description of the UI and tasks. So it's sort of like you give people the storyboard of every little interaction sequence. And it lists all the steps hierarchically. And then you have this gigantic list of how long everything takes, how long one key process, how long it takes to move your eye. And you basically just sum it all up. You just sum it all up down every little branch of the thing. You can find out how long it would take to get to each one. So they found that the execution time, and again, this is assuming that you're a master of the task, there's no errors. They found that you can get within an accuracy of about 10% to 20%. So if you predict that ordering Uber ride is going to take 10.2 seconds, you should be able to be with this model. Just by giving them a storyboard, when you actually build it, you tend to be within about 10% to 20% accuracy, which is pretty awesome. And of course, what you can do with this is since it's easy to just look at these pathways, so you can see a very common pathway, like ordering a car is taking a very long time, you can actually sort of like perform. testing if you're doing like a, for a program, it basically works if there's lots of penalties. Like going into this screen and then back out of that screen is adding five seconds. Like maybe we're going to do it in one screen. And it lets you sort of do like performance testing on a theoretical human. And of course, it lets you do any testing because you can basically just do this evaluation for many different versions. So, you know, it does give you a nice quantitative measure, even though you may be working on a storyboard, or even just a text description of what this thing looks like, you actually get out time, which is fairly unique. And when, you know, a UI section is revised, if you have a really long branch of like, you know, a flight control operator that's basically like scheduling a new piece of like equipment to be run because the 730 seconds are all grounded, is you can basically swap out that one section, plug it back in, and you only have to recompute the one change. You don't have to run the whole sequence again like you would with a user study. So that's pretty cool. Of course, the downside here is that it takes a lot of time. You need specialized people to do this. It isn't just like you can pull out the method and go. It only really works with these very goal-directed tasks. So it doesn't work at all for the creative tasks. You can't model people's creative breakthroughs. It's just too complicated of a cognitive process. And it doesn't address UI issues like readability, memorability, design, intuitiveness. Like if you're just providing it a hierarchical wireframe of all these things, you don't really get the sense of pleasure. Now, people were very skeptical that this was going to work initially. They're like, you really can't boil down a human to something that simplistic. And so there was a landmark study that was published by all these authors, including Bonnie Johns, the faculty chair, and others. And it's really considered sort of like the gold standard of Dom's handwork. So what they did is basically the company that was before AT&T, 9x, was swallowed up as one of the baby dolls that became sort of AT&T's animal. It's basically a phone operator in New York City. And they were considering replacing the workstations with a human operator. So I think even now, if you go to a payphone and you hit zero, it does connect you to it. human operators. Has anyone done that in the last year? I know I haven't done it in years. But anyway, you could try it after class. Go find a paper and press 0 and see if a human picks up. But certainly back in the 90s, pre-internet, if you didn't have a phone book on you, there was no way to look up information. You wanted to find out if a restaurant was closing, but you didn't know the number. You could hit 0 and say, I want to know if whatever this Thai restaurant on 5th Avenue is open, or if you connect me. And they'll connect you and say, what time do you close? And that's basically how everything was done pre-internet, essentially. And so the human operators had these workstations where they'd say, who are you trying to call? And it's like, I want to call Bill Schmidt of Boston. And they'd type it in and say, OK, I'm connecting you. And they'd tack on an extra dollar for basically that help. And they were going to redesign it, because that's a very expensive thing. And a lot of people are placing these calls through human operators. And so a major factor in making the purchase decision was how quickly the expected decrease in average work time per call would offset the capital of making the purchase. Building this system is going to cost like $50 million. New computers. You're going to have to have the company build a new system. So that has to reduce the, it takes 20 seconds per call to 15 seconds per call. And we're paying everyone like $10 an hour. So basically, how long will it be to recoup? Even saving just one second per human operator would translate into millions of dollars saved. So here's what these call centers, this is not an actual call center, but this is what 80 of these call centers basically look like. Thousands of people in cubicles just answering questions from people all day long. So the manufacturer, they basically did the spec work. They made it. They basically drew all the wireframes and so on. And the manufacturer and Dynics agreed that the new workstation would be substantially faster than the old design. By one estimate, they would save as much as four seconds per call. They're estimating that one second save would save millions of four seconds per call. Dynics is like, for real, they're like, wow, this is going to be like extra money for free, et cetera. So then, but simultaneously, we should hedge our bets. Like that's also getting this like weird academic team that's talking about the thing called GOMS. And so GOMS, they got the wireframe team company and they took about two person months. So I think it was like, you know, four people for two or something. And they came back and what the GOMS prediction said is that the new workstation was actually gonna be 0.63 seconds slower than the old version. And so both Ninex and the manufacturer were like, no, no, no, it's like total BS. Like, what do you mean? Like it's so much better operating system, like all these new things we streamlined. How could it possibly be slower? Okay. So this was a very counterintuitive prediction. The new workstation had many superior features. The workstation used more advanced technology to communicate with the switch without the required speed. It was a new keyboard that placed all the frequently used keys close together. The display had a GUI instead of just a text terminal with recognizable icons instead of obscure alphanumeric codes. It was sort of like if you go to an airport and like you go to like the gate agent, they always have those really crusty computers back there. If you wanna change a seat, for some reason it's like 1,000 key commands, right? Like all this crazy modifiers. That's basically what it was in Parallel Engine 3. So we used it. So procedures were streamlined, combining previously separate keystrokes on one keystroke, like all the right things. They did all the right things, right? It sounds like amazing. Okay. So Ninex said, ah, there's no way that a GOMS estimate is accurate. We're gonna build it anyway. So they built it, they deployed the system, and then they collected a four month file and they processed 78,000 of these human operator problems. So what do you think, what do you think the data showed? How many people think GOMS was wrong and Ninex went, and the system was faster? How many people think GOMS was right and the system was slower? I thought I'd speak too much, so I can't. So it turns out, okay, that the real prediction, when they actually got the data, was that it was 0.65 seconds slower, and GOMS was 0.63 seconds slower. So it's a ridiculously accurate base game. They derived the estimate before anything was built, just from wireframes, and they got basically really. close to actually what it was going to be. Now, again, it's held up as an example, a big success story about the model human processor. And GOM is working. But again, this is a very specific use case. This is like a human operator that they knew the kind of very, like, I want to talk to Bill Schmidt in Boston. And it's like, key, key, key, key, key, enter, read it back. And it had very good timing estimates for all these things. So it worked pretty well because it was a very mechanical task that was just repeated essentially the same way a million times. And that's what enabled them to get this result. So nonetheless, that prediction was very counterintuitive. And it also extraordinarily accurate. So it is pretty amazing. So what has happened later sort of in life for GOM? People, like the AI community, are still like, hmm, there's something interesting here. But they try to make it easier. So it didn't require experts. And one of the tools is actually a CMU project. I don't think it's still being maintained, but it's a very cool idea. When you input your storyboard, you basically, these can be paper wireframes. So they can be like Photoshop wireframes. And you basically do the storyboard thing where you connect up all the errors, which is what I made you guys do. At least I did in class for your mail kiosk. And what you do is, on these transitions, you basically explain what's happening. Like, this is a button. And you click this button, and it goes to the screen. There's some sort of like parametization you can do on the errors. And you can basically connect all these errors. Then when you run it, it basically runs it like it's sort of like, I don't know, Figma or Envision app or Balsamiq. Because you can step through your interface the way you would actually flow through it. But at the end, it reports not only how long it took you to go through it, but it reports the GOMS prediction automatically for all the extra users, which is pretty cool. So it's basically a way to automate GOMS and storyboarding. And not just storyboard it for user testing, but storyboard it using cognitive data, like all this great psychology and cognitive cod sci literature that's been run since the 60s, 70s, 80s, 90s, and so on, built right into your storyboarding interface, which is a really cool idea. But despite that, I've never used this tool once. So I'm joking. OK, that is it for today. I've left time at the end of class because I wanted to talk. about your bake-off, too, because I know all of you are being lazy and not working on it. So now you have 15 minutes to meet with your team and have a plan of attack, and you can come talk to me with questions about your design. Oh yeah, turn in your papers guys, make sure your Android Bs are readable. So, I was thinking of having a visit, right? And when I said a visit, he's like, yeah, why not? It's nice to see you. It's like, it's not like we're going to see each other, but like, we're going to see each other. It's like, we have to be there for each other. It's like, there. So, and that is something that, it's not always like that. It's always there. It's always there. Wait till you hear this.  People want to know if you have a Google account. Because I'm just not a Google person. Oh, we all do. Oh, Google. I mean, I'm not a Google person. I also work for a company called... What company? I don't know. I work for Google. Yeah, I work at like, Power. I work at Google. Oh, my God, I work for Google. Yeah, I just started at Google. I'm just about to leave. Oh, my God. Oh, my God. I'm sorry. No, I was just... Yes, that's right. I know. I'm just trying to... I'm just trying to... Oh, my God. Oh, my God. Oh, my God. Oh, my God. Oh, my God. Oh, my God. Oh, my God. Oh, my God. Oh, my God. Yeah, OK. No. No, no. Like, it's like, I don't know. Yeah. It's obvious. Oh, really? Yeah. Let me take a look. Ahem. I don't know. I can do it. I can do it. No, no, no, no. I don't know. I don't know. I can do it. I can do it. I can do it. I can do it. I can do it. I can do it. I can do it. I can do it. This works. Yes. We'll try to fix them. The network's. Thank you very much. We get the effect. You guys feel good? Toby's gonna be here? Okay, okay. Cool. I didn't know I was doing it. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah.\",\n",
       " \"That's all right. Thank you. Thank you. Thank you. Thank you. Okay, let's get going, a little bit of a fail here in the middle of the room. Why don't you both meet in the middle? Why don't both set the tables like synchronized acrobatic swimming? If both of you converge, close the Red Sea, you will be graded on this. It's just kind of freaky to look out into a void, and inevitably there'll be some people that squeeze on the edge. Okay, so number one, I've been two out of your seven days to get this homework assignment in. So I just wanted to find out if there are any questions on the homework that I can answer. Yes? It can be, yeah, you can assume there's some infrastructure and the technologies can be plausible. They could be like, you have, I don't know, augmented reality where you know where everyone is all the time on campus. We don't totally have that now, but it's pretty possible. But it can't be like my smartwatch can read people's minds. So as long as it can be a future technology, but it has to be like, yes, we might actually get there in 10 years, that's fine. And you can imagine that like, yeah, especially if you're from a holography projector in a classroom, it's probably not, but AR headsets in a classroom, sure. Okay, so this is a good question. So we already talked about interface storyboards, where you lay out all of your screens and you sew little arrows. Just grab, you'll have to grab some tables out of a pile and stick them on the end there. And then we'll have to make it more than that. And so when you have little arrows, you know, the okay button goes here and the cancel button goes here. You may want to do that anyway, just to make, just for kind of good prototyping, but you're not going to turn it in. So if you don't do it, I'm not going to know. When I say I want you to turn in a storyboard, or what I say in the Canvas book is video storyboard. That's like the Hollywood style one where it's like, first there's a shot of Chris, you know, running off to get ready for class. Right? And then there's a shot of me frantically printing, you know, printing out the pop quiz I'm going to give you in two minutes. Right? And stuff like that. So that's like more like the video snapshot. So basically, because as you know, so what I recommend in sort of the sequence of orders here, the first thing you should do is meet with your team. up with an idea of what the app's going to do. What's a cool thing on campus you could do? What could a smart watch app do? Maybe the next step after that is, let's make the screens. And then you're going to have to figure out, OK, what are all the clips you're going to need to convey this story? That's the video storyboard. So there should be no arrows in the video storyboard. It can really be like Chris running to class, Chris printing pop quiz, Chris running downstairs to class. And then you basically just film those things, and you know exactly what you need to do. And you can even have what the narrator is going to say under each one of those clips. So there's no arrows. It's really just one through 12. Can you hand-draw? Yeah, it can be hand-drawn. You make it before you make the video. So you could make it like in Photoshop, but there's no point, the point of this is to be a video prototype, move quickly. If you think you need more than a week to do this homework assignment, you're basically doing it wrong. You should be able to do this collectively. The hardest part should be finding common times to meet your team. That's like the big challenge of the assignment, because everyone's so busy. But if you can sit down with your team, you can come up with an idea within one hour. If you delegate, say, make the interface, if someone's like, I want to make the interface, you can delegate that fast. They can probably knock that out within an hour. You guys basically did a five-screen lift interface in like 10 minutes in class. So you do a little bit nicer job, it's going to take you an hour. Then the key is you've got to figure out who's going to be like the filmographer, who's going to be the actor. Filming is probably going to take two hours. And then you're going to give all that footage to someone else, again, delegate. Maybe someone can take point on putting all that video together, someone that has maybe some video editing skills, that might take an hour or two. So collectively, you're talking about 10 people hours, probably like five hours together as a group. If it's taking more than that, you're doing something a little bit too high-fidelity, or if there's some other dysfunction, maybe you should email me. Does that make sense? Yeah? So again, look at the Canvas submission materials. The TA is going to post the actual rubric you'll be graded on, I believe, today. You'll see it's like one point for the video prototype, one point for good motivation, and one point that you have all your screens, and it's pretty, you know, do the hit list. And again. we give a bonus point because he's done a nice job or it's a really cool app. Any other questions on the homework assignment? Yes? I mean, you want to be able to see the interface. It shouldn't be like a long shot of someone on their watch. We do want to see the interface. We want to see how the app is solving that person's problems. So yes, just like you saw in the example I showed in class, generally it's like a shot like this and someone looked like this and then sort of a close-up of them actually doing it. And that's probably the best way to do it. Any other questions? Okay, so just a reminder, the really thing that we're kind of grading here is that it should be situated in a rural context. Why is this app solving a need? And it's about a story. It isn't about the app per se. The app just lives within the story, at least according to my recipe of how a video prototype works. So make sure that you focus on that story, focus on the motivation. What's the problem with this smartwatch app? It's a killer app for CMU students. And I don't really care at all about production quality. If it is a video, it probably exceeds my boundary for production quality. Okay? So don't get too fancy on that. With respect to grading, I'm sort of getting a little bit of a trickle, which is pretty typical for a class that has like 70 people in it. If you do have questions about grading, like I was here for a couple of days, but it's a zero in Canvas, whatever, start with the TA just so he can be a bit of a filter for me. And then if that fails, then you're welcome to CC me. But he's really the point person. He'll be the point person to handle that. And as I said, pop quiz. So with that, if you want, if you want to come to this... Okay, so another minute there. Okay. Okay. Okay. Okay. Okay. By the way, is this anyone's first handbag? How many people have done it? Okay, start to wrap up if you're still going, that's fine. Start to coalesce them if you can. Is anyone still on the wait list? Is there anything that I don't know about? Maybe people can sense there's a pop quiz coming. Okay, we have them all? Yes, final call? Okay, so very quickly here, PowerPoint. prototyping tool because select all that applies how many people put most businesses people can open PowerPoint definitely say that's a pro allows non-designers slash coders to participate more easily I would also say that this is a nice benefit most people can use PowerPoint it can easily embed interactive 3d graphics it might be able to PowerPoint has so many features but I wouldn't say that makes it a useful prototyping tool in fact it probably makes the worst prototyping tool because it ensures it's high fidelity and cannot see built-in graphics and tools are simple encouraging basic prototyping yes I would say that is true even though it's not designed to actually be prototyping okay number two Xerox PART tried to make its new computing interface for its Alpha and Star systems friendly and familiar we heard in the video by selecting which of the following best answers how many people put offering an on-screen cursor that could be controlled with a mouse okay that is true and that did make it easier but I don't think it made it friendly and familiar it's how about be mimicking lots of objects and processes already found in offices of that era and that is indeed the right answer that's what made it familiar to people it's because it was really drawing on things in their office environment there was a little mailbox like envelopes and everything copying and pasting was all stuff that was down the opposite of the editor transported into the digital realm so bitmap graphics on the screen that's true but it's not friendly and familiar no one had seen a bitmap graphic monitor or maybe TV if it wasn't interactive orienting the monitor and of course the configuration of landscape and yes that's who made it more friendly from there like a piece of paper but I think by far the best answer here is B okay the Dynabook only correct answer here is C the cardboard prototype of a hypothetical tablet made by Alan Kay how many people got C okay good and then finally if we can you tension I think for this one so Bill Buxton and mentioned two of his quotes last lecture he hypothesized that any significant technology you're likely to see in the next 10 years it's probably at least 10 years got D. Okay, the hands are like up, but they're like slowly shrinking down over time. Okay, it should be D. Okay, today it's going to be a little bit of a different lecture. I wanted to introduce you to the tool that we're going to be using for a lot of the interactive prototyping throughout the semester. So even though this is not really how you'd ever want to build like a mail kiosk, it is what we're going to be using as a common tool. So for those of you that have never used this language or tool before, it's a good kind of primer, and I want to show you just how easy it is, especially for people that have less familiarity with programming. So, number one question is, how many people have you used processing before? And not like the flow of information processing, but the actual application processing, the language application. Raise your hand high if you've used processing before. Like two people. Great, so this is going to be new to most of you. How many people have used open frameworks before? Okay, great. Well then, most people are going to want to tell you, for the processing people that have used it before, I might have you be like my live debugger if I start screwing up all the syntax. So what is processing? So processing, if you can download it for free, it's at a processing.org. It is a cross-platform IDE, and essentially it's sort of its own little mini-language. It's an open source project. It was designed primarily for the visual arts community, people like Dolan Levin, if you're familiar with him, and CFA. He's a major, like main contributor to processing. He uses it a lot himself. It's basically Java with a really nice graphics wrapper. That's how I think of it. Processing really isn't its own language. It sort of does make some abstractions to make it so it's not quite compatible with Java, but they're actually simplifying abstractions that make it easier. But if you know Java, it means that you know how to use processing. They're pretty much interchangeable. And really what processing is is this big dump jar file, this big ball of Java code that gives you all these wonderful graphics primitives that I'm going to show you in a second. So it has hardware accelerated 2D and 3D. pretty graphic, which is really nice, so if you draw a sphere, it's going to be using OpenGL or something equivalent under the hood. And again, 2D graphics are all GPU optimized as well. So even though it's Java, we think of Java typically as being a little bit slower than sort of the C++ equivalents, actually in many graphics tasks, processing tends to be faster. It's like running in a JVM. It has really good networking and web facilities. There's commands that's like, get web page. You really just like, that's the function, you just say like, whatever, chrisharrison.net, and it really just returns like a string with everything on the web page, like one kind of command, then you can parse it super easy like that. We're not going to get into anything like Arduino this semester. We will get into sensors later in the semester, but there's really easy ways to interface with things like Arduino to be able to pull live kind of sensor data from the environment. There's really good sound and video support. You can run things like OpenCV. You can connect to a Kinect camera and so on. There's all these wonderful libraries that you can get at this link. I use processing a ton in my own research. Like all these projects and all these interfaces that you're seeing here, we're all actually done using the processing graphics library. So sometimes we're using with projectors, sometimes we're using with monitors, sometimes we're using it on mobile devices. It's just an incredibly good rapid prototyping tool. In fact, it encourages, like the interface is not landmark achievements of graphic design, but they get the job done. And often, almost all of my research projects have videos that are associated with them, and the interfaces that you see in the videos are more often than not written in processing. But we don't use the processing ID, which I'll talk about later on. So it's a fantastic tool for hacking, for fast hacking. You're going to be using this like crazy this semester. You're probably going to get pretty proficient in processing, which is great. Importantly, it's a prototyping tool, but the simple things are really simple. It's the hard things that are really hard. So it's sort of like, if you think about a tool like Eclipse or IntelliJ, it's sort of more linear. As you get harder and harder, it sort of follows this linear scale. Processing is sort of like the signal, where the simple things are easier than they should be, but the hard things are like monstrously hard more than they should be. So you have to just realize that it's a tool only for some things. There are some really excellent third-party libraries, like you can just embed OpenCV or something like a computer vision library, or like loading 3D objects and you want to be able to draw them in a 3D space. You can literally just parachute those in with one line of code. And that can get you pretty far. When you go to Optical Flow Tracking or something like that, it's pretty fast. But once you want to do more complicated, like you're going to need huge, like 20 files to make a project, then things start to fall apart. Almost all of my processing applications are single-file programs, which I know, if you're like a software engineer, there's tears running inside your face right now. But that's because it's not meant to be a great idea. There's better tools for those things. So I would say it's great for graphics, but it's really not so good for GUIs. If you try to do a GUI in this, it's going to be much more painful than using something like Qt or Swift or whatever people are using these days. Okay. So lots of libraries. Again, I mentioned that they have things for at least text and reading QR codes. There's three optical libraries. There's physics simulators for particles and collisions and stuff. And you can do some really cool things in a little bit of code. And there's an excellent library. Every time you install one of these libraries into the processing application, and I'll show you how to do that, it comes with all these great examples that really let you kind of cut to the chase and see how it's done. So this is the processing IDE right here. And I'll be showing you this in class. We're going to make a little app together. It's okay. It's a pretty weak IDE for what it is. But again, it's sort of simple because it wants to be simple. It doesn't want to have debug and tracing and profilers and any of those things. So it forces you to sort of be simple on that career strength at the same time. I don't actually use processing in this IDE because I think it's too slow. So what I do is I grab that processing.jar file and I just drop it into clips. I use Eclipse as my IDE. So if you're like an Eclipse lover or an IntelliJ lover, you just Google how to use processing in Eclipse. There's a page on processing.org that shows you how to do that. import the library, and then you can just program in Java, and you're off to the races. So, let's actually look at the... Yes? So, if this supports opening a library and everything, how can it just look like a prototype at the end? So, it is an interactive prototype. So you're correct that it's sort of getting up to the very highest fidelity. But you're probably never going to ship a product that is going to be slower than it needs to be, there's no way to track if you're leaking memory. So it'll get you to pretty high fidelity, but you'd never want to be like, this is my startup's code. So it's really pushing up against that boundary. And again, I'm sure people have probably apps on the App Store that are basically written in processing. But I think it'd be pretty dubious. I wouldn't be an investor in a company that said you've built your whole product in processing. So this is the high. After this, you want to go to a real development team, real software engineering practices. Okay. So let me mirror my screen here. And we're just going to do, we're just going to build something right now. So let me unmirror. Okay. So here we have, I'm going to open up the processing IDE. Can everyone read the text in the back there? Yeah? Back row can read? Okay. So there's two, there's two functions that you have to basically have in every processing application. And that is void setup. And there's, you're going to see me make lots of, amazing how typing on a keyboard, that just like a slightly different angle crashes your error rate. So there's two things you have to really include. Setup and draw. No one can guess what these two functions do, methods, whatever. There is an important difference between the two. But there's also some similarities. Yeah. I guess I've read the code that would start as soon as the application is open and draw would be on the screen. Yes, so basically setup gets called once at the beginning, it sets up your application, and then draw gets called automatically by processing over and over and over and over again, so you can do things like animation. So in things like setup, we probably want to set, for example, the size of our window. Now a window automatically gets created, so we want to set the size, we're going to say 800 by 800. And that's the set that says make a new window, it's going to be 800 by 800 pixels in size. And let's say in draw we want to do something very basic, we just want to draw a background color of black. Can anyone guess what the method is for drawing a background? I most certainly can. Okay, editor text, 36. Look at that. Yeah? Bigger? I can go monster big if you want. I'm going crazy here. Let's go all the way to 11. Okay, so how do you think you draw a background? Yes? That sounds very complicated. You're on the right track, but no, it's even simpler than that. It is called background. Now you're on the right track, if you wanted to draw black, how would you draw black? What's the... Yes, but that's white actually. We can do that too though, so 255... That's full brightness. Full brightness red, green, and blue. So if we run this code, what we'll get is a nice big white window. There it is, white. Okay, fantastic. But for our purposes, we're going to go black. Now what's interesting is that there's many different ways to call background. You can have it with four arguments, and the fourth argument will be a translucency. You can have it with one argument, and obviously there's no way to really specify RGB in one argument, so it assumes that you're doing it in grayscale. So if I just say background zero, this is a background color and it's only one argument to assume it's grayscale in this case it'll be black and so here we have another black one. Pretty straightforward. Now we want to draw a circle okay so let's do a circle in this case because we want to draw more than just circles it's actually an ellipse so we can actually make something that's kind of oval and what we'll do is we'll draw it in the middle of the screen so we'll just do and we have various constants which is really nice like width. So width is just a constant that we gave us and it's the width of the axis once you set this, width is just available to us as a global kind of variable so we can say width divided by 2 and height divided by 2 and then we need to specify the size of our oval so we'll just say like I don't know 70 and 70 okay and now the problem is we haven't set anything about its color so how do you think you set the fill color of the circle? So if we want to set it to white we can just use this we can also do something crazy here like I don't know what to make up a random hex number here. I have no idea what color this is going to be. Boom it's blue okay so now we have our little circle on our screen in the middle and if I were to do live resizing it with the height get updated automatically. Now let's just say we don't want to just draw a circle in the middle of the screen we want to draw the middle. So if we want to have this blue dot and I'll make it a little bit bigger so it's a little bit easier to read let's say it's like 100 by 100 and let's just for simplicity also make it white. We want to have it follow the mouse okay now this can be sometimes kind of laborious other programming languages. I will just save this on the desktop. We are just going to use the again constant variable that I maintained for us such as mouse x and mouse y. Okay and those are again global variables that are maintained for us and they get updated automatically. So now I have a little ball that follows my mouse, okay? And if you zoom in, you'll actually notice that this is anti-aliased and everything, and it is like accelerated. Okay, so, yeah, we have a gigantic parasaurus sensor, okay? Five lines of code in here. Now let's say we want to do something more interesting with the dot, we want the dot to move by itself in a random way, okay? So what we can do here is rather than just saying, well, so we'll have, we'll just make new floats, let's say, that is going to be called, like, ball X. And what we'll say is we want to have a random number generated between, let's say, zero and width, okay? So this takes two arguments random. I can be like zero and 10, and every time this gets called, it'll give me a random number between zero and 10. In this case, we know that the size of our screen is width, so if I update the size here, it'll automatically update it. That's ball X, and then we'll also make ball Y, and we'll make this be height. They happen to be the same. And then what we'll do here is I'll just call ball X and ball Y. Now does anyone guess what's gonna happen here? What's this application gonna look like? Every single draw loop, I'm gonna get a new random X and Y. So what's that ball gonna look like when we run this program? Yeah? It's gonna bounce around. You think it's gonna bounce around? Well, like, maybe you don't see it. Yeah. So it pops there. It's gonna pop around, exactly. So if I just run this, here it is. Okay? I apologize if anyone has like, I don't know how to do this. So, yeah, so basically every draw loop, which it's calling it 60 frames per second is the default. Does anyone wanna guess if you wanted to modify the frame rate of processing how you would do that? It's frame rate. You guys are not catching on to this. So, I mean, I can just specify like 30, and now it'll run at 30. If I wanna make it run super fast, this is above my projector, I think it's only running at 60 frames per second. So you can see it's a little bit faster, but unfortunately this is now running faster than the projector can do. So it sort of looks like the 60. Anyway, that's how you adjust the frame rate. Okay, now, so random is not problem one. We don't want it to ball everywhere, right? We want a ball that sort of floats everywhere. So there's actually another type of function that we can use that's called a noise function. And one of the functions is a Perlin noise function. Has anyone ever heard of Perlin noise? A few people. So if we, we'll go to the whiteboard here. I think I have a pen. So if we think about a random function, I don't think Eric has done this before. Okay, so if we think about this as time, a random function basically just generates that every instant in time you call it, it's just a random dot, right? So this is what we're seeing sort of on the screen. It's going crazy. What we really want is over time, it's like to kind of move randomly, but it follows some function that, like the next time I call it, it isn't totally independent from the last time I called it. And this is what a noise function would do. It's still random, essentially, but there's continuity in the process. And so one of the kind of most famous functions is this guy called Ken Perlin. I actually took classes with him when I was an undergrad at NYU. Really interesting guy, but one of the few computer scientists that's won like an, I guess, an Academy Award that he has in his office very proudly, I will say. Rightly so, I suppose, I don't have one. And the reason why he did it is he won it for Tron, because that was one of the very first computer-generated movies, and they wanted to have textures in those, sort of like, you know, clouds. But it was running on like, I don't know, like some fax machine. It only had like 200K of memory, and so actually storing the textures meant they couldn't even load the program. So they went to Ken Perlin and said, can you generate textures for us? But obviously, you don't want to, every time you render one frame of the video, the clouds are totally different patterns, so they have to come up with a way to generate something that looked like a cloud, like random kind of distribution, but that was consistent, or like evolved over time, so it's like clouds fading out. And so in all what they call procedural textures, they were created on every single frame. So if we want to do this here, what we do is we call the function. noise. And in this case, it will generate something between this function is a little bit different than that. It only generates a noise value between 0 and 1. So in order for us to scale this up, we have to times it by the size of our screen. The other thing that's a bit different is this is a time function. So what we want to do here is we need to make a new variable. Let's make a global variable called int t. Actually, let's do float t so we have a little more precision. And we'll start it at 0. And what we're going to do is we're going to stuff t into noise. And the final thing we want to do here is we need to increment time. If we just run it like this, you'll see it runs. But it's just there. It's not a very exciting Perlin noise function. And I think every time I run it, it will see that it's going to say yes. So what we want to do now is we want to increment t so that we follow along that curve. So what we need to do is we just need to do t plus equals 0.01. So it's incrementing t. t is going to count up from 0 to something. And if we run this now, we're going to get this sort of random walk and sort of Brownian motion like. But it's not quite perfect. There's an unusual behavior about it, which is that black. So when you say linear, you mean it's only moving along the diagonal, right? And the reason why is that again, Perlin noise, it randomly generates that function. But if you call the same position on that line, it will be the same position. So in this case, we're exceeding both their x and y at the same time. And so they're moving. They're basically getting the same value, which is they only move along the diagonal. So we want to make this look more random, which is jump our y value ahead on the line by like 10 points or 100 points, somewhere far into the distant future. And now it'll look like they are independent. So even though they eventually get values lower than that, but the user is never going to see. So now we have true sort of random 2D motion. Okay? Okay, so now we have our ball moving around. It's no longer following the mouse, it's just following its own coordinates. Let's like decorate this in some interesting way. So why don't we add eyes to this, eyeballs to this. So instead of, let's make our face a little bit bigger, like 150, 150, and then let's add some eyes. What we're going to do is we're going to set the fill color now to black, let's say, dark eyes, and we want to do a new ellipse. We're going to make a smaller one, so it's going to be, let's say, ball X minus, I'm going to go up and over, so we'll go like up 50, and then ball Y minus 50, and then we'll draw a little 20 by 20 left eye, and then we'll do the same thing here. So again, we have our circle. We want to go up and over to draw an eyeball here, and we want to go up and this way to draw an eye here, and we'll make little tiny eyes, and we'll see what this looks like. Oh, those are really freaky. I guess the size is wrong. So we don't want to go quite so extreme. I think we want to do like maybe 30, 30, and then we'll make these bigger too, because those like big eyes are way cuter. Ooh, that's because they're going to overlap now, aren't they? Eh, it's okay. Kind of alien-like, but we'll go with it. So now we've got eyes, and now let's say we want to do something cool with the eyes. Like when you click on the screen, the eyes go red. So there's lots of little functions that you can override for key presses, and mouse clicks, and so on, and so what you do is you just do void mouseClicked, and what we'll do, actually what we're going to do is do mousePressed. So there's two different functions. MouseClicked gets called only on the action of the click. When you hear the little click sound in your mouth is when you get the click. MousePressed is always called as long as that's down. we can even do this in an easier way. There's a variable saved for us here called mousePressed. We can say if mousePressed, again, processing makes it so you can do different things. So there's actually just this Boolean that we've input called mousePressed. It's true when the mouse is pressed. And we'll say that if the mouse is pressed, we want to have our fill color. And we'll do something cool here, like red. So we'll do r, g, b. And else, it'll just be black. So again, terrible indentation on my half, but you can get the idea. Okay, so if I click now, yes, it's amazing, right? Now, you'll notice if you're very astute, you'll actually notice there's little tiny strokes around the eyes. You can't see them in the black background, but there is a little black stroke. So we haven't turned off strokes. So you can actually control not just the fill color, but also the stroke of things. I'll just show you that really quick. So we can say the stroke weight to like three. So that's setting the stroke size. And then we'll set the stroke to, we'll just set it to something funky right now, like cyan, just so you can see the strokes on this. Now you can see these strokes. I don't know if you can see that in the background. All the strokes are outlined in cyan, three pixels cyan. So you can turn off the fill. If you want to turn off the fill, you write the function, no fill. If you don't want any strokes, you just set the function, no stroke. So what I'll do, even though I'm setting the stroke, I'll just say after this, no stroke. And that means it'll turn off all the strokes entirely until I reactivate using the strokes that we turned off. Okay, so we've got our eyes. We've got some clicking here. Yeah, great. The other thing we may want to do, so the other thing we can do here is I can set this function called mouse clicked. And again, I can override it. And what I can do here is just to show you that this works. If I will print out something, this is the debug information website. Okay? So let's just say I want to, when every time the mouse clicks, I want to print a line. So this is how you do debug. print no end, unless they just want to print out the mouse, the mouse hex position for whatever reason. So if I click, if I run this now, every time I click, what you'll see down here, it's a little bit small, I apologize, it's saying 446. So any time you do a print line, for debug information, it writes to the console at the bottom of the IDE. But I can come in and click anywhere here and you'll see that it does indeed update the 2.16 and so on. Okay, let's add another behavior to our face here. Let's go back up and add mouse. Add mouse seems to make a lot of sense. So let's say we want to have, let's set a new, we want to always set the fill color to black. We don't want a red mouse, that'd be kind of weird. Let's say we want to have an ellipsoid. We can do arcs and rectangles and all that kind of stuff too. But let's not worry about that. Let's do a rectangular mouse and make it robotic. So rectangle is just rect, nothing super complicated there. In this case, we want to put it, I believe you put it top left to bottom right. So we'll move over 30, in this case we want to go down below the nose, the center of the circle is in the middle, and we want to make our rectangle extra wide, so we'll make it like 80 pixels wide, and smaller, like 20 pixels tall. And I'm going to share all this code, so don't feel like you have to write down all this code, because you're going to get this whole 50 line code. It's a little bit wonky here. That's cool though, it's sort of like cartoony, a little off to the side, kind of smirking. So let's do something cool here where we actually do something with the mouth based on the mouse distance. So let's say we want to set the height, right, our height value is here, 20. So I've compiled a 3D soft center to make it 80 wide, but we only moved it over 30 pixels, right, so to get to its center point we'll have to shift it by half its width, which would be 40. So let's say we want to parametrize the height by how close it is to, you know, your cursor. So as it gets close to your cursor, it tries to eat your cursor, OK? So the way we want to do this is let's say we want to have like a float mouth size, OK? And we want to do it by the distance it is away. So anyone want to guess how we calculate the distance between two things? How about just in general mathematics, how do you calculate the distance between two x, y points? The distance formula? That is true. Anyone know what the underlying math is? We have to reach back to high school here. Square root of the difference in the x position squared plus the difference in the y position squared. Yes, this is Euclidean's formula, right? x squared minus x1 minus y2. And then you put it in a big root function. So luckily, we don't have to think about that because there's a function that's been processing called DIST. And so what we'll do is we'll define the DIST from the ball x and the ball y to the mouse x and the mouse y, OK? So in mouth size, that's going to calculate the distance in pixels we are away. And maybe just for fun, let's just print this out on the screen. So we'll just say we want to put text on the screen. So we're just going to do mouth size, and we're going to draw it at, let's put it like in the top one, so I'm going to say like at 30, 30. What happens then? So let's see what this looks like. Oh, do I miss the fill color maybe? I'm going to put this at the bottom here. So I think it might be that text requires a fill color. Yes. OK, so again, you can set the text size and stuff like that. I'm not going to go into it. I think it's literally text size. But there it is. My mouse is there. So if you look at the document, it isn't a ball, you can see it's very small, very small, 40, and then it's going to go farther away. And this is just drawing text information on the screen, which is useful. That's a good way. Cool. So now we've got the text size. Let's try to figure out how we can take the mouth size and turn it into the mouth size variable. We don't want it to get bigger the farther we are away. Plus 600 would be gigantic. So we know that we need to invert this. So I think we'll do the mouth size divided by, like, 10. So we want to make it... How about we do... The farthest we can get away is around 800, I think. So we do this. It should flip it into a... Yes, it'll flip it into a smaller number. And if we divide this by, like, 10, I think that should make it more scaled appropriately. Let's see. I'll just take this variable and stuff it into the Y value here. Again, you have to sort of play with these values. Let's see what happens. It's huge. As I go away, it gets smaller. But it's sort of, like, reaching up this top of his head. So that's kind of weird. Okay. So what we need to do here, if we want to limit how far... We don't want the mouth going, like, here. So we need to limit the maximum size. And so what we want to do is constrain the values between some minimum and some maximum. And again, we have a really nice function called constrain. And what we'll do is we'll type in... We want to constrain... And I don't remember all the arguments for this. It's 1, 2... It's going to equal mouth size. We'll give it a mouth size. And we'll have a minimum value of, let's say, 5 and a maximum value of 30. Okay? So what this is going to do is, whatever you put into the mouth, whatever you put into this function, which in this case, whatever value we have over here, it's just going to limit. It's basically going to do a min and a max function and give you that. So now, I believe... The farthest I can get away from it... The mouth is only ever going to be 5 pixels small. and the maximum size I can get when I'm close is going to be 30. It's not the most exciting function, though. We need to make it more aggressive, maybe. Let's say I want to divide by... I'm just going to make it more or less, make it smaller farther away. Let's see. It's a very small mouth movement, but you get the idea. And we can still, of course, flick the eyes. Let's not forget about the eyes. OK, so there we have our little eyeball function, and we have a little mouth that has to get closer to a little friend. OK. There's another thing we can do just for fun, which is we can change the background code. So right now we're just drawing that black background. But we could do something more interesting, where we don't try to overlay a black background. In fact, you're not even required to draw a background at all. So we just comment out, let's just try this here, I'm going to comment out my background. So the first thing that happens in draw, when I go through the sequence of events, it's just getting the whole background to black. But that's not required. I can just have the background never get redrawn. Then it will draw everything there. Can anyone guess what's going to happen when I run this? We don't draw a background? I think the default is gray. But then what's going to happen? You're going to have a trail, because it's never clearing the background. Right? So now we've got a little worm guy. Right? It's actually, just to make it easier to see, I'm going to turn back on my stroke. I'll turn it into a small stroke. Now we're going to put that cyan stroke back on things. So I can see. We just created modern art here. What's also really cool about processing, is that everything is being drawn as a vector. It's like a one line function. This is called save to PDF. It will take whatever is in the graphic buffer now, and it will save it as an SVG. like Twitter profile picture, just dump that to this and we're like off in the race a bit. And of course, you know, you can scroll it. Okay. So, but we can do something a little bit cooler, right? So this is not ruining the background at all, which is, you know, I mean, yes, if you want to buy this app from me at the end of class and make it be like desktop background, I'll power it to you. Let's say we want to do it with semi-translucent, so we don't just see it endlessly, like it runs forever, it'll eventually just fill the whole screen for all the noise it runs. So what we can do is we can say, we want to have a black background, we want to draw a rectangle that's semi-translucent. So we'll just say, so in this case, just to clarify, this is gonna be a zero, zero. So this is RGB, and again, you can specify with 255 being fully opaque and one being like very translucent. I don't know, in fact, I'll make this a bit less. So what this is saying, we're sending a new fill color that's like black, but very translucently black. And instead of using the background function, which is a little bit basic, we'll just draw our own rectangle, which is the size of the screen, so from zero, zero to width and height. If you look inside the background function, it's really just drawing the sets that are rectangular, basically just rectangles out the whole background. In this case, we've replaced background with our own function, or our own two lines of code that does something very similar. In this case, we're just drawing a big, gigantic black box, but it's a translucent black box, and so it'll have a slightly different effect. So now, over time, if you think about how it's averaging, it's averaging all those layers together by drawing this black square, is you get sort of this ghost effect, where there's this trail. And eventually, it gets even into the background where you can't even see it anymore. Does that make sense? Yeah, all right. So it's the opacity of the word. Everything is 255, unless you specify it differently. So you can actually override this. You have to look in the docs. You can say it's zero to 100. You can also do all, you can also change the color mode from RGB to HSV, which is kind of interesting sometimes, so. But yes, everything is 8-bit colors, so. 2x5, 2x5, 2x5, 2x5, 2x6, 3, 3, 5, 5, 3. Yeah, it's a little bit trambucious. This 5 has a 2x5, so it'll make 2x7. And if we want to go back to our, if we wanted to mimic the background code, we'll just set this to 2x5. This is now just a fully OK black. And you'll see that when we run this, it's just going to look like it did before. OK? There's different ways to do the same thing. One function I have not shown you is how to do lines. Let me just do that. I'll go back to 5 just because it's cooler. So maybe at the very bottom here, after we do all the mounts and stuff, what we'll do is we'll set a cool stroke color again. We'll set a stroke color of, let's say, red. So 2, 5, 5. So this is, again, stroke color. So if you know how you have fill, you set the fill color. When I set the stroke color, you just type in stroke. And we'll just make it red here. And what I'll do is, anyone guess how to draw a line? Yes. More often than not, it's worth it. So we're just going to draw it from, let's say, ball X, ball Y, to mouse X. And I guess I want this to appear. Well, I'll show you what this looks like. If I do it like this, every frame now is just going to draw this red line. So that there again, this little red line. But of course, it's drawing it from his nose. We want to make this look a little nicer. Let's say it's kind of like a lion's mane. What we'll do is we'll move it up a little bit in our code. So it's before we draw the ellipse. So we'll just put it. We draw the ellipse here. So what we'll do is we'll just put it here. And then we want to turn all of our, we want to probably go to no stroke. After this. We don't need this anymore. So we're just turning on the stroke. Oh, we don't have ball X and ball Y yet. OK. Sorry. I'm going to move this down after we compute the stroke value. And again, it's so large that I can't even see most of the code. But I'll put some lines. Line breaks. here, we are at like 42 lines of pretty sparse code. So just to make it clear, this sort of setting up our canvas here, then we calculate these two ball values according to its curl and noise functions. Then we're going to draw our line. Then we're going to draw our face ball. Then we get down to our eyeballs. Then we get down to our mouth rectangle. And we have this debug text. So now, and I'll also set this stroke up. Let's use a stroke and wait for it. OK. So let's see what this looks like. So now the red line should appear behind. As we draw it earlier, it should appear like, oh, wow. It's pretty freaky. OK. Anyway, so I think you guys get the idea. Pretty simple. So you also notice that our text, unfortunately, we have it in the background, but the text is stacking too. Whatever. So basically, the key thing is, if you want to draw things like circles, and lines, and rectangles, and mesh in the background, images also need to have a reference in it. And you have to be able to create an image on it, like image equals load image, like chris.jpeg. And then it's literally like image chris, 25, 25. It's also really basic. Yeah. Is there a test that's featured in the document, isn't it? Yeah. So if you use some of the libraries, they'll do a bit of testing for you. In the scaffold code I'm going to give you for the big OS, I just do the cheap and dirty hit statement that this has like x, y. So if you want to spend, I find it easy enough just to write the one line of code to do it, but yes, there's many ways to do it. OK. So hopefully this gives you an idea. It doesn't matter what your programming level is. I would highly encourage you to download processing. Again, it's self-contained. You install the app. It'll just run on your computer, Windows, Mac, Linux, whatever. And you should be able to put together something like this pretty quickly. OK. Now I want to show you something even cooler, though. So the other nice thing is that you can do this on Android. OK. So I'm going to show you how to do this on Android. in Java mode, but you can actually deploy to Android, really, really, really trivial. So let me actually just show you how this works. So we're gonna go back to our app here, and it says Java there, and what I'm gonna do, is I'm just gonna switch it to Android. That's how easy it is. Okay, so now we're in Android mode, and what I'll do, and this is why I was searching these phones, is I'm just gonna take this Android phone, I'll plug it into my computer, and what we should see, if we're lucky here, is that it'll appear under Devices. Give me a second. There it is, so my LG phone has appeared under Devices, and what I'll do here, is I will hit the same play button, and what you can see down here in the console, is it's gonna compile this entire project for Android. So it takes a little bit of time to build the whole project, it's using the Android SDK, and now it's saying it's packaged it up, and it's deploying it to the phone, and what do we know? Look at that, unplug the hottest new app on the App Store, going live at the end of class, and you'll see, so what it does for mouse coordinates, okay, is it mouse X and mouse Y, the simple way to handle this, is you just treat it as finger coordinates, so your finger, there's no multi-touch by default. There is a way to get the multi-touch, but all the kind of mouse things are emulated here, so, I mean, literally, like, we just wrote an app, and four seconds later, we're running on Android, and this is an Android app that you can actually put on the App Store. Yeah, yeah, you can just package these up and submit them, there's a, what do they call it, packages on Android, APKs, so it's just an APK, and it actually appears, as on the app, there's little processing icons here, that actually, that you can share with people, and like, drop them into the app. So yeah, it's pretty flexible, really, really, really easy to do some things. Again, you can, there is ways, we'll get to it later in the semester. You can get images, you can get the compass data, you can get the accelerometer data, and in processing it's like, get accelerometer, and it's like X and Y. And we're not done yet. There's another thing we can do. Which is... Try that again. Coming. It's actually running processing in the browser. So there's this big blob of JavaScript called ProcessingJS, and basically you just drop it in your webpage and you can just trivially move your processing apps onto the web. So let me again just show you. I'd like to dispel the myth here of how complicated this is. Let's go to this lecture. Here's my HTML demo. So in temp.html, I'll just open this up. Here's what I have. I'll make this nice and big. Here is my very rudimentary webpage. So what you see is I'm embedding this Processing.js, and I have basically a canvas that has an ID, and I basically just brought my processing script in here. So what I'll do is I'll grab, literally I'm just going to copy the code, and I'm just going to paste it right there. So I did no code modifications at all. Hit save. And now I'm going to go back to my file system, and I'll open this up in Chrome. And lo and behold, there it is running in the browser with no... There's no Chrome extensions here. This is just pure JavaScript that's interpreting my blob of processing code, and I've embedded it into a webpage. And everything still holds true. You can see the eyes still work. In fact, you can even see it popped open a console, because I still have that println statement for it in my analysis. So if you want to have a really sick, like animated icon, like an animated logo on your portfolio or on your website, you can really embed it like a little miniature to set the size of your thing like 100 by 100. And then you can have like a cool, like your name floats out of a glowing head with eyeballs and stuff like that. I won't care. It's going to pop it right on this code. It's really, really, really, really easy. That's the thing I want to convey to you. The processing is super flexible. It really makes simple things pretty easy. Don't go with GUI. Okay. So there you have it. You basically just write your processing code in between the tags and you're off to the races. Hit that. If you are like a JavaScript guru, I hate JavaScript personally, but I know a lot of people love it, there is actually like a port of the processing, like graphics routines that run on the canvas. And so you can just sort of pseudo, it's not quite the same syntax, but a lot of the same sort of principles and function calls and stuff all apply. So you can download that. As I mentioned, I don't ever really use the processing ID. You can just sort of fumble around with it a little bit. What you do is if you're on Mac, if you kind of go to like right click on processing, which is a .app, and you like show package contents, if you navigate into that file structure, you find this big gigantic jar called core.jar. And that is like the heart of processing. And what you do is you just add that as a library, as an external library in Eclipse. So you enter Java build path, add jar and core.jar. And then everything in processing just works as you would imagine. The only thing that's weird about it, and again, follow the tutorial, it's whatever reason in processing, you have to launch it as an app link, which is like super retro. But that's like the only odd thing. If you follow the instructions, it's pretty easy to get up and running. And what's also really nice with Eclipse is that it has like amazing auto completion, where you type in like noise, and then it shows you this is expecting a float, and it even gives you like the Java docs that says this is a value between zero and one, and it makes coding even faster than using the regular. And then I know a lot of people like Python, so there is again the sort of processing Python port, that again uses a lot of the same sort of simple mechanics. So you can do some pretty snazzy stuff in Python too. Okay, open frameworks, I just thought I'd mention this. So some people are like C, C++ people, like Java. like I hate Java. That's fine, I respect that. Then you can do it in C, C++, and there's sort of an equivalent, kind of, package, kind of, it's not, it's not syntax similar, because obviously you have to do a lot more stuff for manipulating, like in memory, like managing your memory and so on, but it's very, it's probably the closest analog to processing. What is really nice about C, C++ is that there's still the ability to compile to iOS, that's all you can do with processing, I mean, anyway, you can get a processing app to run on iOS, so that's a bit of a bummer, but OpenFrameworks does. I have a little video here of some of the stuff that people have built in OpenFrameworks, it's very similar to processing, but I'll just play their demo reel here. And this shows you some of the sort of things that you can do with these lightweight prototyping and logistic programming languages. Okay, one other thing that just occurred to me before I let you take the rest of the class and leave me to get jumping on this assignment. is, we didn't go through it, but there's a really nice other little trick in processing that like makes it for certain things where you, when I was watching that video, you can see a lot of things they're showing are actually in 3D. Right, so it's like, oh, how do you do 3D in processing? Well, the very cool thing is that all of those things I showed you, like ellipses and rectangles and lines, if you just, so I did like line ball X, ball Y, mouse X, mouse Y, there's another line function that's just XYZ, XYZ, and then it draws your lines in 3D. So if you want to have like your mouse be able to control like the Z plane or something, it's super easy. So there's all of the graphics drawings, all the 2D graphics drawing functions can take 3D arguments out of the box, no like special setup. You can also draw like spheres and boxes that look like sphere, box, all the same things that you saw before. And then there's this camera primitive where you can set the camera location, you can also set like the field of view and it gets fancy like that, but literally the camera is like set camera XYZ and you can have your camera basically move around the 3D environment. So everything we were watching like in class today was technically a 3D scene, it was just a camera that was set back and everything was getting drawn sort of on a virtual plane. But if you want to do some sort of crazy stuff like the little face like flies off into the distance in 3D, you totally could, and it makes it really, really easy. That's how they're drawing a lot of these sort of like connect data with point clouds in 3D, so it's literally just drawing dots in 3D and then if you move the camera you see this awesome parallax effect. Yes, can you control perspective versus the refraction? So one more quick thing I'll show you is, we go back to the IDE, what's really nice about this is if you go to file examples, they have this awesome examples all built in here, so you can see there's all these, there's a few of these, there's geometry and image processing, fractals and so on, they even have some great ones down here, I have some of these contributed libraries, like I can go into OpenCV and I can do like, let's see if there's a cool idea that I can run quickly, like optical flow, so let's open up optical flow, here's a little optical flow. It's only 34 lines long, and I'll just run this here. Oh, wait, I can't run that on Android now. I'll just run this, and this is going to now call. This is just using a video that's on disk, so it's doing a real-time optical flow on this image. Again, you can just get all this data. And again, this is like, if you look at actually how this is, he's opening up, here's how you open up a video, for example. And then you say, if you want to get the image, you just say, like, load image or video. And you have literally a compute optical flow function. So a lot of these things are really, really, really easy. You can do some super sophisticated stuff really quickly. The other thing, besides the examples menu, is if you go to tools, add tool, there's this whole install package kind of manager. And so you can go to libraries up here, and we can say, like, do you want to do something with sound? So I'll just type in sound, and it shows me all the different sound libraries that are available. In this case, I already have Minim installed, but there's other ways to do cool stuff. So that's how you basically just click it. You click what you want to do, you hit install, and then it's in the system. And a lot of them are available for Android as well. A bit more hiddenness. OK, so with that, I will let you take the next 18-ish minutes meeting your groups, figure out your next steps. And if you can't kind of come up with an idea and figure out your timetable, then you'll get crushed like come Sunday evening or Monday night. So use this time wisely to plan. Planning is the most important thing in group work. And it's when I'm playing the hottest new app on campus. You can come up and try my phone. Thank you.    ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... But I was in one of the email about the PhDs, and I was also thinking about that at the time. So, I really wasn't much involved. I remember she said you're the best place to start. Yeah, yeah. I knew that, like, for a time. But then suddenly, I didn't even know what to do about it. I'd reach out to him and see what I could help him with. So the reason I'm being objective is not because he's objective, but because he can tell you about pollution and all that kind of stuff. I can't lose him. In terms of research, he has his own research as well. We can talk about that separately. But it's like, he would be a good leader. As the advisor for the program, he would give you something that says that he's a master's degree or some PhD. And then when you're ready to offer something new, he will find it for you. ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... \",\n",
       " \"Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Okay. Let's get going. There's no pop quiz. Maybe Thursday. Depends on what side of the panel we wake up on. So relax until Thursday. Laptops away, please. So let's start with Bake Off 3 questions. Let me try to think. I've probably heard from about half, maybe two-thirds of the teams over the weekend with initial ideas. Initial ideas are pretty good. If your team has not emailed me at all, you're definitely behind. We have exactly one week left for Bake Off 3, which is not a lot of time. It's quite a complicated Bake Off. This is definitely one that you're going to need an iteration on, but now you are becoming masters of Bake Offs. You should be able to turn this thing around in two weeks total, but now it's kind of getting down to crunch time now that you've let off some steam from Carnival and stuff. So let me think about general points that I've solved. So number one, people have asked me a lot about, oh, can I highlight the next letter in green, or if they type something wrong, I highlight some letter in red, and the answer is no. You cannot bias. Sort of the golden rule for this Bake Off is you can't bias in any way. the target phrase. Okay, so if the thing is, Chris loves chocolate, you can like show, press C, then press H, then press I, and obviously that means that you're, you know what they're going to write. If your smartwatch knew what you were going to write, it would just write it for you. You wouldn't even need the keyboard. So you really have to presume that you don't know, and like this takeoff, I'm quite strict about it, just don't know what that target phrase is going to be. Now, you can use things in the English language to help you, right? So if I type in C-H-R-I, it's probably going to be an S, if I had to guess. It may not, there probably is other variations, right? But you can make predictions, but it shouldn't be anything based on the phrase set. It should be something general. So you can use, I linked to it in the Canvas posting, some big corpi, corpuses, corpora of English language phrases, like the top 10,000 words, the top million words, and I give you the frequency. So if you want to do something like that, you can, just make sure you're not biasing the English phrase set. Nor can you learn the frequencies out of the phrase set, that's also illegal. You can't just build a vocabulary of only the words in the phrase set, that's just a tiny, tiny phrase set for the purposes of this testing. And I may change that phrase set on the day of the takeoff as well. So yeah, don't make any of your buttons bigger or green or anything based on the phrase set, that's illegal. If you are going to do some sort of text correction, it can't be based on the phrase set. I will make a big note here, a lot of people think that the gateway to success on this takeoff is some sort of text prediction, crazy auto-complete thing, and they would be wrong. It is true that it helps you, but by far the biggest help is actually an efficient design for typing in characters. I would say if you look back at the previous times I've run this, I've now run this takeoff maybe four times, this may be the fifth time. I've done it for about two years now, I've added it. Originally I only had two bake-offs, and I like the bake-offs, so I added more bake-offs. Half of those teams had to do it. no text prediction at all. Half of the winning teams had just no text prediction. It was just a really clever text entry design. And I think about half had some sort of a prediction or correction or some sort of a language kind of model. So it doesn't mean that to win, you need some sort of crazy algorithm that's doing text prediction. In fact, you'll waste a lot of time on that, and it'll actually probably hurt you in the long run. So unless you think you're way ahead of the curve, I would not try to influence something complicated. I try to influence the best design that you can. And if you have time, add on the little kind of golden dust of something like a text prediction algorithm. But a lot of people put all their effort into the algorithm basket, and they lack on the design, and then it's still very clunky. Keep in mind also that you should be able to type in anything on this keyboard, or you're going to have errors. Like, Wi-Fi passwords are not going to necessarily be in an English dictionary. So if you limit your entire method to just the English dictionary, you're never going to be able to enter in Wi-Fi passwords. SquiffyDog72 or something like that, right? So your keyboard should be able to type in anything. You can still have auto-completion, but there should be a way to override that somehow, so you can just type in whatever. Cheese with five Es or something, whatever it may be. I don't know why I thought of that. OK, does that make sense? So any general questions about the bake-off? Yes? Is the final phrase that you used just lowercase? Yes, no punctuation, only lowercase. No numbers, that'll always be the case. Yeah, we're trying to keep it simple. Yeah? What was the target that you set for the team last week? If I remember correctly, when I pulled this up last week, is I think the best team was around 25 words per minute. And the worst team was like two words per minute or something. Something not good. So you have a week left. It's a team project. Divide the work efficiently, and I would say if it's by Thursday, Friday. and you have nothing working at all, like you start implementation this weekend, it's gonna be a trail of tears to the finish line. So I'd really encourage you to get something working. Even if it's the worst possible version, it's okay, that's what iteration is for. I'd really try to have something done by the end of the week. So that this weekend, you can start the iterative cycle, otherwise you'll just run out of time. No final questions? How many people have a working prototype right now? Some sort of version. Okay, like one team, two teams. How many people have what they think are some good ideas that they're ready to implement? Okay. How many people think they'll have a working version by Friday evening? A working version. Some sort of thing. Okay. For those of you not raising your hand, I would definitely put your heads together with your team and try to, try to find some time on account to get this done. Okay. So today, and sort of leading up to our debrief on this tiny keyboard, we'll have a whole lecture talking about kind of the history of keyboards and text entry in general, even before there were keyboards. And so we'll talk about that next week, but we're gonna start with some decidedly non-text input related things in our first of four lectures on input and output. Today we're gonna talk about physiological sensing, basically sensing kind of dimensions of the human body that are not necessarily explicit input channels. So we've already talked about a couple of things in this class, and we've mentioned them. You know, buttons and mechanical controls, sort of talked about that in lecture one, that sort of pre, we had mechanical and sort of early digital computing. We talked about the mouse and the graphic computer, sort of the radio mouse that never took off. We talked a little bit about trackballs and voice sticks, and a rolling ball trackball that was made to send it, 40, 45, light pens. We haven't really talked about pen-based computing very much. Keyboards, we're gonna talk about in a couple of lectures. Voices, we'll talk about a little bit. we'll talk about it a little bit more, and then gesture input as well. So, there's a couple of these we only sort of touched on very, on the very veneer of the model. But importantly, if you look at all these devices, well they are all basically devices, so you're kind of, the human hand or the voice from your mouth is isn't something that's digitizing it. So they inherently mediate the input. There's no reaching your hand directly onto the computer per se. They are digitizing some sort of output from the body and it's becoming input to the computer. That's important to realize. Even a touchscreen, fundamentally, that's your finger meeting an interface, and then that interface is basically sucking it in and passing it to the computer. But there's also ways to think about sensing the body itself directly, which is, it's related, but it's a bit more abstract. We're going to talk about a bunch of different topics just to give you a flavor for this, especially if people are interested in sort of doing sensing-related domains. So we're going to talk about eye and gaze tracking, pupilometry, affect, specifically face affect, voice analysis, which is really interesting, electrodermal efficacy, so basically electrons on your skin, bio-artificial heart rate, breast rate, muscle sensing, and finally, if we have time, we'll conclude with some brain-sensing technologies. So first off, let's think about assessing the cognitive state, or if you want to sense, for example, emotion. So what's interesting about emotion is that humans tend to give it discrete names, like you are happy, or you are sad, or you are elated, or you are disgruntled, whatever it may be. So we have language that's sort of pushed into these little holes, but actually emotion and cognitive state is fully continuous. So where does happiness end, where does sadness begin? It's really hard to say. We know that they exist, but it's very kind of a grid area. And so what scientists would say is that these kind of emotions are along a gradient, that it's a continuum, and they define these things as arousal. So that word has different meanings to different people, but arousal basically... sort of the strength of that emotion. So you can be, many things we call arousal, stress, excitement, pain, or fear, anticipation, being startled, or an emotional engagement, and often it may not be totally visible on the outside. So this is a very common and I think a much more kind of nuanced view of emotion, very cleverly done, and what you have here is you have this high valence to low valence, okay, which is basically kind of the positive, right, you know, like a valence, you often hear that term like electron, right, so it's about charge, and so you can have a positive valence or a negative kind of valence, and then you can have low arousal to high arousal, so if you're positive, but you're very kind of, when you're kind of in a positive mood, but you're in a very low arousal, you're being considered calm, and as we sort of move up in arousal and towards more positivity, we go relaxed, serene, content, happy, and then as we start going to kind of really high levels of arousal, but again we start to sort of diminish that positivity, we go happy, elated, excited, and alert, so alert may still be positive, alert doesn't necessarily have to have a negative connotation, but we know that sort of like maximum kind of arousal, and then it starts to move into sort of this negative aspect of tense and alert, again, there's sort of a bit of a connotation there, and I think that the words that they've selected are really, really good for this, so it's sort of, they've laid this out, and I think it's actually a reasonable kind of distribution of how we think about human accent or kind of emotion state, so it'd be interesting if we could have computers that can actually automatically sense your affective state, humans do this pretty well, you can look at a friend and see if they're happy or sad or thinking about something or lost in the moment, whatever it may be, humans are pretty good, but of course we're trying to extract that, like, an affect is inside of your brain, and so we want to try to pull it out, and so one of the kind of common ways to do this is with pupilometry, sometimes they do this in sort of like a, kind of even like lie detector test, but certainly you've seen it in movies, if you've ever seen like, I don't know, like, someone takes some drugs, or they're like, I don't know, they're like, getting chased by some sort of demon, you know, have that close-up eye shot where the pupils sort of like expand, right? It's almost like a sort of a meme in cinematography, right? And so basically what they found is that the pupil, the size of the pupil, which varies by, normally varies automatically by the intensity of light, like your eyes are doing this right now, but if you get it to put into an eye-arousal state, they'll actually increase in size. And we theorize this is basically to let in more light to improve your reaction time. So if you're about to go into a fight or you're about to go into some really crazy kind of stressful situation, your eyes will dilate. And it's a bit of a cue and you can't control it. It'd be awesome if someone could control their eye dilation in this class. I'm pretty sure no one has that ability. But computers can obviously use this and actually detect if you're basically going through. If you look at the delta, you kind of look at the derivative, the first derivative, and see if they're basically going through a high-stress event. And again, it doesn't necessarily tell you the valence. It could be that you're seeing your love for the first time all the way to another pop quiz. You don't know what the spectrum is, but you know that they're getting stressed out about it. Another really interesting one is voice analysis. This is actually very hard to get reliable literature on this because I think it's all mostly kind of classified stuff. But there's a couple academic papers on the topic. So it's either called voice stress analysis or voice risk analysis. And basically, they can take a recording of someone's voice and they'll pick up on small differences that can produce kind of a risk assessment. So obviously, this is of great interest to the CIA and NSA. They probably are doing this for all the calls in the United States. Seeing if people are basically exhibiting especially unusually risky behavior or their change in mood. And people have probably seen this a little bit. If you've ever watched a friend go up on stage that was nervous, you'll often hear their voice kind of starting to shake. How many people have seen someone's voice shake when they're up on stage? You'll often hear they're actually changing. And that's because you can't control it. It's like basically just really stressed out. Like I've definitely been super stressed out like that. and you'll actually hear that their voice starts to modulate or even sort of crack, and that's a very extreme example of when they're like in probably the scariest thing they've ever done in their life kind of moment. But you can do it, computers can do it even better than that. I'll tell you just a quick story. When I was at, after I finished my master's degree, I went to work at AT&T Labs, like Bell Labs, kind of research division, and in the basement of that office, which was in Murray Hill, New Jersey, I was commuting from New York City for finishing my undergrad. And in the basement was this gigantic computer. It was like a massive, the biggest supercomputer ever. It makes the supercomputer that's in the CIC look positively tiny. This is an enormous corporate campus, and the entire second basement level floor was just one gigantic computer. And I kept on asking them, because I was just interested in research, I was like, who's doing all the supercomputer research? Who's doing all the supercomputer research? And no one would ever give me a straight answer. They'd never be like, oh, that guy up that hall, he's doing some stuff. But if they did send me to someone, I'd ask, and he's like, oh, yeah, I can do it on my machine. I'm using some big computers, but I don't need that gigantic machine. And eventually someone told me, but I could tell that it wasn't totally the truth, is they said they were doing long-distance call monitoring. So this was like, I don't know, in the 2000s, I guess. Am I that old? I guess maybe I'm that old. Maybe early 2010s. Anyway. And so they were looking at people making calls all across the United States and looking at unusual patterns. Like, you know, every single weekend I call my mom, and then all of a sudden I'm calling, like, I don't know, Iran or something. I don't know. And they're just like, that's really, that's an outlier. Why would he be doing that? And so they were basically looking for patterns of abnormality. And people at the time, I guess, long-distance calls were very expensive. Again, this was sort of like pre-common of cell phone. And so they're like, you know, getting charged like $4 a minute, and it's like someone who's in your house that's like illegally calling on your phone. It was a very, like, thin story. It also didn't make any sense because, like, AT&T made a lot of money by people placing these long-distance calls. They were, like, cutting off their own revenue stream, trying to protect, like, long-distance fraud. It all seemed very weird. And that's because it was a total fabrication. It turned out, like three years after I left, someone found a similar facility in San Francisco. And it turned out there was these two data centers that were doing real-time voice transcription of every call in the United States. So you can Google this, there's a great New York Times expose. But they were in real-time, back in the 2000s sometime, like the 1800s, they were doing real-time transcription of every call being made on the entire telephone network in the United States. And of course, under the Obama administration, they basically did this sort of like warrantless wiretapping with a clamp down on it because it was just so egregious. But they were basically doing, even though the voice recognition technology at the time was not that good, it was maybe like 50% accurate on a telephone call, maybe 80 if you were really lucky, but it was good enough to get out unusual patterns of words, right? Like White House, visit, you know, whatever. I don't want to say anything, it's just like, there's no, you know, happiness, happiness. So, you know, so they were doing that. And again, like it all came to a head and now they've all kind of shut this down. But that was what they were doing like 10 years ago. So I'm really curious what they're able to do now with the advent of all these new learning technologies. And I'm pretty sure that there's a lot of really fun stuff that's happening in this kind of waste. And now it's the space. That's like, it would blow our mind if they were doing it. If they were doing it real time on every call they made, it would not surprise me in the slightest. And then it probably goes to a human, like a observer. Anyway. Okay. So another one that's very common in IPV is this running it to you, the space effect. You know, there's a very good reason why our faces are the way they are. Including things like, why do we have these little hairy tufts above our eyes? It's not to stop rainwater falling in your eyes. Like most scientists have debunked that theory. The reason we kept our eyebrows, even though we were fully basically hairy before, is that it's a really important expressive channel. But it was actually a selective advantage to be able to express our facial emotions using these weird little kind of hairy caterpillars that are above our eyes. And so this is basically meant to be an expressive tool that helps with face-to-face communication. And that was actually beneficial to early humans in communities. They're able to express their thoughts. And actually, it's really hard to hide when you're happy or when you're sad. I mean, unless you're like a professional poker player, it's really hard not to smile when you're actually happy. So there's a lot of kind of inbuilt things. And you'd think that that almost would be like a disadvantage, that if you're like fighting an opponent, or like you're playing a game of poker in like the 1500s or something, and you're smiling and you can't help yourself, you'd think that'd be a disadvantage. But actually, on that, it actually helps human communication to be able to have these tells. And even professional poker players still have these tells, even though they fight so hard to not have any sort of reveal anything about their emotional state. And of course, professional poker players are also highly tuned. They can look at your face, like look into your eyes, and they can actually kind of guess with a pretty high probability the strength of your hand, which is almost incredible, even though those people have like defensive countermeasures, you know, trying to stop them. So this way, looking at things like eye shape, you know, the eyebrows, the mouth shape, we can learn a lot about, you know, about not only, for example, their affect state, but also, you know, you can actually take those kind of raw things, those kind of raw biometric features and turn them into like fear and happiness. You can turn it into gender and age. I'm sure people have all played with these sort of technologies. But right now, they're sort of contained in these very simplified demos. But it's not hard to believe that in the near future, that smartphones just, you know, when you open up your camera, the camera will give you all the pixels for the image, but it'll also just automatically return, just by default, all of the people in the scene, all of the ages in the scene, all of, you know, are people's eyes open or closed? Are they happy or not? I know on my phone, it actually waits for people to smile before taking the photo. It also beautifies me because I have a Chinese phone, and so it always wants to give me these like really luscious lips and stuff. It's really hard to turn it off. But it is the benefits of having a Huawei phone. So anyway, so face-to-face recognition, I'm not going to go into too much detail, because I think everyone is fighting that a lot. So probably more unusual to you is electrodermal activity, which is, in the vernacular, often called galvanic skin response, but the more appropriate term is electrodermal activity. And it's still not totally understood. So what happens is that when you're in this high arousal state, again, independent of the veil, so you're in this sort of high-stress, high-arousal state, you will actually sweat a little bit. And you probably have noticed this in some times of your life. Obviously, you get goosebumps, which is kind of interesting. We're not going to talk about that, although that would be interesting for input. But you'll actually feel that your body gets almost a little clammy. And as I'm probably talking about this, I want you to be surprised if you're starting to think that you're getting sort of a little bit sweaty. And what happens is, under high stress, your pores will actually open up a little bit. They'll relax, and they'll actually secrete more sweat than typical. And because sweat is a salty kind of water, it conducts electricity really well. And so the net resistivity on your body goes down to become more conductive, less resistant. And so even though it's not fully understood, we just know that it happens. I mean, why is there an evolutionary advantage to having your pores open up? Maybe it's so that if you have to go into a battle, like your earth is ready to sweat, it will keep you cool, right? Maybe it's a thermal regulation component that doesn't do the whole point of sweat. But we know that it happens whenever there's basically sort of a big event. And measuring, you can take like a multimeter, or even with like an Arduino, you can measure the resistance of the skin pretty easily. And there's even smartwatches that do this now as well. And you'll get this sort of secondary measure of the emotional response of the user. Again, it's hard to know the valence, but you do know the arousal. Likewise, there's a lot of smartwatches that have heart rate now. So heart rate is obviously correlated with many things. Often, if you're sitting still, and your heart rate just goes up. So obviously, if you exercise, you go for a jog, your heart rate goes up, because it's basically trying to move around more oxygen. We all know that. But even if you're just like watching a movie, it's like a scary movie, your heart rate. we'll start the race of something that's really crazy going on, despite the lack of any physical exercise. And it's because basically your body is priming itself for sort of a flight-or-flight instinct, right? So if you're in this kind of high-stress thing, your body wants to super oxygenate itself ahead of time so that if you have to like go and go fist fight, you're sort of ready to go, right? So this is sort of very primal response to basically kind of pre-prime your system ready to go. So if you can correlate, for example, accelerometer information with heart rate, you can know this is a heart rate response that's due to elevated levels of sort of attention or stress and not because of just the exercise. Obviously there's going to be some false positives there, like you could be like on a lift or something, I don't know, something that your hand's not moving so much, and then you get a false positive. But nonetheless, heart rate's a really interesting one for determining act. Here's another interesting one. Let's see if the sound works here. So let's have to turn these lights off. So this is color amplification. So what you're seeing on this guy is he's tinting green and red. Let me actually explain what's going on here. So when your heart pumps, right, you actually expand a little bit. Sort of like if you have like one of those like balloons, like those kind of like whatever like balloons that they make like dogs into, if you like squeeze the balloon, right, the air pops out somewhere else. I don't know, I'm sure I've played with a balloon before. Your heart's basically in the center. When your heart contracts, your capillaries, your whole basically circulatory system, expands a little bit because that volume of blood has to go somewhere. And what happens on the capillaries into the surface of your skin is that you actually tint red very, very slightly. Because you're actually, basically the red is moving, your blood vessels are actually moving closer to your skin. So you tint red slightly. Now if you do this with a webcam, like I just took out my phone and I filmed someone's face in a very kind of steady fashion, what you'd find is that the red value, like the RGB red value, it'd be indistinguishable to the human eye. Like red might be like... 2, 2, 1. And when you pump your blood it goes to 2, 2, 2. And when your heart opens back up it goes to 2, 2, 0. Like we're talking about a percentage change of like a fraction of a percent. You can't see it. But what these researchers did, this is an MIT paper, is they looked at when you're told you're going to head very still. And then you look at small color variances, and you would use global ones. You find the mean of that pixel is 2, 2, 1. But now you're going to have 2, 2, 2 to like 2, 5, 5. And 2, 2, 0 to like 1, 60, let's say. So you're stretching that red variance out. Then you actually get this effect. And what this is correlated with is this is actually the person's blood pumping. Awesome. It's actually quite an old paper, but it's a good one. I'm going to get the video to play again. So again, this is just amplifying that color channel. And it's a very pronounced effect. This is just the capillaries expanding on this guy's face. You can imagine if you plop this over, it's pretty easy to recover that pulse from this other webcam. It's hard to see in the projection here. So here's a little baby. In this game, they hooked it up to a EKG machine, so they're actually able to correlate. And it does correlate pretty well. You can see it's really high, 150 beats per minute. So it works remarkably well. And again, they said that it was about 100% color magnification. That means if your RGB is changing by 1, you're going to change it by 100. So if the value is 220, you're going to make it 120, which is why the person's sort of tinting green. It's a very cool little effect. And you can do this with a webcam and actually be able to... It has a decent quality. It wouldn't be hard to do with any old webcam, but if you hold... still enough you probably could actually get this to work, even with just a smartphone camera or a webcam. So you can slide this into something like Processing, which has a good video library, and do this. They're doing this like a little more fancy math-wise, but you can even just do this with a simple app. If you just get like an eyedropper, and you just click it many, many times like to plot that value like in Excel, you'll see your heartbeat. So respiratory rate's another obvious one that you can imagine computers could digitize. So, you know, the hard part is actually measuring your heartbeat without instrumenting your chest is actually still quite a challenge. So if you look at robust examples, they're almost always these sort of chest drafts. You'll find that 600 cats use an accelerometer or some sort of like a strain gauge or some sort of like stretch sensor. And that's how like if you have a professional runner that has this monitoring their respiration rate or like an Olympic athlete, they almost always have something on their chest. We haven't found a really good way to like sense your respiration, like from your wrist, for example. There are some examples of some wireless technologies that are doing things like Doppler. So this is actually a really cool paper from Kahn. But basically, it was emitting a signal and it was looking at the Doppler shift off of people's chest cavities. And what you're seeing is as you kind of inhale and exhale, there's actually a little bit of a Doppler shift on the chest cavity. And if you zoom in to the little peaks there, these little tiny peaks are actually your heartbeat. You know, an average human only breathes like five, you know, five times, what, 0.2 hertz? Half a breath per second, does that make sense? That seems too slow. I forget exactly the rate. But basically, your heart beats that much faster. And they're doing this just with wireless signals off. Sometimes they're even hacking Wi-Fi base stations to be able to read this signature off you, which is pretty interesting. So you can imagine like the Wi-Fi chip maybe in your phone, in your pocket, or in your watch, being able to do this sort of Doppler sensing on your chest. Now, these are all, so those are all examples of basically sort of things you don't necessarily control, but you can sort of, you know, kind of use as input into a computer. But it's sort of this. because it's not implicit, it's very much, sorry, it's not a very explicit interaction, it's very implicit, like you don't really control your heart rate, you don't really control your pupil size, but we can think about areas of the body that you could use explicitly as an input device. So here is one question. I'm not gonna give you five minutes to do this now. If you imagine that computers could understand where you're looking, so we're using your eyes as an explicit input mechanism, right? So you can move your eyes anywhere, it's not like breathing, you can sort of put your eyes anywhere you want. What kind of applications do you think would be useful? Like if your smartwatch or your smartphone or your laptop knew where you were looking, what could you do? So I want you to just brainstorm in your groups, groups of roughly four, you have four or five minutes to come up with uses, write them down because we'll turn them in at the end of class. Get a sheet of paper, brainstorm. Okay, I'm gonna give you two minutes to study what your eyes are doing. I'm gonna give you two minutes to do that. So if you can think of what you're doing, then we're gonna bring it back to the same screen. So I'm gonna give you five minutes to try and come up with an idea, and then we're gonna take a look at what you're doing, and then we're gonna have two weeks or so to do this. So if you have five minutes to do it, we'll see how you do it. All right, thank you. So remember to put your Android IDs on this piece of paper. Okay, one more minute, and I want to hear some ideas. Okay, let's hear some ideas on gaze tracking. If computers knew where we were looking, what cool things could they do? Who has an interesting idea? Sure. I hope that doesn't come true, but yes, that's a good example. So if they're locked in, or if they can move their eyes, then you can basically use that as 2D, like it looks like a person. Yeah, absolutely. What else? Okay, so Samsung phones actually already had this feature built in by default. What else? So when you're playing first-person shooter games, you can look at different angles on the screen, look at different portions, and you can change that. That helps in the navigation. Or maybe you could just move the targeting, like the reticule, to where you're looking. It'd be kind of weird if you changed your navigation, because I can walk this way, but I can look around. It would be kind of weird if I only walked in the direction that my eyes were, but yeah. Yeah? I'm trying to find the application where either you're shopping, you just want a place where you can combine, or if you're like, you have a business of that sort, you know. You mean if I'm working on an object, I can target it, yeah. Or if you're like a fish, or if you're a beetle. Or something with like, if you're looking at flowers, you can kind of know what it is. Yeah. Okay, that's just sort of like, yeah. Kind of gaze-based kind of information, yeah? I remember it's the first person to your thing, the new Borderlands 2 in VR does that, and it feels super unnatural. And it's followed by your rotating? Yeah, it's like, you shoot wherever you're looking, and it feels very strange. In a bad way. Okay. Good to know. Killed it off my list. Off my list. I don't know if it's something you're looking at, or if it's something you're not looking at, or like, or if it's something you're looking at. Or if you could just play off entirely when you're not looking at it? Yeah, yeah. So if you look at it in order, but you should look at it in order, because I think it could expand to show more of that. Okay, so if you're gazing at it for a while, and it's like, let me give you all the details, yeah? That'd be cool. And if I maybe just glance at it and look away, then maybe it's like, I'll remind you when it goes back in the queue. Yeah? I wanted to spot out the idea of like, you can have meetings, so if you're in a concert, you can have meetings. Yeah. Yeah. Yeah, I'm already running that algorithm. I've been keeping an eye on a couple of them. Yeah? I'm not really sure what the security issue is. That was a joke. I'm not looking at it. I don't seem obedient. Just to be clear. Call Fox. I'm not really sure what the security issue is. I'm not really sure what the security issue is. I'm not really sure what the security issue is. Hmm. Interesting. Yeah? Yeah? Yeah? Maybe. So those are all interesting. All good applications. So EyeGaze has many, many uses. I actually think it's going to be sort of the future of it. of computing. I really think that in 10 or 20 years, the mouse will be gone, and we'll just use data. I think data is so powerful. Here's a nice little quote that I pulled out. The eye has a lot of communicative power. Eye contact and data collection are very central and important cues in human communication. For example, in regulating interaction and turn-taking, establishing social-emotional connections, or indicating the target of a visual interest. The eye is also essentially a mirror to the soul or window into the brain. Data is a behavioral reflex cognitive processes that give hints to our thinking and intentions. We often look at things before acting on them. This is like a tour de force. When people look upwards or look at the sky, that's often denoted with increased cognitive activity. If you're talking in a conversation, you can tell when people are lost in thought and like to see their gaze behavior. They'll be like, hey, are you paying attention to any kind of thing? Obviously, it's very much indicating a target of interest. If I'm looking at Chrome in my doc and I say, open, or I say, minimize, or whatever, it should be able to respond to that, and I really think that's going to be coming pretty soon. So eyes not only give you that target, but also really give you that kind of window into cognitive processes, which is super, super powerful. Lots and lots of uses. So, unfortunately, it's sort of unwieldy to get gaze right now. The best ones are the head-mounted ones that have your camera really looking straight at the eye, but obviously no one's going to like, oh, let me use Mac OS 11 and put this on. It's so goofy. If you're in a VR headset, I think VR is almost certainly all headsets will ship with gaze at some point. The newest ones are, like I think HoloLens 2 does. Obviously, Vive has a version of theirs that has a gaze tracker inside, and the Magic Leap has to, because that's how it's doing the multi-planal focus. It's looking at the convergence of the eye to do some kind of stereo. So I think that'll be, I guess, given that you're already having to work something in your head, why not just look at the eyes simultaneously? There are lots of ones that kind of come in this bar format that you might put above a monitor. They tend to sort of look like this, where they have one or two cameras. So here's an eye-censor camera, and then there's all these illusions, these little LEDs that are around this camera. And there's a second camera here that's off-centered. And I know the geometry of these things. Here's a big illuminator to illuminate the whole face. And how pretty much all of these devices work is as follows. Is you have the pupil. Normally it's dark, right? But you have the pupil of the human eye. And because you have that infrared illuminance, you have the infrared camera and the infrared LED, it's because you know where the infrared LED is with respect to the camera. You get this little kind of glint that's off-axis. You get this little glint on the eye. And so if you have, let's say, your camera right here, and you have a camera in the middle, and it's surrounded by infrared LEDs, like this camera is. So it's a camera here. It's got six LEDs around it. But those kind of blur together. And what you'll see is something like this, with the infrared glint right on it, because you're basically looking down through the barrel of that light. But if you imagine that you look down, so my pupil's looking down here, but that glint will still be on my eye, just a little glint above. And basically, what you have to do is, here's an actual shot of this. So here's the glint, and here's some of the people. You have to find two circles to get this algorithm to work. You find the big black circle, which is roughly here. Then you find the smaller white circle, which is here. You find the middle of those two. And the distance they are apart basically tells you the vector of the eye. It's actually a pretty straightforward algorithm. You can implement this like an OpenCV with a Hough transform. Pretty straightforward. So here's actually an example. Find the center there. Find the center there. Find the center of two circles, black and white circles. And then you can get that different value that gives you this. And again, there are more sophisticated things you can do. This used to be this state of the art. Now, it wouldn't be a surprise if you're starting to use more convolutional neural networks to ingest this whole image and produce estimates because there's other strengths and weaknesses. Something that people sort of mentioned is sort of related to advertising. The most common use of ECI for gaze tracking was to look at where people look on screens. So gaze trackers have been around probably for like 30 years. There's some other sites that go back like that, I think to the 40s, and they're doing this on film, no computers involved. And you know, you want to look around. So here it's like, you know, there's a number here, so a person looked here, and then they looked here, and then they looked here, and then they looked here, right? So you can see, like, over time where they're looking on this page. You can also convert this sort of vectorized information into just a key map of where they're basically spending all their day's time. And it can help you optimize things like the placement of ads, and also sort of the placement of information. But you can imagine also reading this to see if people are getting confused. Like, if you see, if you're running this in Microsoft Word, and they're looking through all the headings, and they're glancing up and down trying to find something, you can ingest that behavior and basically make a prediction that, hmm, maybe that there's something that I can help with. Maybe looking for something, or maybe I can suggest that they're looking for help. So you can imagine kind of using these key maps or sort of gaze pathways to be more proactive and not just trying to sell us like more junk. Okay, so very much related to gaze is this classic HCI problem called the Midas Touch Problem. Okay. So can everyone remind us of the story of Midas? He was this dude, and I think he touched her to gold, including, like, his family and food and maybe his face. I don't know, but everything. I don't remember that part of the story, but yes. So it's, you know, I guess it's biblical. Midas is like a king. He, like, loves gold, and so he gets this one wish. He said, I want everything I touch to turn to gold, which is, like, awesome. Again, gold necklace, gold jewel. And then, of course, he gets, like, hungry, and he tries to pick up his, like, apple so he can turn to gold. And then, you know, then he, like, hugs his wife and, like, turns to gold, and he's like, this is actually terrible. Like, I'm not going to enjoy this. So that's the story. And that was called Midas's Touch. And there's this notion of Midas's Touch in HCI. So how do you think it relates to HCI and Dave's input specifically? Anyone? New? Yeah? Maybe it's like not everything a person looks at is necessarily something they want to look at. So that's definitely true. And how would that be a problem for systems that use data? Because it's hard to recommend when they want to enter a link versus if they're just looking for it. Yeah, so I mean it's everything you look at, you can't assume is going to be interactive. Like let's say I did have your application where it shows me the competing price on Amazon. So everywhere I look, I'm looking at your shirt, I'm looking at your laptop, the eraser, my letterhead just like adds everywhere. You can get it for $2.99 on Amazon. You know? Like it'd be totally insane. So how do you solve this problem? How do you have it so that it's more explicit? It isn't just constant. Yep? You can have a link where it's like there's a whole lot of screen. Okay. And you're almost like asking for the system to guide you. Yeah, so you do the link? Yeah? Yeah, that's definitely one possible way. What are some other options? How about wearables and stuff like the button or something? Yeah, so you could have some other kind of auxiliary input mechanism. Yeah? You could have speech. Speech? You could be like, tell me the price of this. Okay. Yep, absolutely. What else? Those are all good examples. Yeah? Jackie's FB. What? Jackie's FB is a great session. Okay. It's like every time I blink or smile or something, this brings up ads. Okay. Anything else? Those are all good ones. So yeah, I think you covered the big ones. So the most common one is blink. So you almost always cover with another implementality. Blink and dwell. So we don't talk about dwell. Blink is very common. And dwell, where you just look at something long enough that basically it infers it to get action. It's a pretty common dwell. But it's actually surprisingly unnatural. You can try this right now. Try to look between the O here and the O over there. Just go back and forth with your eyes. Don't move your head. Just go left, right, left, right. It's harder than people in the front row. tiring actually. So eyes are, you know, even though we can override them and kind of control them, it's actually quite exhausting if you have to use that, for example, to type on a keyboard. If I gave you a gaze keyboard and I said I wanted you to type whatever, design human-centered systems, right, it would be actually really the worst. It would be like, look, dwell, take out the D, you know, it'd be actually really, really tiring. And people have said this in a lot of studies that have looked at basically the failings of gaze. And so, yes, we want to do something like gestures, like pulling an airlobe or something, do voice or some sort of a button, those are all good auxiliary controls. So you tend to need a second dimension to make this work. Now people have looked at sort of how you might be able to use eye-engaging in a really important channel. Yeah? Also, try to blink at this point. Blinking, too. Try just blinking, like, 100. Use blink for the next two minutes. I want you to know how it is. Like, it's fine at the beginning, and then, like, it's just like, you get really complicated. So you wouldn't want to enter 200 letters, which is only like 50 words or something. Very, very tiring. So you can have, on gaze tracking, you can have sort of overt, intentional stuff, like, I'm looking at the light switch. Then you can have sort of attentive user interface that are eye-aware, but don't necessarily take it as command and control. They know that I'm looking at the help dialogue, and they're going to, or that I'm performing some sort of action, and they're going to basically proactively assist. So it's sort of, it's not quite direct input, but it's like secondary input. Then there's kind of user modeling and identity recognition, which is, I understand now that the user is confused. I'm actually trying to model their confusion, or their cognitive load, or if they're writing in Microsoft Word and banging out, like, the next Game of Thrones book, and it's amazing, and we want it to happen, then, you know, we're just going to, like, prune the whole interface down, hold all notifications, because we know they're, like, in this writing flow. And then all the way to sort of passive monochromatic, so basically diagnostic applications. Where are people looking so we can optimize? with a Google home page experience. This is unintentional, almost pervert, all the way to a very intentional gaze behavior. So people have studied this. And there's a link to the paper if you want to read more. So here are some uses that I put together. It's an automatic scrolling. So you get to the end of the page. And on that, you scroll. So you read on your Kindle, and it goes to the next page. We're going to talk about magic pointing, which is that gaze-assisted kind of Fitts' law thing, where if I'm looking at Chrome, and I'm moving my little triangle towards Chrome, why do I need the triangle? Why can't I just look at Chrome and just say, open, or snap my fingers, or something? And it's a great way to use Fitts' law. And I think it's actually a great way to get rid of the cursor entirely. We already talked about scrolling on something that brings in text and information. Looking at an object and activating it, like saying to it, land on. But again, because I'm looking, it doesn't turn everything on. It isn't that my coffee machine starts making coffee, and my microwave turns on. The only thing I'm looking on turns off. Devices can be in standby while the power machine is looking at them. It doesn't tend to confusion. Virtual agent conversations, having good turn-taking. It's really funny how good humans are at reading IQs for turn-taking. You know, in a conversation, when you can jump in, or when you can pick up the conversation. Robots are sort of terrible at this. There's a really nice study about Gates. It was done here at CMU by a guy named Bill Gates, PhD student. And he had a robot have these three behaviors. He had one that just stared unblinkingly at people. And he read them a story, like a fairy tale. He had one that sort of just randomly turned, but it was sort of nonsensical. It just sort of looked in various places around the room. And then the third condition was it mimicked human behavior, where it would look at someone for a little bit. Then it sort of might turn around and look at the sky. It didn't do any gestures. It looked at the sky a little bit. Then it'd kind of come back and look at the human. But it was much more human-like. And then at the end of these three conditions, different participants went into the different conditions. And they said, I've been given a quiz about the fairy tale. It's like, where did Billy go after the treehouse? And it's like, to the supermarket, right? And they basically looked at their recalls. of information, like what was Billy's friend's name? Emily. And it turned out that the robot that read with a human-like gaze behavior, the recall of facts was dramatically higher, statistically significantly so. And that's because you would attend to the robot in a much more kind of human-like manner. So you actually listen to what they're saying. When it's like off just talking, and you have no idea what this robot is doing, you tend to basically just phase out and sort of ignore what it's saying. They didn't tell them they were gonna test them at the end. That was sort of part of the manipulation. So, interesting things like that. And we know also, and maybe I didn't mention it earlier in the semester, is that there's also a really clever thing that humans do called negotiated approach. That if you see a friend, like you're on the cup, you're walking towards class, and you see your friend walking towards you like 100 feet away, you don't just stare at them and just approach them like, oh, hey, you just met your kid, right? It's like, it's like as you freak them out, okay? So what humans normally do is you make eye contact, and then you sort of like check your watch, you kind of look at your phone, you pretend that the trees are so pretty. And then when you're like 10, 15 feet away, is you look again. And then what happens is you can tell, normally it's then in that instant you know if they're gonna stop and talk to you, or you can reach for the facial expression like motor right past you, right? And humans do this like automatically, like you're never really been trained to do it, but we're so good at it. And so you know, it's like, okay, how's it going, like, off to class? And so you're like, hey, did you enjoy carnival, blah, blah, blah. And that's called negotiated approach. And again, robots are terrible at this. Like their algorithm, because they're programmed by a bunch of computer scientists, okay? Is, you know, they're like, oh, we can see this human moving like this. And we want to plot like an intercept course that's like as efficient as possible so we can get there, right? So they like figure out the trajectory. And then we just beeline it to them. And of course, they got the little like, you know, googly eyes on them, go straight at you. It's like you're on tech cut, and you see this robot coming at you. It's like vectoring, like adjusting course like intercept, like a missile, and it freaks people out. And so what you want to do is use kind of these human behaviors. Like the funny thing about robotics is the hard thing is like building this multi-million dollar robot. And the easy stuff, like having it blink, or having it gaze, like that's. It's like four lines of code, and they get that wrong. So this is why HCI is really interesting, is that it brings together just that little spice of human psychology and perceptual principles, and you can make these things so much better. People's ratings go from worst thing I've ever had, like I felt like I was getting attacked by this robot, to like, it's my best friend, and I love it, and it's literally like 20 lines of code difference in a bit of HCI knowledge. So it's a good little example. They've actually mostly, Gates used to have a lot more robots in it, but people were getting, I think, annoyed by them, because it was always bumping into people in the elevator, like it was very aggressive about getting in the elevator. It was running over people's toes and stuff, and no manners, and people were like, it's so rude, and you know, they're anthropomorphizing this, and it's just like, it's code is like, get into the elevator, because you have to deliver some envelope to the dean, so that's its primary directive. It's like some Terminator kind of stuff, you know? And everyone else is getting their feet wet over this, so it's really funny. Okay, moving on. So muscle computer interfaces, also another great way to get input from the user. They used to be a lot more popular a couple years ago, and you have devices like the Maya, which I'll show you in a second. They have these kind of arm bands, and what they're doing is, because you send electrical impulses from your brain to contract your muscles, is the theory behind EMG, or electromyography, is you want to sense those nerve firings, and it's very weak kind of voltages, but then you can sense that the muscles are being activated. And so you get little signals like this, EMG, and you kind of pop them into electrodes like this, on the skin, and you can detect if the muscles are being activated. The problem is, is it's very person-specific. All of our muscles are a bit different. How you place them on the skin is also gonna vary the signal tremendously, often because it's medical. Electrons have a special type of adhesive that's conductive, because the signals are very, very weak. Like, you're not a high-voltage object. There's very low current, very low voltage, and you're having to sense it through a gigantic insulator. It's like sensing electricity through a wire, but the wire had like, you know, one centimeter coating on it. weak signals. And so you need all this signal you can get, so they tend to use these sort of like special, what they call wet electrodes. You can get dry electrodes, but the signal is even worse. But you can do cool things with this. You can imagine sort of not having to grab a mouse, but like, your mouse is your arm, like, automatically. So here's a nice example of this. Traditional computer interfaces employ physical transducers, such as keyboards and mice, for input. These physical controllers allow people to use the dexterity of our fingers and hands to control a variety of applications, from word processors to airplanes. However, as computing continues to expand off the desktop, we face many situations where we need to interact with technology without a physical artifact, because a physical controller is not accessible, or because our hands are already busy. The muscle computer interfaces we present here allow a user to interact with the computer using hand and finger-based input without holding physical input devices. We use forearm electromyography, or EMG, to directly decode muscle signals from the surface of the skin. To do this, we first build a gesture recognizer by collecting muscle data through a narrow band of sensors on the upper forearm, while a person performs a known set of gestures. This gesture recognizer can be used with the band of sensors in real-time to detect actions performed by the hands and fingers. Here we show our system recognizing strumming and fretting, allowing someone to play the Guitar Hero video game with an air guitar-like experience. And here comes the reveal, which is, it's not incredibly practical. In our case, we present the results of a laboratory study. So, the technology has advanced since that paper came out that was from maybe a startup, Mayo, or Ethoprime, I'm not sure if they're in any of that as well. And everybody says, I actually have a couple of these. upstairs. Has anyone played one of these? One play with Amaya? They released it for like $199. It may have been like a Kickstarter campaign. And you have to basically calibrate every single time you wear jeans. They make you do this gesture like this that basically maximizes certain muscles so they can calibrate. But it never really took off. They stopped selling it last year because the sales were very lax. They didn't really ever find the killer. Why would you want to control using your arm? It's very corpse. It wasn't like they were getting finer details on your fingertips like I could control a mouse with the tip of my finger. That'd be quite interesting. It was really for very coarse actions like controlling a drone. But there's probably better ways to do that. So because the signal is weak it tends to be less reliable and only works for really kind of crazy gestures. And that was a big limiting factor. People have also looked at it for the face. So this is the look you want to pull off. At CMU you could probably get away with this. But most people are not going to want to wear this around on a daily basis so that their computer can know what their emotion is. Very related is this thing called mechanobiogram. Very much like EMG, but it actually turns out that muscles, when they contract there is an electrical activity, but actually muscles are actually keeping contact by lots of little tiny firings. Again, I'm not a biologist, but basically you're using all this ATP and it turns out there's lots of contractions. So they tend to kind of vibrate at a very low level. If you know if you're doing weights or something or pull-ups, as you kind of run out of energy you start to kind of shake as basically the radio firing goes down. You can pick those up with special microphones and it's just a different way to get muscle activity. Some research that my own lab has done, I'm going to try to show you some projects that I've done, is we took this technique from the medical literature called Electrical Encased Tomography. And this works by basically, it sort of moves like galvanic skin responses. You put electrodes like around, in this case they're around this person's leg. So you put these electrodes around the whole body, like let's say eight. is you measure the resistance, essentially, or in this case impedance, so AC instead of DC electricity. But you measure the resistance between all pairs, right? You can measure, this pair, this pair, this pair, this pair, this pair, and I can say, I want this pair, this pair, and in the end, you get basically sort of two choose, eight choose two combinations of pathways. And so you get this sort of like mesh of measurements, and then you can actually reconstruct that using something called a radon transform into a 2D image. So that's how they actually get the person's lungs. They can sort of see that one lung has more capacity, or one maybe is filled with liquid, and they're just doing it noninvasively from sensing from the outside. So we took this, this machine cost like half a million dollars, and so I challenged one of my PhD students, I said, can you make an EIT machine that costs like $5, or $50 as a prototype, and they did. So I'll show you this video to give you a sense. Tomo is a hand gesture sensing technology based on electrical impedance. This technique is used in many industrial and medical applications. However, these devices are large, expensive, and generally cumbersome to wear. Our version is low cost, can be small and low power, and is noninvasive, allowing for integration into worn consumer electronics, such as smartwatches and armbands. The simplest electrical impedance tomography setup involves one emitter and one receiver. The emitter outputs a high frequency AC signal, while the receiver captures the waveform. This data can be used to calculate the impedance between the two electrodes. We can add more receivers, measuring the impedance along different paths through the object. We can also multiplex the transmitters and receivers to create many combinations of paths. With enough data, we can compute a 2D image of the armband. interior impedance distribution. The more electrodes, the better the resolution. Here you can see a basic test of our system using a bath of water. If we dip the cylinder made of plastic, you can see a red circle appear on the visualization, denoting the location of where the change is occurring. Alternatively, metal, which is conductive, produces a low impedance signal, visualized here in blue. We built a prototype sensor band with 8 electrodes, which transmits data over Bluetooth at 10 frames per second. This band can be worn on the wrist or arm. Here you can see a user performing different gestures. Note how the raw pairwise data and the reconstructed image change their response to the muscle. From this data, we derive a series of features, which are passed through a machine-learned classifier, which performs line recognition. You have to be quite explicit with these gestures. It's still not quite sensitive enough to get fine-grained motion. Our approach can be integrated into worn devices. The cool thing about this technology is that you get a live cross-section. It's almost like you're looking inside of someone's arm from this direction. You get a live cross-section of what's happening inside their anatomy, but you can just do it with a worn smartwatch. You don't have to do any wet electrodes or anything. It's just little copper patches that you can imagine integrated into the inside of a smartwatch. Another kind of topic that I've been researching for a long time is bioacoustics. It turns out the human body is incredible at transmitting sound. This is why you can take a stethoscope, which has no batteries, and your doctor can hear your heart beating or can listen to your intestines percolating. It does this with this Victorian-era technology. there's a fluid-filled sack and it turns out that water and bones conduct sounds incredibly well. Much, much better than water and also a much higher speed of sound as well. And so you can have a microphone. This is an early system. This is what kind of inspired my later research. Is they add microphones to the carpet that's actually kind of sandwiched around the wrist and then they perform these different acts of patting and rubbing and flicking. And you can see that the acoustics actually look different. And so I picked this research straight up a couple years ago or a while ago now. So this is me tapping on my awesome computer science muscular arm. And if I slow it down, I'm very heavily lifted. If I slow this down, you can see what happens. You get these ripples that are propagating out. And this is not just my arm. This would work for all your arms as well. I'm going to have to put on like 30 pounds to get this to work. This effect happens actually two different ways. One is you get these surface ripples from the skin, but you also get sounds that move through the body. These so-called kind of compressive waves as opposed to these transverse waves. And so what we did was a super, super happy demo. We took all these little tiny kind of like tooting forks, essentially. These are piezo tooting forks. And in order to modify them to different frequencies, they literally hop on different sized ones onto the air. It's like a super, super rudimentary project, but it actually works. Imagine just like having all these different tooting forks, kind of two pods of five, tuned to very particular frequencies. Like I got a weight, and then I would adjust the hop loop to get it to be tuned to a very particular frequency, like 77 hertz. Here's a list of frequencies that I chose. And because tooting forks are very resonant, they can actually take very small amounts of sound in that particular frequency, and they amplify it. This is why you can have the wine glass, and if you just get the perfect right frequency, it'll like shatter. It's because they can act resonant. We'll talk about this maybe next lecture. But next, things that are resonant can tend to amplify. So here's actually the signal from those 10 transducers. You can see if I tap my arm versus my palm, you can see with your naked eye that the signals are very different. And that's because it's sort of like you're like a gigantic xylophone, where you tap on your arm, internally sounds quite different. And so we can pass that to machine learning. So in this case, this is me training it. So it's segmenting the impulses. In this case, the sensors are up here on my upper arm. And I'm tapping each of my fingers five times. And then after that, we can train it. And now I can just touch any one of my fingers as buttons. So I've basically turned my hand bio-acoustically like a little controller. It also relates to bio-acoustics. So those are bone conduction microphones. Your ears are attached to your head. And so you can actually run the sound through your bone instead of having to put something in your ear canal. And again, it's because bone, but also liquid. Bone is even better. It's a really great conductor of sound. Much more truthful than air. So obviously, we had to make a game. So here's Tetris. You get the idea. We can play Tetris. I don't know why Keynote has to go there. And then of course, as I showed you earlier this semester, it's the same system we use to do projected buttons on the body as well. Okay, so let's talk about brain-computer interfaces. This is sort of getting into the frontier here of physiological input. So the most common approach by far is EEG. This is recording electrical activity in the brain. So again, the same way that your muscles are basically firing off little electrical signals, your brain is firing off tons of electrical signals in everything. up those little minute firings, and of course, because our brain is special, there's different parts of our brain for different things, we can try to actually understand what maybe you're thinking, or at least what you're cognitive state is, thinking you're thinking. So here are just some examples of the different kind of waves that you see, so here's like deep sleep, here's restless sleep, here's drowsy, here's relaxed, here's excited, there's all this beta waves, alpha waves, and so on, and so you can actually see the signals quite distinct and quite different based on your current state. Now typically you have to wear these sort of crazy caps, what they do is they make you wear this kind of blue thing, and then they squirt a gel, basically a conductive gel into the little hole, and they basically add on a little snap, these are done in glass, and then you have this like, you know, hundred points on your head. But again, they have to use that gel because it's so hard, they know that the electrical voltage in your brain is, I don't even know, millivolts, maybe even microvolts, and it gets through, again, like a centimeter or more of bone and flesh and hair, and it's just very, very difficult to get reliable signals. There are attempts to reduce the cost, so this is actually like a consumer-oriented one, so you kind of wear this weird-looking thing, and again, people, basically their application, again, is like drone control, like up, down, left, right, it's very hard to control, it's actually very exhausting. What you find with these interfaces that try to do brain control is they make you map very strong emotions with it, so if you want to control a drone and you want to go to the left, it's like that horrible experience you had in high school, and to go right is like your first kiss, and to go up is like warm chocolate chip cookies on a spring day, you know, it's like these really strong things. And of course, in order to control it, you're like thinking about this and going to this, and it's actually exhausting, it's very slow. And that's if you want to control a drone like up, down, left, right, if you want it to do anything else, it's like impossible. Often what you see it to do is for things like that text input, it's really like a grid, you can go left and right, or even just sometimes right and select, you go like right, right, right, right, right, right. and then you start back up again, and you select, and you get two. And two is a lot easier. And toggling between like eight really evocative memories, it's just emotionally draining. But it needs that very strong signal in order to pick up something real fast. Here's another technology. It's a little bit different, and it works in a very cool way. So it's functional near-infrared spectroscopy. It's an autobase. You don't have to wear the gel. It's actually very cool how this works. What happens is they have a big infrared emitter, a big infrared LED essentially, and they shine the light into your body, into your head in this case. I'm sure we've all done, like, you take a flashlight and you sort of put it behind your hand, and you can see that you glow red. Right? How many people have ever done that with a light? Where have you people been? Are you humans? Are there more aliens in here than I thought? What it would do is that if you had a sensor in that flashlight, a certain amount of light would actually reflect back to your skin, and it would be tinted that characteristic red color. Now, what happens in all parts of your body that use oxygen is that your hemoglobin actually tints differently if it's oxygenated versus not oxygenated. So I know red, a lot of it's exposed to the atmosphere. It's that very dark red because it's saturated. It's in like an oxidized form, I guess, or it has oxygen in that hemoglobin, and so it's that red. But if it's deoxygenated, and it's transporting away other molecules on the carbon dioxide and stuff like that, it has a slightly different color. So what happens in this is that they have a grid or an array of infrared LEDs and very sensitive photoreceivers, and they're looking at the tinting of that red to know if that part of your brain is in high active mode, and therefore consuming more oxygen than other parts. So it's basically like it's really doing cognitive state as a proxy of how much oxygen that area of your brain is using, but that actually tends to correlate extremely well. And what's nice about this is you don't need any of that jet. You just need a direct contact on the skin. This is actually very similar to how the apple... watch is doing pulse and oxygenation is they're also looking they have that bright LED on the bottom and they're looking at basically that capillaries like the Eulerian magnification it's actually looking at you taking your skin very a little bit just by basically the expansion of the capillaries down the wrist. Sort of similar but not a great idea. Okay I want to do another class challenge here and I'm going to hand out post-its for this. So imagine this is even crazier than it is. Imagine that your computer actually knows how you're feeling your emotional state. I want you to brainstorm individually for like three minutes and then I want you guys to go back to doing some epiphany diagramming. I want you to see if you can group your ideas together. So start brainstorming. What will your computer do or your smart phone do? Smart phone, smart watch, laptop. As if you know how you're feeling right now. What kind of mood will you set up? So brainstorm individually first for like three minutes and then I'll let you break out into your groups. Right on my post-it. One idea for post-it notes, remember this is designed to be precise and to be practiced. And one thought... So, importantly, this is not what you're thinking, but how you're feeling like emotions. Are you sad? Are you happy? Are you stressed out about the end of the semester? Are you excited to make it off the road? Are you nervous? Are you anxious? Are you anxious about the end of the semester? Are you nervous? Are you excited? Are you anxious? Are you anxious about the end of the semester? Are you nervous? Are you excited? Are you anxious? Are you anxious about the end of the semester? one more minute Доброго дня, Друзья! Я Роман Григорьевич Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Я праздную День Рождества Two minutes to get all your ideas on the board. Thank you. Twenty seconds. Twenty second. Okay. Before you guys have to run in a minute, let's hear some categorizations. Okay. Starting on the left. Can we hear your Rocky Mad Hats? Okay. Let's hear it. How about you? What do you think?\",\n",
       " \"I don't know if you can see it, but we have a lot of people that are trying to have a conversation where they can't see the question. And they're trying to learn what they're looking at. So that would be a problem. If you can't see it, you can't do it. That's why you don't see it. Yeah. That would be an issue. I would be a little bit more reliable to see if either of them can do it. That would be a problem. All right. Thank you. Great. Thank you. Okay. That's a great idea. Thank you. Thank you. Great. Thank you. Thank you. Yeah. Thank you. Thank you. Thank you. Okay, so quite a bit to talk about today because I didn't get to cover as much as I wanted on text entry, so I'm going to try to squeeze together sort of the last half of the lecture and also kind of try to wrap up the semester on a more futuristic note as well. Before we get into that, just some administrative stuff. So are there any Bake Off 4 questions in general? I did bring phones for people. A couple groups asked me for phones, and I actually was able to buy some more of the more better kind of phones on Amazon, the $29 phones, alerting for you guys. So if you have a really terrible phone, you can upgrade. You can swap it. But any general questions on Bake Off 4? Okay. Just some notes that I've picked up on. Some people are sending me ideas that are very similar to the scaffold code. There's like four squares and like a dot that moves around. There's no obligation for the squares. There's no obligation for the dot. It could be anything. It could really be anything other than you have to show four possible things and then two possible things. But it could be a map of the globe. It could be any kind of visualization that you want. A couple teams look like they're just sort of being lazy and just tweaking the scaffold code. If it's like you're using the gyro and the light sensor and the accelerometer and it's like, oh, instead of twisting or tilting or something very rudimentary, I would encourage you to ideate a bit more. Those are just the example sensors that I wrapped up in a very particular way. There's no requirement to use a light sensor. You don't even need selection. It could be the first target you touch or anything like that. So I would try to be a little bit more creative. Don't just sort of minimum viable product. Your last assignment of the semester, try to be a little bit more creative. You should read the Canvas posting, because there's a lot of details in the Canvas posting. It must be like 13-ish bullets in what you can or cannot do. For example, this bank out here allowed to bring props. The only requirement is the props cannot use electricity. They can use batteries. They can use electricity. But if you want to bring paper or xylophone or whatever, your best friend, that's fine. They can be part of the design. You can high-five them left and right or something for the two options. You can bring props and run electricity. It has other details in there as well. The other thing that I noticed is sort of this auto-locking, auto-select. Someone got confused about what can be the target or not. You can't have, for example, there's four options. All the other ones don't move to phase two, but only the correct one does. There's no notion of it sticks to the correct target. It's kind of functionally snapping, so be careful about that. All the paths should be equal. Also, if they get the first choose four stage wrong, yes, you have to complete that trial. You can't say, oh, well, I know they should have chosen option B, but they chose option A. Therefore, I know they're going to get the trial wrong. I'll just take the penalty right away. You can't do that because in a real interface, if they accidentally chose Netflix instead of YouTube, you're not going to know. So they have to get through to the end of the trial, even if it's wrong. Yeah. You can have the option to go back. You can have the option to go back. But it should always be there. If you get it right or wrong, usually that's the option to go back if you're going to include it back. So again, just think, what's the fairest way? Again, you're putting a green highlight or something on the path that you want them to choose, but everything should be equally accessible equally as fast. The key is, it's sort of like Bake Off 1. Your goal is to make selecting these things as fast as possible. All of the options should benefit, not just the correct one. That was it from the comments I got so far. Yeah. You really want to know? I feel like this adds more. worry than help, but I have been sharing it with you previously, so let's see. Make off for... Let's see what comes up. Face off for... Okay, last semester. So the time we're recording is fastest. It's how long to do on average one trial, not total time, but average per trial, and it looks like the fastest mean was 1.3, and the fastest, and then the ones I made it to the finals were like 0.8. So under, it should hit you under a second. You're like B1, A2, as fast as possible. See how fast I said that? Even faster than that with your hand or something, okay? So yeah, so I would say you should be in the worst team, which was the worst by a good margin, almost twice as fast, four and a half seconds. Does that make you more worried or less worried? Aim for a second, aim for a second. Any last questions on that? Okay, so the bake-off is in sort of roughly two-ish weeks, and I'll read like a week and a half, so basically next week you're going to be doing finals, you know, wrapping up things, reading days, and so on, and then we get back and that Monday, at least, Monday or Tuesday, Monday? Monday, the 13th, I think, we're going to meet, have some fun. As you can tell, this bake-off is meant to be a little more fun and jovial. It's going to have people screaming and, you know, spinning in circles and stuff, so you can be sort of creative. It's a bit more bipartisan than the other ones. Today, very sadly, at least I'm sad, is the last lecture, so there's not much more I want to tell you about HCI. Let's talk twice as fast as I normally do today, and just using that last little ounce of energy to share some final topics with you. And then I already sent out an email about this, but it is really important, and I would really appreciate it if you could send it to the course evaluation. for this class. It's not only important for me to be able to iterate on this class, and I've been telling you all semester long, like, save time for the iteration. Like, iteration is really important. There's ideation, there's iteration. I iterate on this class, it's getting better and better and better, I think, over time. And it's also really important for me to grow as an instructor, and it's also important for my department to give me things like tenure. CMU has the longest tenure clock in the U.S. at nine years. You know, make Stanford and MIT look like a cakewalk. And so, I do want to get tenure eventually. I'm going up for associate this summer. And so, it also is important for my department to see how I'm teaching my students. Okay? And with that in mind, even though I agree it's a little bit risky to say this right after I say, you should fill your course evaluations in, there is one final tough quiz. I am that evil. How many people are graduating? Raise your hand if you're graduating. So, think about this. This might be the last tough quiz you've ever taken in your life, and I get to give it to you. What an honor. Okay. So, there is a back back expert. Покладіть тісто на тісто і дайте йому відпочити на 20 хвилин. Покладіть тісто на тісто і дайте йому відпочити на 20 хвилин. Покладіть тісто на тісто і дайте йому відпочити на 20 хвилин. Покладіть тісто на тісто і дайте йому відпочити на 20 хвилин. Покладіть тісто на тісто і дайте йому відпочити на 20 хвилин. Дякую за перегляд! Okay, maybe another 30-ish seconds. Okay, here we go. How many people are done? Raise your hands. I'll give you guys another 15 seconds, most of you look like you're done. Okay, start to coalesce those for me. Hand over the final pop quiz of your life. Hopefully it was a good one. Reaching back, remembering. Yeah, maybe some of you won't graduate now. Okay, getting towards last call. Okay, do I have everyone? Thank you. Okay, just really quick, because I know everyone wants to know. Okay, so, the most popular story, but not necessarily the right story, but the one you often hear in the media for example, about the QWERTY layout, it centers around preventing those keybars from jamming. That he purposely, showed purposely, slowed down the keyboard. How many of you type slower to stop those keybars from jamming? And that's why we have this unusual arrangement of letters that seem suboptimal. Keystrokes per character, that is, and sort of in the name, if the average number of keystrokes, or presses, or actions in general, needed to enter one character. So if it takes you, you know, tap, tap, to type in the letter G, that's two keystrokes per one character, so two to one ratio. That is C, the number of actions entered in your character. What is the string distance between these two lines? So the string distance is insertions, deletions, or swaps. So in this case, between my correct name and my wrong name, I get to add an H to Chris, remove an S from Chris, and add an R to my last name. names. That would be three. Three changes to get those. Okay, typewriters. The original typewriters. They were initially developed, including those index typewriters, they were initially developed for legibility on forms and standardization of font. Scholz, even when we had gotten this far as a QWERTY keyboard, even Scholz didn't believe it would ever be faster than handwriting, and it certainly isn't good for your hands. The ergonomics of keyboards today are bad for you, so it's only the first two. Some people just put the first two. That was a little tricky. Okay, when entering, that is a huge blank, and the computer guessing that the next word is a noun, that would be an example of using a language model, the syntax of a language. So it isn't a keystroke model, it isn't a letter level, because it would now just be like the next letters of T, or something like that, and it isn't a mental model, it's a language model. Okay, professional stenographers on steno-type machines tend to be fast, so how many people have a stenographer type in shorthand? Yes, they do, and that does make them fast. How about they have expert training? Yes, that is one of the reasons why they're so fast. The size of the pixels is generally smaller than I can see. Not really relevant here. Accorded keyboard design requires less finger movement. That is also true. They have less things to move, so it makes them fast. Okay, now we're going back a couple of letters. So, vibroacoustics encompasses the study of vibration through what media? And the correct answer here is all of those. All of these, and human tissue is bioacoustics, but that is a subcategory of vibroacoustics. That's a big umbrella stuff. Partial points off are the various things, but liquids, gases, solids, humans, humans are just composed of liquids and solids and gas, so we're just, yeah, super sad. Okay, ambient noise, this is C, miscellaneous background noises generally unwanted, so like the hum of the air. And finally, if you imagine that this sheet of paper is a wall, there's these two sensors, it should be aligned like this, and I actually saw most of you did this. There's a straight line between the two, if they arrive at the same time. How many people got that answer? Yeah, it seemed like this. Wow. Can you guys believe me? How many people feel good about that final thought quiz? Raise your hand if you're like, yeah, crushed it. Oh, wow. Wow. Okay. Well, on that note, I thought I'd give you a little bit of good news, which is there's this great chart of what age you want to be in life. So when you're down here, when you're 12, you want to be, on average, five years older. And then as you get older, like me, you want to be younger. And I would suspect most of you are like me here. So this is as good as it gets. So you should enjoy it, because it's only going to be downhill from here. Real, real low. So I bring this up partially to be comedic, but you have to enjoy this time. Even though I know we're going into finals, try to keep a smile on your face, get positivity to other people, try to be positive yourself, because life is good. Life is good. You guys should enjoy this time. Don't super stress out. I know I'm giving you homework assignments and finals and stuff, but life is still good regardless. You're applying your intellect for interesting challenges. That's good. The other thing I mentioned, that I should mention, is if you like this course, and if someone who's actually followed this series, if you don't watch Graphical Computer Science, it's OK, let's bring it. So you probably have noticed, if you're an astute observer, that a lot of the DHCS lectures are sort of similar to some of the things that are in there. And in fact, some of the slides are literally copies of DHCS. This is a video still from the series, and you'll notice the graph is literally the graph that I put in the slides. So me and another professor's team, you co-wrote that whole series, produced by the Green Brothers and so on. And if you like DHCS, I think you'd probably like this series. So if you want to have sort of a historical kind of thing, and a human-centered perspective on the origins and the evolution of computer science. I highly encourage you to go watch this movie. It's a really fun collaboration. Okay, moving on. So I wanted to conclude on some example text entry. I got to kind of end with number keypads like T9 and Multitap, but I didn't really get to show you any other experimental methods that are out there. So I sort of put together just a very quick compendium of other ideas just to show you what researchers, like what you're doing, this exercise that I'm making you do for like MAKE-A3 and things like MAKE-A4, this is what HCI researchers are doing. They're doing it in a more formal way than I'm doing it in two weeks. But nonetheless, they're doing very similar ideation and iteration that you guys are doing. So here's an example of a paper that was published. It is fairly old, 15 years old, but all these number keypads are saying how to make text entry more efficient. And in this case, what you do is you hold down a number, like in this case, the number seven, which has TQRS, and you would tilt the phone in the way you want to type the letter. So P would lead to the left, Q would be up, and if you wanted capitals, you'd actually tilt very far to the extreme. It's basically a bake-off quality idea, and they were able to study if this would be effective. Again, this was a free iPhone, and they were getting around 13.6 performance, which is actually pretty, pretty competitive. People have also looked at things like number keypads on remote controls. So in this case, you might have a remote control like this, like on your TV or your stereo receiver, something like that. And again, they can divide up the whole grid in the alphabet like this, so that you use a double-tap, there's a keystroke for character two. I press the one key, it takes me to this box, and if I press the one key again, I get the letter A. So it's a two, so it's basically a double-step kind of iteration, and I can use a number keypad to get to every letter, including symbols. We already talked a little bit about handwriting recognition, and that's around sort of the 20 to 30 we're committed to a range. And so you might think, well, why don't we just use handwriting recognition everywhere? We know that we can take a pen, and we can write it in size-nine font on a piece of paper. So we should be able to do that with smartwatch recognition. It's sort of generally true, and people have been looking at handwriting recognition for a long time, back to the 90s. Here's actually handwriting recognition from the Newton's. This is 1993 technology. And it heavily relies, it detects a sad onion, and it got it. It's kind of a weird phrase, but it capitalized the O in onion. So now it's going to write something a little more unusual, like celery overcoat. It's kind of getting nastier, and what happens is the language model is like, I have no idea what a celery overcoat is. I've never seen that word in my 5-gram model. And it's sort of messy as well, and so it just fails to even try to recognize it. And this has been true for a lot of different handwriting recognizers over time, is that it does sort of fail. And in fact, handwriting recognition goes back even before 93. Windows actually, like Microsoft, did it first on this gigantic tablet back in 1991. We always think, Apple dropped an iPad, and all of a sudden, tablet computing was born. But actually, Microsoft had been investing in it for over 20 years prior to this. And Windows computers still have handwriting sort of baked in. So now they have it. It's in a newer generation Surface Book. And you can see it doesn't sort of continue its handwriting recognition. It's actually writing. It's doing the transcription. It wrote I am, and it transcribed it as Ian. So there's a long way to go. Humans are really good at finding that context in a person's experience. And it's really still quite hard, just like voice recognition is. So you can see, as it's moving along, it— So before that, to increase not only the recognition rate, but also kind of disambiguate input from users, is there's been a tradition in the exact— providing sort of like pseudo-alphabetic inputs, probably the most popular, which is graffiti, if anyone owned a Palm Pilot. Anyone owned a Palm Pilot? Okay. Yep. Getting rarer. That's cool. So, you know, this was sort of like the thing in the 90s, and maybe like early 2000s, I guess. And it's sort of handwriting-like, so it's simplified strokes to make the recognition for the machine learning easier, but also to disambiguate. And Unistroke is basically academic. version of it. So here was the unit stroke top of that. And what they did is, because it's really, if you can just have one stroke to recognize, like basically, when you get a touchdown event, you just track the point until there's a touch-up event, and then you pass it to the classifier. If you have some sort of a weird letter, like a P, and you draw it in two parts, you know, do they draw an I, and then like, I don't know, like a C or something like that, it gets hard for the recognizer to know. So they made this concession where everything is done with a single stroke. You start with the dotted, and then you write. And you can see, it's sort of weird, like the J, like this, that makes sense, but H is like this. And so this was the original sort of academic HCI publication that proposed this, and it was pretty fast. But then Tom came along and released Graffiti, and because this was a consumer product, they needed to make it a little bit easier, and so there's concessions and accuracy to make it easier for the human. So now you can see that things generally look much more like the sort of, you know, letters or tables like this, and the K is sort of weird, but you sort of, you wouldn't guess that with anything else, but it sort of looks sort of K-like, and M, and N, and so on. The W here is pretty strange. So this worked much better, but it did have an overall reduction in accuracy to about 11 or 12 words per minute. People have plotted this over time, so you have, you know, basically if you do multiple sessions, and you always want to do multiple sessions to see how good people are getting, and what you can do is sort of draw these lines out to know, if you only train them for 100 sessions, how fast might they get? And you can see HumanStroke tends to win over Graffiti, but it didn't matter because if you can't get it to market, and no one will adopt it, you know, it's sort of a moot point. But this line here is what you would call the power of authorship. It's actually a power relationship, it's not logarithmic. So, I'm not going to talk about the feeding. Let me skip ahead actually here. I just want to show you the more interesting ones. This one's a very interesting technology. So what you do is, you move towards the letter that you want to type. In this case, it's with a mouse, but you do it with a pen. And you can see what they're typing up here. It's writing, Hello, Al. are you, I think, and you basically are making the letters different sizes, and it's easier when you see it in person, and you basically move towards, sort of draw a path, sort of swipe like, but letters are flying at you from the side, and you can fill out this whole thing. I'll play that again. We'll see if you can follow what he's doing. So he's going towards the H, which tends to be larger. Oh, yeah, pretty crazy. It's a crazy idea. But it is interesting. Now, the thing that's very kind of crazy idea, the thing that's a big red flag in sort of the academic community, is the only number reported in the paper is that the authors achieved 34 arithmetic. Now, it's sort of, as you know, on your own bake-off, right? Like, you tend to be artificially fast on your own design, and maybe other people would eventually catch up. But, you know, the thing that's really interesting about this is that, you know, it's sort of a, you know, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a, it's a. you can think about this. Approximating the difficult is sort of navigating essentially through a curvy tunnel, but the tunnel has branches and there's different letters of every branch, different words at the end of different branches. So you could actually model this as a steering long, but people knew for a long time that stroke based methods were going to be faster because you don't have to do this sort of targeting with your finger and so this is actually the very early, this is from 1998 the very different origins of basically this swipe style keyboard that we've seen that's fairly popular today they didn't arrange it like a cursive keyboard, they thought that was too difficult algorithmically, instead you got this arrangement like this and you sort of dip in and out, every time you kind of went into a little spot, you type that letter and it meant that you could keep your pen on the surface, which would help if you have a 4. scale, or if you're walking. To keep my pen on my hand, I can apply half a newton to pressure or ten newtons of pressure, and it always saves it. So I can be very coarse in my interaction. Even if I'm running along, I can probably keep my hand on my own body. And unlike if you're trying to do lots of typing, if you hit a bump in the road, you're going to accidentally type letters. So keeping your finger grounded on the surface makes you a lot more average. And they were saying you could get up to about 20 words per minute. But the real origin of this whiteboard was actually this IBM paper. So IBM was a guy that I mentioned before, Suman Zai, sort of like the czar of text input at Google. He did the original one, and what he built it on top of was the Opti 2 keyboard that I showed you in the last class. So that really unusual arrangement of letters. There was even one that was like a honeycomb, and that's what they did the original swipe on. Only later did people realize that you could actually superimpose it right on top of a QWERTY keyboard, and that was sort of the magic leaf. Well, not magic leaf, but it was the breakthrough, because before, it's a really unusual keyboard layout that was optimized so that swiping methods were like possible. It meant that it was an unusual keyboard design, and so the breakthrough was, let's use QWERTY and make it a little bit nastier for the algorithm to disambiguate some words, but now people can start using it essentially right away. And later on, they've actually ported this now to Android first, and they can get about 22 to 24 words per minute, which is pretty competitive, and you do this on your smartphone. So here are just some other examples of people sort of thinking about text entry. You know, the original Google Glass had voice input, but obviously voice input is not possible to say everywhere, and so someone came up with a gesture algorithm that you can use. use on the side of the blue glass, which is basically a track map. And so you have this up, down, and swiping gesture set that's sort of like, sort of metaphoric, sort of follows a little bit of what the letters look like. So like an O with a down and an up, an I with an up, a U with a down and an up and a down, and kind of angles, and you just hit this on the little kind of the bridge, I guess, of your glass. And they're able to get about 20 words per minute as well. This is a really interesting idea, and if smartwatches had it, you know, one of the things that they could do, even if they faked off, is to split their keyboard in two, because they couldn't fit that many buttons in. And what the insight in this case was, is what happens if you had a watch that could recognize what finger is touching the screen? Like, it's not inconceivable in the very near future that, you know, all these fingerprint technologies that are on smartphones are small enough that you just basically put them as a whole watch screen. And since it can recognize fingerprints, it can recognize what finger is touching the screen. So to type, you know, the left hand character with a Q, you use your index finger, and to type the W, you use your middle finger or ring finger, right? So you can actually disambiguate your target, even though it's still finger-sized, by basically using different keys. And again, they do this nice plot of, you know, almost all text-centric papers include a plot like this that show, you know, by the time you get to the sort of the 10th day of practice, you're X-fast. You would never report just the, you know, the accuracy after one trial run like we do in the big ones. Okay, so the other thing that we should talk about is, you know, voice input. You know, obviously this is getting to be very fast. Humans speak at around 150 words per minute. I mentioned this before. And so this may be sort of the holy grail. But, you know, for people, think about why you would not want to use voice for everything. How many people use voice input for any of their devices or data? Because I use it with my phone. Okay. What are some downsides that you've encountered while using voice input? Yeah. Oh, sometimes you're in a quiet environment and don't answer other people. Absolutely. Yeah. Yeah, it'd be really embarrassing in class, the people that are on Facebook right now, it's like you're accidentally, I can hear you whispering, like some sort of weird thing that you want to do this weekend or something, so they're not going to say that out loud. The other big kind of downside is that, unless they can uniquely recognize you, is that you're going to end up controlling everyone else's devices. So if someone's like, playing my favorite Britney Spears album, and everyone's like, oh my gosh, and we're all having to listen to it, because you're in a bus, you're on this little hill together or something, that's going to be a big problem. So not only do you have to disembiguate people, but it's uniquely you, but there's that sort of privacy invasion and sort of disturbing. So pros, natural form of communication, you don't really have to learn anything new, you know how to do it, you're speaking already, and it's also eyes and hands free, which is a huge plus. If you're out running, if you're a soldier, you're a pilot, you're going to want to be able to use your hands and eyes for other things. And it can be really fast. But, of course, as I already mentioned before, it does have to do with external noise, people just talking out loud, disembiguating from other people. And what's really holding back, I would say, the language right now, is kind of a voice input right now, is that the cost of correcting errors is really high. You can talk super fast, like Google Translate, but the problem is one in every ten words is slightly wrong. And then you have to go in, like on your touchscreen device, and correct it. And if you look at studies that have looked at voice input, the correction phase takes longer than the saying phase. So if we could find ways to make the correction, like not that there, the other there, the E-R-E there, and it could somehow interpret that and sort of stuff it back into your sentence, that would dramatically increase the text. I'm amazed at how many mistakes it makes. It's like, that's only going to dramatically correct, like why would it choose that version? Anyway, one other video that I think is sort of comedic about language input, let's see if this plays. All my videos dead? No, I crashed. What's going on? Maybe, look, maybe all my videos are at my toes. No, I'm... No. Well, I thought that. I will post you a link so you can see why Keynote has finally died on me. Okay, I'm ahead, I'm ahead. Okay. Let's not talk about sign language input. I do want to talk about... And then finally, you know, people are using gaze. One final example. If you are locked in, you know, kind of Stephen Hawking style, you don't have very limited motor movement, you could use gaze and blink and you very slowly type it out. In the case of Stephen Hawking, he could twitch this cheek muscle. It was basically the only thing that he could do. And he would basically move through like a keyboard and he had like a dwell. And that was the only physical motion that he could produce other than maybe breathing. But actually, he might have even been on a ventilator in later years. And so anytime you see Stephen Hawking talking, you know, that has a very, sort of robotic voice that sort of became his personality, all those were prepared for speeches. I think his words per minute effectively were something like 0.2. So to write a sentence would often take him like 5 or 10 minutes. So anytime you see him speaking, like in an interview, it's almost always pre-recorded. So that's a great scientist that's passed. Okay, moving on now. Now we're going to talk about output technologies. So, you know, we have our sort of traditional senses, right? You know, the five classic senses, hearing, sight, smell, touch, and so on. But there's actually a whole bunch of other senses that we don't necessarily think of as the classic senses. But nonetheless, you could think about from an HCI standpoint. So one is the anesthetic sensor, commonly known as the peripheral section. This is the knowledge of where your body is located in space. If I were to blindfold you, I'm going to climb up. If I were to blindfold you and I moved your arms into some unusual configuration, your whole body into some unusual configuration, you'd be able to tell me what pose you're in. You know where your body is located in space. And that's that sense of proprioception. You don't have to look at your body to see that. You can just sense it. And that is proprioception. Pain is sort of interesting. No one's really on the forefront of HCI plus pain research. But it is a sense that's distinct from smell and touch and so on. It may be non-liquid under touch, but it actually is a different system entirely. Balance is a really interesting one. We know this is becoming important. We're thinking about ARP, or people getting kind of sick, because the vestibular system is not in sync with the eyes and you sort of get that motion sickness. But your sense of balance is very well-tuned, and it is one of those senses that we don't really traditionally talk about. You also have a whole bunch of accelerometers in your body. You feel this the most like you're in an airplane and it drops. Suddenly, you sort of feel that weird sort of feeling in your stomach. It's sort of like, almost like a tickle. But you have actually internal sensors on your organs that can sense kind of weird accelerations, and it's not a traditional thing. So, you know, we're thinking about ARP, or people getting kind of sick, because the vestibular system is not in sync with the eyes and you sort of get that motion sickness. But your sense of balance is very well-tuned, and it is one of those senses that we don't really traditionally talk about. You also have a whole bunch of accelerometers in your body. It's sort of like, almost like a tickle. But your sense of balance is very well-tuned, and it is one of those things that we don't traditionally talk about. So, you have actually internal sensors on your organs that can sense kind of weird accelerations, and it's not a traditional sense. Vibration is often used under touch, but, you know, our fingertips are fantastically good at recognizing vibrations. And this is actually why people theorize we actually have the ridges. Or one of the two reasons why we have ridges on our fingers is when we stroke a surface, the interaction with the ridges actually lets us discern the texture of things, like fruits and so on, or cloth, whatever it may be, at a higher fidelity than you might otherwise work. Also, supposedly, you know how your fingers feel? They feel like they're in contact with something. So, supposedly, the new theory is that that's actually going to improve your grip, and we're in more like aquatic, moist environment, sort of thing. And if you do that, you know that, but when you're carbon, like when you have less oxygenation in your blood, you start to breathe harder. So, there's a whole bunch of chemical receptors that are keeping your insulin and sugar levels and all those sort of things intact. You don't really sense them, per se. You know when you sort of need some energy, and you know when you're out of breath. So, you can sort of roughly sense these things, but they're a little bit different from some of the other things. And certainly, if we kind of look beyond humans, there's also the cool stuff, like, you know, sharks being able to sense electromagnetic fields and echolocation with bats. And so, you know, it'd be really neat if we could somehow add those to ours. And maybe we can if we have, if we use technology. But probably the classic kind of seminal thing that I think was done in the 90s is someone wore a belt with vibration motors in it, and it would vibrate where the compass north was. It was just, you know, kind of a Reno-style project. And eventually the person was able to basically integrate that into their perceptual system and had a much better spatial awareness of where they were. That's sort of one of these, like, ridiculous studies. But it is sort of interesting that over time you can integrate these new, you know, the brain is sufficiently plastic, you can integrate new sensing modalities. But let's start with the easy stuff first. So we're most familiar with computing in terms of light, vision, and motion. So typically we have things like computer models that display all these pixels. We talk about how efficient is this super-high bandwidth channel. And, you know, this is going to be the most pervasive form of human-computer interaction for a long time. People are starting to superimpose on other cool things, like peripheral lighting, where the TV is sort of sampling what the average pixel is here, and sort of trying to give you this kind of ambient glow around things like TVs. Again, your fovea, kind of the high-resolution part of your eye, is going to focus on that TV, and you're going to sort of automatically interact with what's around. So you get this sort of benefit, even though it's only like 10 pixels. Here is one of my favorite projects. Let's see if any of these videos play. No? Oh my gosh. None of my videos are going to play at all. What do I need to do here? Do I close something? Save? Let me just try restarting Keynote. I think that's the problem. Some of these are interesting. Ah. Yes, okay. So this is thinking about having projectors render onto the room. Okay? So I'll let these people explain. The Loomer Room is a proof of concept system that augments the room surrounding a television screen with peripheral projected illusions to enhance traditional gaming experiences. For example, such illusions can change the appearance of the room, turning the living room into a cartoon world. The illusions can distort reality, extend field of view, and enable entirely new gaming experiences. Our vision for a productized Loomer Room system is an ultra-wide field of view device that sits on the user's coffee table and can cover a large area surrounding the television. Our current prototype uses a commodity wide field of view projector and a Kinect sensor. The Kinect captures color and geometry of the room, and the projector displays illusions around the television screen. The Loomer Room system is self-calibrating and designed to work in any living room. This is a structuralized game to get a 3D model of the room. The most obvious way to increase immersion is to simply extend the content from the television screen out into the room, replacing the physical reality with the game's reality. Instead of simply extending the game content, one can focus only on the high contrast features, for example, highlighting only the edges. With focus plus context selective, only certain game elements escape the television. For instance, with a first-person shooter, we can delete only weapons fire or explosions out of the television. We can also display markers representing other characters or key items in the game. The room furniture can be used to mask out some areas, making it appear that the game is actually a game. the game is being played beyond the wall of the room. Illumaroom can change the appearance of the room to match the mood of the on-screen content. For example, one can saturate the room colors. So this is very cool how they do this. What you do is, because you have an image of the room with a camera on the projector, is you figure out what the colors are of everything, and then you render red on red to make it even redder. Or you can project another color onto the red to desaturate it. Make it sort of simulate like a whole tiny room texture. And then render thick lines on things, and it's a really convincing effect. By distorting and reprojecting the room texture, it is possible to warp reality and make it appear as if the room itself is responding to the game. So seeing this in person, I got to see this demo, and it is really convincing. And because they have a 3D model, it isn't just like they're projecting on the wall naively. They're actually projecting it. So if you're sitting on the sofa in front of it, it all looks correct. It isn't like it's distorted on things. And when you get these texture remapping, it's really convincing. One other example that I'll just mention quickly is they have that example of the snow. And again, because they know the geometry of the room, it's like if you stop in your car and the snow is falling, the snow actually accumulates on the surface. It will accumulate on your coffee table. And then when you hit the accelerator, it will blow off onto your legs, because they have a 3D model of the room. It's super cool. Yeah. So can you talk a little bit about the games? It's almost like a feature where they program it in 4K. Yeah. So because this work was done on Microsoft, they were able to get developer builds of some Xbox games to be able to get this extra content. So yeah, you'd have to have support, unfortunately, to take advantage of it. But these guys have since left and done their own startup company. So we'll see if they can ever do anything. But people don't want a projector behind them. That's the thing. It's hard in a typical living room to have your TV and then also a projector and wire thrown into them. So there's a hard sell to the Microsoft folks, I think, to ship this with the Xbox. And the Kinect is a key part of it, too. Any other questions on the project? Very, very cool project. Yeah. I don't think they have that right to go into this room, or could they just pop a projector anywhere in this room? It's like, where is the key? It is unsettling. It's gone into the archives of research that no one will be able to read or look. Um... Yeah, so it was portable. As far as I know, not that I'm privy to Microsoft's development plans, but as far as I know, it's sort of shelved as being unproductizable. Because it's cool, but would you spend another 500 bucks on your Xbox to get it? Yeah, but I don't know, maybe... I didn't have a Microsoft in five years, so I have no idea. Maybe they'll do something cool with it. Too bad, right? Because it'd be very neat. Okay. Moving on beyond... like, you know, kind of projector displays. There's a whole class of research in ambient displays. These are sort of little props that might use color or light. You can't say which product, because I think you're missing things like pillar changes or gear position. And the idea is to convey sort of a few bits of information to you. So, you know, when you have things like this, Forbes might be able to tell you what the weather's going to be like, or your stock market for colors. So if it's like blinking red, you're like, oh my gosh, I'm screwed. If it's like blue and happy, you're like, oh my gosh, I'm going to have a fancy dinner tonight. And you see these used all the time in various rest of the applications. They're basically like indicator lights, but are programmable. I already mentioned this project here earlier in the semester where, you know, more people walk, the more it would glow. It's actually sort of an ambient light behavior. And people have also learned stuff with looking at water, which they kind of strap on this. And a proof of concept product under your water tap. And it shows you, you know, how, are you being friendly to the environment or not? And it can sort of give you this extra dimension to use. You don't always have to use LEDs either. There are some cool projects that have been done with like flowers that will pop open. These are felt flowers, but I think they have some sort of like shape memory alloy that can actuate them. And so they sort of unfurl when you've like, I don't know, you've done the dishes or you've finished your assignments. You have no e-mails left in your inbox that are unread and they sort of bloom. And it's also kind of a nice way to be able to look at how you're facing the environment. other sticking screens everywhere. I think it's a more holistic experience. Here's another cool video. I think it's a really neat idea. Let's see how this one will play. Come on. So they've added little magnetic dots to the back of plastic or paper in general. And you can actually see these, like, snap together. But that's not the cool part. So you can stick it on this board, and it can act like a magnet behind it, and it'll actually move your Post-it notes around. And then when it gets to the right spot, it can actually re-stick to the board. Again, also, when you're, like, done with the Post-it note, it can, like, wave at you, kind of get your attention. And when it wants to get rid of your Post-it, it can attach it to the board and track your motion. This is an interesting, like, mixed reality kind of component. Awesome. Here's another example. So this is a huge grid of motors that can move up and down. It's really hard to keep this thing running, because this is, like, 25 individual motors. That's 400 motors. But this is a really interesting idea to tell a blackboard. Thank you. I didn't expect it to be able to happen like reality. Yeah, that was incredibly loud. I don't know why. We can't get this right. Such an old technology and the interface still sucks. I don't know why. OK, so beyond moving beyond a visual interface, the sound is also really, really interesting. People have not done a huge amount of thinking about the next generation of sound. So we know that taking a photo, confirmation sound, sort of a shh effect, when you send an email, all those things kind of add to the user experience. There was work done like 10 or even 20 years ago now on these iconic sounds. One was called ear-cons, one was called auditory icons. There were two different approaches. Ear-cons is because we already had eye-cons, and so they wanted to make ear-cons. Yeah. So ear-cons. So the ear-cons is a brief distinctive sound used to represent a specific event or convey other information. And so this is things like beeping and chimes. It's sort of meant you learn, basically it's a recall. You learn like boop, boop means like you're having an email. Auditory icons take a different approach. Different work, I think, by German tech researchers that looked at basically playing iconic sounds. So when you got an update about the Steelers, like some sporting event, you'd hear people cheering in a stadium. Or if there was some sort of traffic alert, you'd hear cars honking and engines running. So it was like a metaphor of what the category of sound was. Versus ear-cons were just like melodies, just sort of like jingles. But both could be potentially useful. But no one's really followed up on them in a long time. Here's a bunch of papers if you're interested to read more. Probably my favorite one, I think I mentioned this earlier in this semester, is this paper from the late 80s that one of the original designers on the Mac had proposed that the trash can, when you drag a file into the trash can, that they'd parametrize the sound by how full that trash can was. So if your trash can was totally empty, and you sort of dropped it in your dock, and you'd be like, you can't hit the bottom. Or maybe like reverberate around you and you'd hear like a big steel trash can. But if it's like full of trash and you really need to do good kind of digital hygiene, you drop it and immediately falls like a big kind of crunchy sack of documents and it's totally for free like you're going to play a sound anyway so why not kind of introduce that little extra bit of information and I think there's work to be done recently I've been thinking about trying to do myself like you know instead of having just like the oh you got a notification like the beep sound or you know you got an email it's like is there a way to somehow build into that sound effect how familiar you are with that person to the urgency of that email so if it's like a spam message or it's just like wow tap off whatever another 10% off list or something like yay it's like you know not a very interesting sound it's like someone coming from the dean that it should play a slightly different can still be the same sound but like slightly different key or I'm not a musician so I don't know different like scale whatever it may be I think that's a right opportunity no one's really thought about sound design as we strip down interfaces sort of more minimalist design we've gone all in on the visual side and no one's gone back to really think about how we can make sound sort of as good as it okay so moving more exotic now smell and taste so there's not too many interfaces with smell and taste there are researchers working on this and there's a couple startups that are doing crazy stuff they kind of all look like this where there's like piles of kind of smells that are sort of like perfumes that they'll like inject into your nose or kind of walk into your nose that when you put on the vr headset and you're like running through the meadow you smell like the meadow flowers I did have the unfortunate pleasure of using this device which was at the CHI conference a couple years ago and you would stick your tongue into this kind of little orifice on the thing and it would electrocute your tongue and basically cause your whatever neurons to fire but I guess you're like nervous nerve receptors to fire to simulate like different flavors that was sort of the goal like if we pulse them with this like frequency of electricity like it'll taste salty and if I pulse with this electricity it'll taste like greasy potatoes or something to me it just felt like my tongue was electrocuted quite honestly but that was not the disturbing part is that after I put it down it is handed to the next person It's the exact same thing as me, and I was like, how many people have stuck their tongue in this thing? That was like the third day of the conference, like 4,000 people. So that was pretty nasty, and it's a major impediment to taste, I think, interfaces, unless you have one for yourself. So, I didn't get sick though, why not? So that is, so taste and smell, you know, it's kind of a little bit weird to be able to get working, but there's actually a very good reason why we don't have more smell interfaces, besides like it's kind of strange. You know what I'm thinking about, why smell? Smell, let's talk about smell, why we don't have more smell interfaces, yeah. And then a lot of you smell things. Sure, but why not have a notification that you have a new email, and like you just smell, like, what if this is coming out of your pocket? Um, like, you know, like when I put it on the floor, all the time, and I think it's weird, so that's why I don't get to smell anything, which is like, you know, I did it, so I'm gonna do it. Yeah, that definitely happens, yeah. Okay, yeah? Yeah, it just smells really weird, so we have to like, often text you all the time. Mm-hmm. Yeah, so that's another big one, is that you accommodate the smells, and you probably know this, if you like, leave your dorm for like a month, and you come back, you're like, this smells kind of like a little bit different than I remember it, because you sort of reset your system, but if you live in a place, you don't really smell, like this room probably has a smell, and we probably have accommodated it to it over the whole semester, yeah. Yeah, so very low switching time. Like, if I had an alert that like a pop quiz was coming, and I filled this room with like rotten eggs, now that it's over, like for the rest of class, you're gonna be like, smelling, so very low duty cycle, even if you have something worn on the head, the switch rate is probably on the order of like, tens of seconds, maybe even a minute, and then you do have this accommodation problem, where, you know, if you want to like, run through the meadow, but now you're like, going through the swamp, like hacking your way through some like, you know, kind of hack and slash, like kind of a search kind of thing, it's just gonna be very hard to do reliably, it's not like sound or. Yeah, so you have to get these kind of ink cartridges, essentially, to keep on plugging in. You are like, gassing chemicals. Yeah, that's another problem. Yeah, yeah, yeah. Again, what's interesting, though, is that humans are not actually very good at smell. You know, compared to, like, a bloodhound, they can, like, smell some, you know, piece of cloth, and they'll, like, chase someone for miles. Like, incredible what some animals can do. So humans are actually pretty bad, and you can actually simulate a pretty wide variety of smells with only, you know, under, like, 20 constituent ingredients, you know. So you can get something that's pretty realistic, but again, doesn't really add much to the experience, and, yeah, you have to, like, keep on replacing these cartridges, so it's fairly impractical, in general. So, yeah, as I was saying, not very good. Low bandwidth, high latency, slow dissipation, you know, but nonetheless, there are people that are researching it, and it is quite evocative. I mean, smell being a very primitive sense is that it can evoke very strong memories. Often, if you taste something or smell something, it can elicit memories, because it's more kind of like the, you know, pre-mammalian, like, lizard part of your brain or something, and it's quite deep. And so it can enhance experiences, but it's unclear, like, if you're typing in Microsoft Word or, like, scrolling through Facebook, if you want it to really be smell-augmented, unclear. But, you know, maybe it'll become more practical in the future. A lot of things with touch, you know, touch is something that we use all the time, not really as an input channel. We use our sense of touch to, for example, type efficiently, but we don't really do a great job of burying that texture. Sort of the state-of-the-art is vibration motors, that when you're typing on your mobile device, you know, you sort of, you kind of get these little tiny impulses back that are And we don't really think about texture, or thermal, or, sort of, like, you know, the surface texture of things being quite the same. So touch does encompass all these different things, you know, the pressure that you're applying to the screen, the texture along the surface. temperature of the surface, and when you hold a gallon of milk, or something that's metal or glass, you know it's metal or glass, not because you have such an incredible sense of touch on what that surface is, but actually because glass and metal actually feel different in terms of thermal conductivity, and that's how you can distinguish. So if you're mindful people, you ask them to guess what these things are. They're using all these different components together. Most of the vibration motors that you see in the phone, minus some Apple products, are just little tiny DC motors. They're just spinning motors, and they just move away onto one half of them. So it oscillates, because it's not even, it's not symmetric, it just wobbles back and forth. And for the most part, like all the Android phones, certainly all the cheap Android phones that I'm giving you in class, is that when you want to render a haptic effect, it's like motor on or motor off, and that's not a very expressive vibrational channel, given how rich our sense of touch is. Apple sort of went, you know, a step way beyond what else is a haptic engine, which is a special controller, and it's sort of similar, there's like this electromagnetic coil and a big mass, and it can basically pulse it more like a speaker, and like a speaker for music, and it's much more fine-grained than anything else, and can render slightly better effects. But even still, you're touching a piece of glass, and no one's really come up with a holy grail on how to, you know, make the glass feel like silver, you know, sandpaper, and things like that. A couple different technologies, but nothing that's really taken off. Or if you want to have really detailed things, we actually get a sense of touch, and I can reach out, and I can grab, you know, that sword in VR, as you tend to see in mechanical things like this, which are, you know, kind of armatures, or sort of exoskeleton-type things that wrap around the body, you can arrest the arm, you can arrest the fingers. There are some more product versions like this, that are more CAD, architectural, and it's basically just to get an arm, and you, and it is still quite convincing, you grab the pen, and now you may be wearing an AR VR headset, but when you probe, you actually do feel a 3D object, and it's sort of mediated through the pen, and it's actually quite realistic, and you can actually simulate a texture, so if it's like a sphere, or like a box, you can feel the different sides, and you can actually have the different sides of the different textures, and it feels quite realistic. you know, it's still relatively limited in going hands-on. People have done a lot with actuators, so you know, covering the body to be able to provide for the haptic and vibratory stimulation across the whole body. That's a very common thing, and there's actually some really clever work that's taking advantage of illusions. Let's see if I can find the video on this. Tile illusions, such as when two vibrating actuators are placed on the skin, the user would feel only a single vibrating point in between the two actuators, not two vibrating points. We formulated perceptual models to precisely control the position and intensity of such a virtual vibrating point. The Surround Haptics Control algorithm scales these models to an arbitrary grid of actuators, and moves the virtual vibrating points anywhere on the grid. This allows us to easily create arbitrary and continuous moving tactile shapes on the user's skin. The resulting tactile display is near the infinite resolution and can be placed anywhere on the user's body, such as on the back, the arm, or the palm. In the same way that we have visual illusions, there's also illusions for everything. There's audio illusions, there's smell illusions, there's haptic illusions as well. And so this is a great example of a haptic illusion, that if you have two vibrating things, you don't actually perceive them as two individually vibrating points. If you mix the kind of signals just right, it feels like there's a point in between. And if you carefully control them, you can put them anywhere in between. If you do a grid, you can basically have it feel like someone's sort of tracing a finger on the back of your body, but it's a low-resolution grid, 5x5, or something like that. And once you start having that across, like if you're wearing a suit with sort of little vibrators built in, then you can have sort of like, you know, the sensation of people... whipping you or putting a backpack on and things like that to make an experience more immersive. So in this case, it's sort of pre-VR AR plus, they're doing it in a, in this case there's a simulating what's happening on the back with this projected graphic just to give you an idea. But in a game, you can imagine this is really valuable in AR VR. Here's another Disney project, quite interesting. These interactive graphics have expanded into the real world. We can use physical gestures or other body movements to interact with the computers. However, these natural interactions offer a physical feeling. We introduce Arial, a new haptic technology designed to let users feel free air sensations in the physical world without any instrumentation. Now Arial, you can't see it, well it's a little black. That's CG. But you'll see it in a second. The Arial device is comprised of five actuators enclosed in 3D printed materials and uses a flexible 3D printed nozzle with pan and tilt control for free space movement. When the materials are combined, the device allows free air sensations to be directed anywhere it sticks. Here's the clay. It's just the air. It's basically like smoke rings. And they can project them actually pretty far into the room. And what you feel on your body, it's like a little, I would say the strength of one breath. In that soccer game, when you block a ball, you kind of get that on your hand. And it's not amazing, but it's better than nothing. It sort of makes it a bit more immersive. You can imagine like in like a first person shooter, you sort of feel, you know, things hitting you like your vest or something like that. So you've looked under. A lot of these things are trying to augment that experience. Here's a totally goofy one where you'll see the person is going to step on that step. their feet when they take a step there. You can sort of guess what's going to happen. Oh, it becomes gigantic, right? So you have those extended legs. Hopefully you guys saw that. I'll step off in a second. And it does give you that simulation. Maybe you'll see it in the background. It gives you that simulation that you're going up steps. Because how do you do that in VR? Right now if you want to go up a staircase, it's a total fail. And so you have these kind of crazy looking shoes that you can put on to simulate this. And this is, in some respects, it shows you the future maybe of VR, but also maybe the failure of VR. There's some things that are going to be incredibly unwieldy to ever be able to achieve in AR and VR. If you really want to have, like, I can feel the wall in front of me, I want to feel climbing up the steps, but I want to do it in my own living room, it's going to require sort of this goofy array of crazy stuff worn on the body that's going to make you very, very heavy. Or you need to be put into some sort of sphere that you take out of your closet and you hook yourself up with strings. But it's one of those things that's going to be very, it's like the antithesis of the Nintendo Wii, which is meant to be sort of a casual gaming experience. This is like the, I'm going to have to go spend $5,000 to augment my living room with a cable entry system in order for me to do kind of a sentiment game. So that's a little bit unfortunate, and it's probably one of the biggest limitations of fully immersive AR and VR, unless you go to a specialized center. Here's a bit of a different project. This is a simple technique to change the material properties of malleable objects. This example demonstrates jamming. A soft bag filled with particles turns stiff as air is removed with an attached string. When the air is pumped... Let me just explain what this effect is. They use this in robotics a lot. So in this balloon, it's basically like coffee grounds, or pieces of sand. And what happens is, if you just imagine a balloon full of sand, you can sort of squeeze it like one of those stress balls, right? But if you suck the air out of it, what happens is all those little grits of sand sort of lock together, and it actually becomes rigid. It doesn't become squishy at all. This is why it's called jamming, and it becomes actually, basically like a rock, like a brick. And they've used these a lot in robotic actuators, and if you want to pick up an egg and you're a robot, you sort of deform, you kind of squeeze onto the egg, and then you jam it, and then basically it stays, and it's like a soft brick. And this group, this is at an MIT lab, they are trying to think about ways that you might be able to use this in the future for sort of deformable devices. It's sort of an interesting idea. Techniques into the jamming surface in order to enable human-computer interaction. Usually, the particles inside the jamming will not allow for optical sensing embedded into the device. However, if the surface is deformed, and the surface deformation happens with a structured light depth sensing system built into the table, the screen shows the 3D deformation to create even smaller sensing. A very lightweight pneumatic jamming apparatus shown here. This apparatus does not have any sensing embedded into it, but it enables us to quickly prototype complex jamming-terminable devices, like this mobile phone mock-up here. You can see the mobile phone jam this way, and back to soft, based on the jamming apparatus. To compute this view, we show a short interaction scenario of a small and malleable mobile device putting the phone over its head. This device changes from soft to quick and back to soft to accommodate different usage scenarios. We can imagine, like, maybe you could have, like, it's kind of crazy, obviously this is a long way from being reality, but you could have sort of one, like, malleable computer that's stretchable, and you could have it as, like, a smart watch, and then you can, like, release it, and it, like, just becomes like a ball. You, like, stretch it into your smartphone, or a tablet, or a keyboard, whatever it may be, and it's like a shape-shifting kind of user interface, which is really intriguing. Obviously, we don't have anything like that in terms of, like, PCBs and technologies and screen technologies to really make that work, but it's an interesting sort of vision, nonetheless. I did a very little bit on deforming interfaces. I did sort of, like, an ATN panel, but that wasn't that it could appear and disappear, so help people that are maybe blind, or just improve your sort of performance by giving you tactile targets. Here is a simple display, and we toggle between negative and positive pressure. So, but it was all static, was the most important thing, but it was a little, it was an early project in changeable interfaces. Okay, another really interesting form of output, okay, is using your own muscles, right? So, we don't, you can actually, we control our muscles for output, but it's interesting if you think about giving control of your muscles to a computer, and they can actuate it for you, and this is sort of like, it's almost the opposite of VR. It's like, in VR, you kind of go in, and your muscles control, like, a virtual avatar, but you don't think about sort of, like, the virtual world coming into our reality, so it's more like reality virtualized. I don't know, it's some other pairing of words, where actually the avatar is like you, and you're being controlled, as opposed to the avatar being controlled. So, some really cool work in this space, let me just show you some examples, if I can get any of you to do this, all right? Here's an example on cruise control for pedestrians, so in this case, this guy has a smart phone, and he can actually hear this person in front of him walk around the office. And the way this works is by controlling that user's... So it actually electrocutes your legs so that your muscles tighten up, and so when you're walking, this leg can't move as far, so you sort of tend to walk in circles, right? So you can sort of nudge people in certain directions. And it's painful, and the premise is kind of interesting. I mean, I sort of said it at the beginning. It's sort of like, you know, you're walking to campus as fast as you can to get to DHCS, but you're reading your phone while you're walking down Forbes. And what you do is, because you want to read the latest stories on Gizmodo or Docker or whatever, right? And then you basically, the phone's just like, I can see that you're visually distracted. Do you want me to take over control of your legs? And you're just like, yes. And then we just wait for the stop. And then when the traffic goes, you can cross and stuff like that. It's not that crazy. It's totally possible that you can actually do that. That's basically what they're trying to say. So that's the way. This is what people have done a lot with arms as well. So here's a good example. So this is an appropriate session. So this device can both read your muscle pose and also set your muscle pose. So it's AI. Let's say the computer's been using Skype for a while. It's a progress bar. But the user can still override the computer. So here it's going and moving its arms for him. And with these arms, you can just be like, I'm going to take over control of the computer. So you're still in control. So here's one example of interaction, which is, you know, this guy gave me a professional interview where he was talking to me, like, lecturing to this kid, and there's some videos, like I'm showing you right now, and as the video moves, I know how far I can get the video to my hand. It can go farther and farther, and someone can ask a question, and I can pause the video with my hand or something. It's kind of intriguing. It kind of gets out there, but it is interesting. This, though, I think is more interesting. So this is about conveying affordances, which is, as we talked about, it's telling people how you control that object, right? Okay, so I approach the object. I grab it. Once it's a squeeze, I'll have to hold it more and more with more pressure. So what's happening is the computer's controlling what their muscles are doing, and, you know, before when we talked about affordances, it was probably like a doorknob that was round or a doorknob that had a push plate, and so there's a physical affordance. This is like a digital affordance. So I pick up an object. It's just like a square, and somehow that square is going to tell me how it operates, basically, right? Okay, I think it, yeah, it's turning my hand. It wants me to, I think it wants me to turn it. They're not doing that. The computer's controlling their hands. It doesn't want to turn it. They're trying to touch it. Okay, it doesn't want to be touched. It's like the object wants to be pulled closer. Okay, I'm pretty sure I'm supposed to eat it. Maybe like shaking, like this? Or maybe just shaking? Like this. One, six, zero. Just this motion. One of the objects that they had, I won't show you the whole video, but it's a really intriguing paper. And of course, you know, it's all like totally insane, because who's going to ever walk around with all these contraptions on their arm? But it's really interesting to think that you could have objects that have a digital affordance when they do them. One example that they used that reminded me, because of the shaking motion, is you pick up the spray can, and if you've ever used a spray can, the first thing you're supposed to do is shake it for 30 seconds, right? But unless you read the instructions, people forget, or maybe you haven't used a spray can very often, like spray paint very often, and so you forget. But it'd be pretty interesting is if you grab the spray can, that you basically, whatever, read the QR code, whatever, the RFID tag, you basically start shaking it automatically. You don't even have to think about it. And you start like pre-shaking it automatically, and basically it's like passed the app to your body. It's like an app store for your youth, which is kind of interesting. Let's see if there's another... Yes, you'll never have that problem again in the future. Here's another one, same group, out of Germany. In this case, like a mobile experience. Like, you don't need to have the crappy vibration motors that we have in these devices. You can see it bending the fingers. You can see this is quite painful, actually. It's not the most pleasant sensation being essentially electrocuted. So here they have this little game. It's like a fan kind of thing here. So it's blowing like they're playing to the left. And it's just using your own arm to push against the mobile phone. And you can imagine maybe having the electric like... So I'll also mention, this reminded me of another project that I saw. It wasn't a research project in HCI, but it was a great video that I saw on the web, which was that there was a bunch of Japanese researchers who were trying to solve a problem, and I hope I'm not too, like, stereotypical here. I apologize if you're Japanese. But there was a complaint that husbands were not going out with their wives to dance with their wives. This was after that movie. I forget the name of the movie. Very famous kind of Japanese movie. I think it was the 2000s. It's How We Dance. There's an American remake, too. The Japanese version is much better. But anyway, so what they said is, in a minute, like, I don't want to go dance. I don't know how to dance. Love is like all the practice pieces. And so, basically, the researchers made this outfit that the men would wear, and it basically controlled their bodies. So they basically would get them to do, like, the waltz with their wife. But the funny thing was about this video, the thing that I remember about it, is if you look at the faces of these four guys, they're all twitching, where they're getting like this, and they're like, I don't know what to do. Okay. Yes. There's a similar sort of idea, again, that if you're in VR, and you're lacking those really efficient haptic controls, we don't want to have that gantry of, like, wires, or, like, this ball we have to pull out. Can we use our own body to basically be that mediator? So in this case, it's like a boxing game here, and, you know, when you get, you know, punched and you block it, you're actually using your own muscles as that feedback mechanism. And what they had, what they found in this research is that you can't just use your muscles alone. You often hear that clicking noise, and that is actually a song wig, which is just a electrically controlled wig. Essentially, you needed a little bit of another piece of haptic to make the illusion, right? If your partner did this, you wouldn't feel it. You would just sort of snap on your arm and you push back. It actually feels like you're sort of getting punched lightly on your arm. I'll show you another example here. The other one that I like here is here's soccer. I'm gonna say soccer foot. You can see it in the VR perspective. This is sort of the, you know, balancing and juggling the ball on your feet. And again, you're using your own foot. Here's a ball. It's a ball. And again, the latency is kind of crappy right now. I'm gonna do it all very slowly. Anyway, it gives you an idea. Quite a few minutes left. The thing that I wanted to leave you with before I let you go is, now right now, sort of the state of the art is being able to inject notions into your muscles to be able to control your body. But it's not inconceivable that one day we're gonna have to inject stuff right into your mind. So when you think of a Google search, you're like, oh my gosh, that's totally 1892, obviously. And it's just pre-caching it or somehow stimulating you. We're not there yet. Maybe when I can take a test in another 20 years, we'll be talking about that kind of research. I'll leave you with that thought. And I will see you on Monday the 13th. Good luck with all your other finals. If you need a phone, come talk to me. Otherwise, good luck. I don't know if you have a phone already or not, but I don't know what I'm doing here. Oh my gosh, now you've got me down to the last good phone. Oh my gosh, now you've got me down to the last good phone. Oh my gosh, now you've got me down to the last good phone. Thank you. I am Kevin Lamont. I am Kevin Lamont. I am Kevin Lamont. Yeah, yeah, yeah, yeah. Hello? Yeah, we're all here right now. Yeah, yeah, we're all here. This is Nathan Alfred. Well like this is the 40th anniversary of Nathan Alfred. So you might as well do the song and dance before I do it. Yeah.\",\n",
       " \"Okay, so it's sort of ugly, correct, but there can be tables at the front of the room, and I'm trying to get one table set up so I can get a large room. So, always against the wall, you're going to have the seats on the outside because your participants are going to be sort of facing outwards. So, sort of like fences against the wall, it's going to be on this side. They can sit down easily. You're fine here because it's kind of in front of the TV. You might have to be out a little bit. Try not to be right here. Okay. Thank you for being here. I'm sure most of you are comfortable with your team. And I'll be here. I will be playing my game. As long as I don't have to. I'll be here. You guys can talk to each other. You don't have to worry about it. I'll be here. So... I'll be here. I'll be here. I'll be here. I'll be here. I'll be here. I'll be here. Meet me. Meet me. I'll be there. I'll be there. Meet me. Meet me. I'll be there. I'll be there. Meet me. Meet me. Meet me. I'll be there. Meet me. Meet me. Meet me. Meet me. I'll be there. Meet me. Meet me. Meet me. Meet me. I'll be there. Meet me. Meet me. I'll be there. meet.meet.meet.meet. hour. hour. alarm. now. Hall. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. Now. So you can go ahead and modify your code right now and set numberRepeat equals to 3. That means cheat button is going to appear 3 times for a total of 48 possible checks of that. If you have no idea what I'm talking about, that is worrying me. So at the very top of that code is the random generator. Just set that to numberRepeat equals to 3, okay? The scaffold code already outputs the average time for a click. That's going to be the metric that we're going to be testing against, okay? Average time for a click. The one that incorporates the penalty. Just a reminder, I already went over this a couple of lectures ago, but just as a reminder, there are three sort of primary roles. I think there is one team of four, and you will split the sort of recruiter-recorder role, but for every other team, you're going to have three roles. One is the experimenter. You're the person that's going to really sit with the laptop, run the experiment, or just sit down. You're the one who can explain how it works and demonstrate. The second role is that recruiter-reporter. All the participants are going to come to the middle of the room, and they're going to wear little post-it notes with their team number on them. I'll get to that in a second. And your job is to basically say, oh, I haven't run team 13 person yet. Come back to my table. I want you to run the trial. And you're also going to be the person that's responsible for entering in the Google Docs spreadsheet, which I'm going to provide the link to in a moment. So you'll have your own laptop open, and you'll be the person that's responsible for entering into that spreadsheet. And then finally, every team needs to donate one person who will be in the participant pool. So you need to figure out who that's going to be, and then they're going to come to the middle, and they'll get a name tag. Okay? So take 30 seconds to decide your roles, and then we're going to move on. Okay, so here is the next instruction, if you are a participant, you're going to weigh a little orange pad, you just write like P12 or P4 on here so we know that you're a participant. Data recorders take a blue sticky and you're going to put it on your table in the corner somewhere if you're team 1 or team 3 so I know what team you are. So, I guess the super matter is if you're a participant, you're going to wear one, you can also take one back to your table to denote your team number. This is the number that's on Canvas. Yeah, this is the number that's on Canvas. Okay, from the experiment, let me explain to the experimenters what's going to happen. Your recruiter is going to bring back your table and you're going to get to run them through your experiment and record the time. As I mentioned I think at least twice before, you're allowed to run one trial as a test. So, you're going to have a number you can set to 3, so you're going to get 48 times for you to explain how it all works and then you don't have to finish all 48 names, I'm ready to go, it makes sense. When you turn off that first trial, you don't get any more training problems. You have to go to the actual time trial. If you can't practice for 5 minutes, you're designed that complicated, it's not going to go well anyway. I'm going to record that in average time per click. And I have to... Come on guys, give me your attention for like two more minutes. Then we can get started on this. Okay, so again, the scaffold code automatically computes this for you, but the accuracy threshold is 95%. If you get less than 95% accurate, which in this case, we're doing 48 bugs, it's about two buttons per preview. You don't get penalized. After that, you do stack up this penalization, and I think this code applies a 0.2 second time penalty for every whole percent below 95%. So in general, most of your participants should be highly accurate. If there's some sort of bug in your design and you see people accumulate a bunch of errors, that's going to hurt, and you're just going to have to enter that key into this ranking as it is recorded. Participants, if you see a team sort of cheating in their funding or asking you to do it again, you can just whisper into my ear, I don't know, like Hawkeye them, okay? But again, your grade does not depend on your performance in this, so really, don't cheat. This is meant to be instructive and fun, okay? Okay, finally, here is the Google Doc spreadsheet that you're going to be using. So if you're the data recorder, you should open up this spreadsheet and find your row and column, okay? So I'll show this in a second when I pull it up, but there's going to be rows and there's going to be columns. You're going to put all of your data into the column that's named after your team, and participants are going to be all in one row, okay? And that'll make sense when you open up the spreadsheet, okay? So we're going to get started in one minute. Verify your roles, open up the spreadsheet, and then I'll tell you when we can begin. Two more things that you'll see in this spreadsheet is two of the rows are special participants and that is Judy and Toby, our TAs. I trust them more than any of you, at least right now. I don't mean that in any negative way, but they are an even ten because they have no gray or any skin in the game. So try to preferentially recruit them. If you see them open and they have their names written along their tags, you want to recruit them because you want to try to get both of them to run through your design. If you don't complete every team, that's okay, but try to recruit Judy and Toby if that's possible. Yes, everyone can do it once. Try to get both. If there are any questions I can answer about the bake-off, this will make a lot more sense when we do it once. Judy and Toby. Have any of you done this before? That's okay. Right. Or you can make it practice. You can practice doing it once. Yeah. So they can practice harder. Yeah, that's fine. Okay. Participate. If you want to change the mouse sensitivity, do that during your training round. It doesn't make no one care about that time. You can pause on the 18th button, change the sensitivity all you want, and it will give you it. But once you finish that training round, you have to progress to the timed round. Does that make sense? Okay, so participants come to the middle of the room wearing your name tags. Recruiters, you can start finding your own people. Lots of participants and no recruiters. Lots of participants and no recruiters. Lots of participants and no recruiters. Lots of participants and no recruiters. ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... It's not an actual, it's not an actual hit, it's a recorded hit. If you have multiple hits, it's a record hit. You're talking about like, you know, how many hits are hit? How many hits are recorded? The default scramble code in time with penalty is what you should be entering into this spreadsheet that calculates everything automatically for you. Including the freebies and everything. Okay, I'm going to switch over to the leaderboard. Okay, the data is plugging in. Okay. So remember that you get to run your own participant, which will be the diagonal in the matrix. And maybe there's a problem that passes on your design, so make sure you run your own participant. So yeah, I'll just show you, like, each of them. Okay, so you can just add them out, and then I'll show you how to do it. So this one is a double-clicker, so you can double-click it. And then you can run it. ... ... ... ... ... ... Team 16, you've only got one time on the board. Well, I'm going to claim that, yeah. I think... This might be a piece of... I mean, I didn't believe what you said, I didn't believe what you said. I don't know. I don't know. I don't know. I don't know. So currently the fastest time on the board is .408 seconds, like 50.4 seconds. So the fastest time on the board is .408 seconds. So currently the fastest time on the board is .408 seconds.   I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm just going to be lucky, it's not every day that I'm going to be able to do this. I'm going to try to figure out how to put that line in there, so I can figure out who you are. I'm going to try to figure out how to put that line in there, so I can figure out who you are. I'm going to try to figure out how to put that line in there, so I can figure out who you are. I'm going to try to figure out how to put that line in there, so I can figure out who How are you? Good. Good. How are you? Good. Good. Good. Good. Good. Good. Good. Good. All right. Very good. Now, I think we should start with the next one. So what you're going to do is you're going to put your key where it belongs. And so you have to keep your arm in front of your key, right? So there's two ways of doing it. One is to put it on top of your key. And the other is to put it on the key that you're wearing. So what you're going to do is you're going to have to put your key in front of your key. So you know that you're going to put it on top of your key. And also since the heart is going to be on the bottom of your key, there is a possibility that it might not be on the bottom of your key. So what we need to do is we're going to have to put it on top of your key. and then with this I'm going to... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I'm going to have to do this in the current public, and the next public might be able to do this in the public. I need to get out your Spanish for you. I'm going to be like a yellow collar worker. So I'm going to have the judges to stop me. So I'm going to play. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. Hi. So we're going to do this for about another 10 minutes, and she'll try to infill, and then we're going to run Judy and Toby, and then we're going to go to the lightning round. I need you to have a plan. Oh, thank you. Yeah, but... I can't hear anymore. Oh, okay. Do it again. Thank you. Okay, so... I'm going to be a bit... I'm going to be a bit shy. I don't see the room. Oh, no, no, no. I'm going to be a bit shy. I'm going to be a bit shy. I'm going to be a bit shy. I just do... I don't think that's what I said. Thank you. All right. I'm going to be a bit shy. I'm going to be a bit shy. I'm going to be a bit shy. All right. All right. All right. Oh, yeah, good. What's up, everybody? Hey, what's up? Hey, what's up? Make sure you run your own purchase, your own person will probably be the fastest, so make sure you run your own participant.  I mean, one is to be consistent. Another way is to change. So you can be very remarkable. And you can see that like, even when I'm young, one is to like me, and be like, you can be like me. You can be like me, and be like me. And when I'm young enough to be like, I want to be like me, and I want to be like me. And then, you know, with the kids, like, oh, they're not over it yet, but they can still be like me. I don't know how to teach them. Yeah. Yeah, I mean, yeah. That'd be nice. But then they have to be like me. Then you have to be like me. But even with the mischief. With the mischief. With the mischief. With the mischief. So everyone's got more... How much money do you guys want? Oh, like, real? Oh, okay. Oh, okay. So you think it's like... Also, the number of people who are at work, what happens if... What happens if you have to work like a new or old company, and when that happens, what happens? Okay, just a quick announcement as we're slowly going to wrap this up, is I've marked every team that's in the five with here, and what I want you to do is you can pick anyone on your team, I'm not limiting it to your person, but just run the fastest person on your team, and if you're marked with the word here, you're going to put it to the spreadsheet that time. Okay, so if you're one of those six teams, pick your fastest person and enter the time in there and we'll see who goes to the super final round. For everyone else, you can try to wrap up, we'll probably convert into about... two minutes, maybe grab one more question if you can. I'm not a real child. I can't just do this to break me up. I'm not a real child. I'm not a real child. I'm not a real child. I'm not a real child. If you're one of the teams I put here in the Super Final, some of the best, with your laptop, for the Final Showdown running right here. Okay, there's one person with a laptop, the person that's going to be at you, you're going to come to this table, and take a seat where the chair is, and I will guide you. Please join your competitors in the back row. Everyone else, come on over and we're going to play a game of science with some people. Come on over. Right there. Everybody, come on over, in the back, in the back. Everyone come over. So we're going to play a game of science. I'll hold up the laptop. I'm going to try to see what these designs would like to see. I'll be your holder. Alright, so this flashing light here would be better than what's shown on the actual project. This is the first one. And we increased the sounds with padding, so once you get close enough, those turn gray, so you know you can flex it a little bit. And you can spacebar to flex, unless there's a blue box for the next one. So basically, you're moving, and you can spacebar to flex. And sometimes, if you're on the same box, you have a bird that makes sense to it. Hopefully, it's still good. So the trajectory, this padding, this flashing light, I don't know. So when you hover over the target one, it becomes yellow, and you can click on it, right? And the next one is the button. Oh, excuse me. Yeah. The next one is padding, and people padding. It's not up here, because all the buttons have the same color padding. So that's an enlarged area. What happens if you go up the bottom? If you go up the top? Yeah, that's a hard one to exactly answer. Simple. It's very simple, except this one I noticed that some of the other designs were hitting kind of unusual colors, like current and matte. This one's very simple. They're both red, but the brighter red one is the one you want to follow. And then you have a nice cover that's white for all the other ones. Again, it's simple. They're both red, but the brighter red one is the one you want to follow. So basically, the reason is that if you think about this, every single pixel is a target in both eyes. In terms of number of repeats, I'm going to show that here. I like 10, 10 is a good red one. It's a modest red, so it's a 10. That's 160 pixels. So with 0.5 a second, it's going to be less. And with a minute and 20 seconds, it's going to be more. And with a minute and 20 seconds, it's going to be more. Raise your hands when you're ready to go. You can begin when you're ready. Raise your hands when you're ready. Raise your hands when you're ready. Okay, now we've reversed order of energy, so time with error. So what I want you guys to think about, and we're going to talk about this next lecture, but I want you to reflect on all the designs you saw, especially for this, because we've seen so many of these designs you've had to see before in class here. Think about what worked, what didn't work, what you thought was going to work well in your design, what else didn't work, and we're going to discuss some of the HCI principles that back a lot of the good designs, and why they work better than others, and how you could approach it with whoever had a personal line of code. Until then, have a good rest of your day, and have a good evening class. I have a question. Hi. I was wondering if you could explain your glasses, but I was afraid to ask. Unfortunately, you don't get the glasses. What do you do with that? I don't know. I don't know. You don't have the glasses? I don't know what that means. Oh, I have the glasses. I don't know. Are you able to see? Oh. I was wondering if you could explain your glasses. Oh, yeah. I just had my disability. Yeah. I'm sorry, but I've never seen your glasses. Yeah, I'll get back to you. I have to know. Which of the persons that are looking at the glasses are you? I'm probably the person with the glasses. I don't know. I don't know. I don't know. I'm going to go back to my laptop. It's still basically a computer. Let's tell me the rules of the Recycle Box. So that you can now put stuff into a Recycle Box. Yeah. Yeah, so that's how you make the video. It's not about making a video for it. It's really like, you know, for making the video. Can you update the video now? Because I don't want you to get point-top money. Is there a way you can update the video? I don't know. Um... We all do. I mean... Let's do just a little bit. Just since you guys make a correction. What's going to happen is the gradients are going to change. They're going to see that it's illegal. So, I'll... You're really going to put in the extra work. You can see, like, this was a design, but then we found out it was illegal. Then it's got, like, an extra scene in the video. It's like, you've got quite a lot of time to fix that. I'm going to start looking at it. Oh, my gosh. It doesn't have to be you. Just, like, get it in. And we won't get late. I'll wait. Just email me and the TA. And say, okay, here's our new video concept. And Professor Harrison said it was okay. I'm like, okay, cool. I have to go and send Chris, right? I have to go and ask Michelle. You got it. Thank you. All right. We're going to try to... Scratch that one. So, this is our final design. Oh, wow. Oh, wait. Are they plugged in? No. That doesn't work. That's an outcome. Yep. Is there something to offer here? What? Well, you're welcome. Thank you so much. Thank you. Thank you. You're welcome. Good luck. Thank you. Thank you. You're welcome. Well, you can start with the EGA. Do you see my eyebrows? They're right here. I'm sorry. No, I have to get that part out. No, I know that. I just need this. Do you want to use that one? That one? It's ready. Oh, it's ready? Oh, no. It's still like this, but I think you can do it. So I'm holding it. You can do it. You're welcome. I'll just get your- Oh, I'm sorry. Thank you so much. Can you move these? Oh, I can't. Just a second. Oh, that's fine. I'm sorry. You were so late. This is very slow. I'm sorry. Thank you very much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. I think this one will hold better, this one will hold better. I'm more bullish on that one. You got it? You want it? You got it? Are they doing it on purpose, or who's to say? I don't know what this is. I don't know. This cable is like, so useful. Right here? Yeah. It's really bad. Try to find out. Try to add another cable use. Really, really cheap. This one's pretty loose too, but whatever, we'll just knock on it. Alright, let's try this again. Wait, shit, what? You know what I'm telling you, it's like a tiger, right? Oh, I'm dead. Alright. Why doesn't it default? Yeah. I think it's... I don't know. No, maybe it's actually... Oh, can we do, like, this? Sure. We don't have to do that. So we really just plug it in our computer cable, and it makes it... Is that the VGA cable? Well, let's change the game. Let's change the game. Wait, where am I going to stand on the table? I'm going to get 14. I need 7 of those. I'm going to claim this from Karen. Oh. I'll just stand right here. Yeah, that's fine. Oh, there we go. You need to switch tables. Oh, right. Wait, did you use that to get out? Yes. That's cool. Weird, though. You need to extend it. Is it extending? Not yet. I wonder if I can change this. Can you just connect? Can you just zoom in? Yeah. Just zoom in. It disconnects. There's something wrong with this. Go back and see. Oh, that's pretty cool. This is like how it was doing yesterday. You're trying to make it silent. It's not done yet. There you go. That's cool. Yeah. Oh, yeah. All right. All right. Thank you. Okay. All right. Thank you. All right. All right. All right. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Okay. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Okay. Thank you. Thank you. Thank you. Okay. Thank you. Thank you. And thank you.\",\n",
       " \"I don't know. Do you want to do it? Yeah, I'll do it. All right. All right. All right, let's do it. So, I think we're all set. I'm going to do it. I'm going to do it. I'm going to do it. All right. I'm going to do it. I'm going to do it. Okay. All right. All right. So, we're all set. I'm going to do it. I'm going to do it. I'm going to do it. I'm going to do it. I'm going to do it. I'm going to do it. Okay, let's get rolling. It seems like we are maybe missing a few people frolicking in the wonderful spring weather. So I didn't get to finish the last, like, two minutes of the lecture, so we'll start on that first. This was talking about perception and all sort of the weird quirks of human perception. I had shown you my research on kineticons, or kinetic icons in full, this notion of having sort of an iconographic notion that you can apply to anything, not just application icons, but also drop-down menu items and so on, to get people's attention or help convey an extra dimension that isn't actually graphical, like isn't pictorial. I wanted to give you, I'll show you one other project, which is our work on lights, wind lights. Small point light sources are used as indicators in a very wide variety of devices today, from digital watches, mobile phones, and toasters, to washing machines, desktop computers, and cars. Although exceedingly simple in their output, varying light intensity over time, their design space can be rich. Unfortunately, a survey of contemporary uses reveals that the vocabulary of light behaviors in popular use today is small, unimaginative, and generally ambiguous and weak, often just on, off, or blinking. In this work, we hope to reignite people's imaginations by demonstrating the impression and largely unrealized richness of point lights and their utility in ever more capable modern electronic and computing devices. To achieve this, we looked at current design practice, conducted two online surveys, and ran a design session with a group of interaction designers. This produced a wide variety of new light behaviors and potential informational states devices might want to communicate to users. We then ran a 265 participant study on Amazon's Mechanical Turk, looking to identify what light behaviors conveyed informational states, if any at all. We found that several lights, just a bit more detail, so sort of like that last study, so when we showed people those iconographic motions around this, some like 2,000 participants on Mechanical Turk, we would show them that icon that was like running or waving at you or jumping up and down, and we would just have this and we have this basically fill in the blank, I think it was semi-structured, that basically said what is this icon trying to tell you? Is it trying to notify you of something? Is it telling you that it needs an update? Is it telling you that there's new mail? You know, whatever it may be, fill in the blank, which is a very generic little icon with a plus sign on it. It didn't really have a lot of details. Similar here is that we show people, here we basically brainstormed, we had this affinity diagramming sort of like jam session where we came up with all these different lighting behaviors, like what does like a lighthouse sort of look like? You only have one single LED that you can control brightness, if you have some sort of kind of like a brighter or less brighter, or like a candle flickering, or like a light you go, you know, kind of thing, right? And so we just came up with like 40, 50 odd behaviors, and again, you only have one LED and it was white, and you have this sort of make, make it have expressive output, and then you take them, you show people, and you show them that behavior, and they go, what do you think this is trying to say? If your smartphone embodies that behavior, what is it trying to tell you? That was sort of the idea, and we didn't get enough people, we ran this on Mechanics. We found that several light behaviors are iconic. For example, this behavior produced a strong impression of a notification, while this one generally conveyed to users that the device was turning on. Alternatively, this behavior was seen to suggest a device in a low energy state. Finally, this behavior had strong interpretations of the device actively working on an operation. In total, we explored 24 different light behaviors and found 8 to be particularly strong. Please see our paper for full details. So if you remember back to our lecture on other observational structures, we had this notion of an elicitation study where you show people a light and you get them to tell you what's causing the light. And so for things like that, sort of random blinking, sort of router-like things, people were tending to say, oh, it's like processing an operation. It's working. It's like it isn't falling asleep. It isn't charging. It's like working on something. And again, that set function, if you have it go off to kind of slightly brighter, slightly brighter, slightly brighter, slightly brighter, people are like, oh yeah, it's charging or it's turning on. It'd be very weird to say it's almost out of batteries. So we know sort of instinctually that different lighting behaviors can convey this, even though it's a very simple design space. And what we found in this research was that device makers, Samsung, LG, Google, Apple, routinely violated reasonable design principles. And we actually, after we published this work, I believe LG adopted our entire lighting behavior set as their default LED behavior set across their devices, which is pretty cool. So even playing this, it's a fun little playground to work in. It's a cool little project too, because you're highly constrained and you've got some really actionable stuff. And it shows that all these devices that just blink at you, which is really kind of 85% of what you'd see in the wild, or if you went around seeing and surveyed all the different LED behaviors, they're pretty much all just blinking. And we can do so much better than that. One other behavior that I thought was pretty cool, this is a video, it's not a video, I only grabbed an image of it, but it was just sort of another little kind of perceptual hack where when you hit a menu bar and it fades in, what they do is they just carefully, the ones that you're more likely to go to, in this case, sort of abstracted, it'd be like countries and brands and stuff, but you can imagine when you click a menu bar and you can guess that they're gonna go to coffee, or bold, or use a pivot table in Microsoft itself, they make those fade in just ahead of the other items, and your eyes are naturally drawn, and this is sort of a way to hack the Hick-Hyman law, where it's an even choice, but if you have them just fade in ever so slightly ahead of other items, you can actually get them to decrease the time to select, and again, it does mean that you have to, like currently, I think when you open up a menu bar and select Mac or Windows, it's all popping at the same time, so really what you're doing here is you're actually delaying information by maybe 100, 200 milliseconds, but they're able to show that if you do this, that you can actually draw the eye, the items that are more likely, and if you think about it, if I have a column highlighted in Excel, there's definitely gonna, you know, there's like, how many menu items are available in Excel at any given moment? Probably 100, if you think about it. There's probably seven menus with about 12 to 13 items in it. That's a lot of options, but some, there's probably only five that are gonna be highlighted. I probably wanna do a sort, or maybe a conditional format, and again, you can imagine just doing a little perceptual trick like this to draw your eye. So, kind of interesting. Just wanted to give you two extra examples there of applications in HCL. Today, we're gonna talk about two things. One, there are some chairs over here. We're gonna talk about time and perception, so sort of closing a little bit of a loop on humans, talk a little bit more about human perception of time, and we're gonna talk about Bake Off 3 in your new groups. So, a good place to start off with time is talking about frame rate. So, who can tell me what frame rate is? Oh, number of times per view. Okay. And so what are common frame rates? 60 is a common one. What else? 30? Okay, what else? 144. Is that a common one? It's like a new trend. YouTube videos are 144. Anyone else have a common number in film maybe? 24? So why those numbers? Like why 60? Why 30? Why 29.97? Why 24? Yeah? That is definitely part of it. Why not 10? Yeah, so it's all related. So what they find is that if you have something that's anything under about 1 50th of a second, you actually start to see them as individual flashes, right? And so you actually need to be running at around 50-ish frames per second. Otherwise, you start to see all these, you start to see not continuous motion, but rather kind of flashing and flashing and flashing and flashing. And if you watch like old-timey kind of shows, well I guess I'll show you. So generally what you see for computer monitors is they refresh at 60 hertz. Now if it doesn't explain 30 hertz or 24 hertz, we'll get to that in a second. And that's because if you really clock your monitors like into the 40 hertz, you'd actually see it all, you'd see all of it just flashing at you and not actually animate at all. And it's especially true in your peripheral, when you talk about the distribution of rotten cones, the rods are really sensitive to motion. And actually sometimes like these... fluorescent light that will start to get old, it will start to flash, and you'll often be able to see it out of the corner of your eye and you'll sort of see it flashing, but if you stare directly at it using your cones, you actually can't see it flashing. So our sensitivity of motion is actually not the same across our entire field of view. So initially, you know, film was really expensive to develop. So the origin of frame rate was that, you know, they wanted to basically get the minimum viable product using the least amount of film, but this was so expensive for the term of the century. And so most kind of silent films in the silent era were shot at 16 frames per second, and then eventually they moved to sort of the talkies era, sort of post-Charlie Chaplin, which we talked about in the beginning, moved to 24 frames per second. And the problem with both of these is that they're both below that 15 frames per second threshold where you start to see flashes, and this is why, as I was mentioning just in the previous slide, like if you ever see people try to stylize old movies, like if you watch like Man in the High Castle, for example, and they have the old sort of, you know, tape reels, and you often see that sort of like flickering kind of like, you know, you kind of hear and you see like images sort of flickering on the screen, and that's because in the old era, it was really right at that threshold. But they still needed to solve this problem of, you know, 16, all you see is basically flashes, it wouldn't look correct. And so they had to do a very particular trick to make this look correct. And the way they did that was by frame doubling. And so, in early cameras, what they had is, they would actually have a wheel that spanned in front of the image, and they would actually show the same image twice. So they'd move the flicker amount, they'd actually double, they'd actually put less image on the screen, if you think about this, in terms of absolute brightness. So you're actually reducing the effective kind of brightness of the thing because they're blocking it twice as many times. But this had the effect of actually flashing it at twice the frequency of the double shutter. And you'd actually, it would actually hide the flashes. Even though there's more flashes on an absolute scale, it actually caused it to hide. kind of a trick, but it worked really well. For early, kind of, silent films where you had only 6 FPS, they'd often use triple or quadruple shutters to hide this, which is kind of interesting. Okay, so that, this is, so this is why we really have like 16 and 24. 24, again, even when you double it though, like 48, it's still like right on that edge, and sometimes you do see that effect with kind of old movies, that you sort of do see that kind of like a flickering, right on the edge of your perceptual threshold. So now early TVs came along, right, so this is now, I'm just sort of like the late 30s, 40s, 50s, 60s, and sort of like the color TV in sort of the 70s, and these were all analog, and so they needed a way to have reliable oscillations to basically lock onto the signal. And the way they did this is by using the electrical power lines. So rather than having expensive analog electronics inside the TV set alone, is they used the electrical power, which in the United States is 60 hertz, 120 volts, and they would use that to basically kind of phase lock loop of a signal. And so this is why we get actually 60 frames per second, instead of actually the power line frequency. So it wasn't anything chosen, anything specifically for 60, other than it was above the kind of fusion, flicker fusion threshold, and it was a convenient thing to like basically lock our signals onto. In Europe, they actually use 50 hertz power, 240 volts at a 50 hertz, and the power format is actually a 50 frames per second, which is kind of interesting. Of course, it was really expensive to do, 60 times per second, and it couldn't actually have the electronic switching up, and so this is where we get this interlaced scan, which happens is it basically skips every other line, so there's one raster scanning on the way down, and one raster scanning on the way up, and so you do get sort of two full passes at 60 frames a second, but they're really only half passes, so in reality, you only get one full pass every 30 times a second, but again, it helps to break up that kind of flickering. But the crazy thing was, if you look back at the early kind of broadcast, like 40s, 50s, in the 50s, 60s, is there's no way to store all this media. If you wanted to film a TV show, you filmed it on chemical media. There was no hard drives or even magnetic tape to store it on. And so pretty much all of the early era, like 20, maybe two or three decades of the early kind of television era, was just broadcast live. And what that meant is that you'd have a camera, and the camera would have basically like a little kind of gun that would scan the brightness of the intensity, and it would literally go right out onto the antenna and then to your TV set, and then the TV set would just basically render the brightness again as like a little dot moving along a CRT. And there was no, nothing was stored. It was all this analog transmission, analog broadcast, and then it was lost forever, I guess, radiating out into space. But they'd have to record it to film if they wanted to record like early TV shows. And so it's pretty incredible, but it did mean that we lost like 20 years of early TV. So now, because we're sort of set at that 60-hertz kind of notion, and again, the reason why we see a lot of digital video filmed at 30 is because they're meant to broadcast, for example, like VHS tapes are meant to run on TVs, and so they use it as a multiple of 60, and they get in 30. It's very hard to get 24 frames per second on like a CRT, like old school TV. But as technology has progressed, now we can go from 60 to 120, to 40, and again, you want to pick multiple, so you have to have these weird kind of frames that are in between. So modern TVs are probably doing like 120 or 240, or even 480 and beyond. So when I mentioned this before, this flicker fusion threshold, again, there's some debate about exactly how this effect persists, and people are still researching it, even though it's been debated for probably over a century. Essentially, it's when the intermittent light stimulus, like flashing, appears to be continuously steady, and you do not see the flickers. And that's why it's this fusion threshold, is that you can actually have that little kind of knob where there's like flash, flash, flash, once per second, and then you increase the rate. And you ask people, you know, when do you not see it anymore, essentially? And that happens around about 50 times per second. And it's related to this... somewhat debated notion of persistence of vision, which is sort of that trick that I showed you with the Eiffel Tower. But even if I show you something for just a fraction of a second, 10 milliseconds, it doesn't perceptually sort of live on your retina for that 10 milliseconds. But there's this notion that if I flash something to you for just one-tenth of a second, it sort of almost appears to your brain that it was there for like a half a second, so you can digest it. And they call this persistence of vision, like it's actually persisting. In the eye of your mind, it is persisting for longer than it really is. And that's why you don't see these flashes, is that previous image in the darkness. So you know, flashes are, it goes image, then dark in the cinema, then image, then dark, is you basically don't see the darkness because like the old image is sort of sitting on your eye. Again, it's a nice idea, but perceptually it actually doesn't seem, or like cognitively it doesn't seem to really work quite that seamlessly. Anyway, so there's some cool things that we can do with this to kind of hack and make interfaces a little bit better. Here's a nice example from NASA. So I'm Brent Theodore. I'm Assistant Division Chief of the Human Systems Integration Division here at NASA Ames. They came to us because they wanted to know the impact of the vibration on human performance. What we focused on initially was measuring visual performance. If you're going to fly the vehicle, you've got to be able to see the displays and controls and read what's on them to understand what's happening in the vehicle. And so they wanted to find techniques to mitigate that vibration, and they had a couple clever ideas about how they could either passively damp it with things like springs or actively damp it with motors that would sort of fire anti-synchronously with it, and then it would get rid of the vibrations. And so that was projected to cost many hundreds of millions of dollars. We can use our map. So just to clarify, so they're talking about building displays for like astronauts, right? So you're like on the shuttle, if you're not on the shuttle anymore, whatever. You're on some sort of like Falcon 9 Heavy. You're going off. Now it's all glass cockpit, because again, these missions have to be reusable. like going to Mars, you have to have interfaces that are totally reusable so they're used for launch, then they're used for like travel, then when they land on Mars they have to basically repurpose all the interfaces. There's a great HCI team working at NASA that's actually probably 80% of the HCI people at NASA. And they're basically building all these interfaces. Now the problem is when you're launching or when you're in kind of high turbulence environments is that all the things are shaking, right? So like you know you're like in the rocket, like if you watch like Apollo 13, all the displays are shaking. And the problem is it's really hard to see. It all basically blurs, kind of smears all over your vision. And so the two proposals that they had were basically to have some sort of on springs, like so the chair is on springs and the displays are on springs, everyone sort of just bouncing around sort of lazily in this high-vibration environment. Or what you do is you do this active dampening where every, having like a really high-core accelerometer, every kind of bump up is you kind of counter fire like little motors to basically keep it stable, sort of like a steady cam. But of course like imagine mounting all these displays on like kind of steady cam rigs that are super, you know, can survive space travel, super, super expensive. So like okay, can we basically use a perceptual act to do this? Hundreds of millions of dollars. We can use our knowledge to actually think about how we might solve the problem. And with a little bit of thought we were able to do that and come up with what I think is a pretty clever, cost-effective solution. The technique we developed was a strobing technique, so the display flashes on and off very quickly. It basically freezes the image and you can see it clearly. Here's the recorder. I can still read all the rows down the smallest. And they're vibrating. Okay, yeah, I can still see the smallest row. Okay, so I still, yeah, it's way tougher to read. I still see the three, six, eight. Oh wow, so much clearer. Wow. Yeah, three, six, eight, three, got it. So again, what they're doing is they're just timing it, they're figuring out the vibration of the motion, and they're just flashing it basically at the top point. So even though, again, it's only on for a very small part of that thing, is that, again, because of persistence of vision, is that it's basically as long as you can flash it at that one moment, it'll always superimpose onto that person that always appears to be kind of stable. It's a very kind of clever effect. So with respect to sort of timing, because again, sort of what we saw in the model human processor, the perceptual cycle time is around 100 milliseconds, so anything under 100 milliseconds essentially feels like an instant drop. If I hit save, and that dialogue closes in 100 milliseconds, it feels essentially instant. It's really hard to make it feel that much more snappy. And so do reactions tend not to need to be much faster than that 100 milliseconds, but perceptually, you can't really tell the difference. But it does fail for animation. If you have to go more than five seconds, you probably want to use something like a progress marker. If you click save, and it's just like five seconds elapses, then things start feeling kind of weird, like you think your computer has crashed. But animation is where it really fails. So for discrete actions, like opening a window, closing a window, that's fine. If you do something for a longer delay, like one to five seconds, you really should put something on the screen indicating that it's processing. Indeterminate progress bars are pretty good, but you can also use an actual progress bar. What you find is if you look at when people start getting anxious, like if you were to monitor, for example, their brain waves or their galvanic skin response or some sort of a notion of affect, if people start to get really weird about their computer, right when the human turn-taking protocol gets strange. So humans, when you're having a conversation with a friend, the main cause for an awkward amount of time is if I just all of a sudden go quiet for five seconds here. It starts getting a little weird, like if you just had that in the middle of a conversation, I couldn't even hold it any longer. It actually breaks it. There's a very natural cue, like something that we've learned over time is sort of conversational turn-taking. And when you break that, it feels strange in the conversation. And actually, I would say that most of the sort of smart speakers right now, like when I ask like Alexa or Google something, I'm like, hey Google, what's the weather? And if it has to, I guess, reconnect to Wi-Fi, it'll just be like... I think it didn't hear me. And then I'll be like... And then I'm asking it again, and I'll be like, oh, the weather in Pittsburgh is 72 degrees today. How many people have experienced that with a smart speaker? Like the timing is a little bit off. So again, when you break it, it feels kind of strange. And actually, that error, kind of that threshold is right around human perception principles. But if you have continuous actions, like dragging an icon, you really need to be more like under 10 milliseconds for it to feel real. So here's a great example of a touch screen that Microsoft has been working on that sort of illustrates this effect. So currently, touch systems have about 100 milliseconds of delay between when you touch and when the image actually changes. Well, if you're moving a finger at about 1 meter per second, that means that the image is going to be 100 milliseconds behind or about 10 centimeters behind. That's really obvious. And so this analogy that you're trying to draw of moving a real physical object really breaks down because the object is falling so far behind. So we built a test system in order to understand how much better the experience would be as we decrease that latency. We used our test setup to examine different latencies. So here you can see that this is about 100 milliseconds latency. This is like cheap Android tablet. The delay is still very clear. Now, that's about the level of current devices. We moved this down to 50 milliseconds. So this is definitely better than most devices on the market, but it's still quite a bit of lag, even at 100 milliseconds. these slow speeds. Now, if we move that down, yet again, to 10 milliseconds, you can see it's dramatically better in each of these steps. I'm using a little stranger, because none of you have heard of that. But it's not really staying on the finger. It's kind of rubber banding around. So if we move all the way down to one millisecond, then you can see now it's really staying on the finger. And in fact, if you were playing with the real setup, you would notice a real perceptual flip, that this really starts to feel like a real physical option. So I got to play with this. This is actually a pretty old research project, 2012. But I got to play with this setup. The problem is that it's not really a real tablet. Like, this is like a resistive touch screen that goes to an FPGA, that goes to a projector, that they act and all of you render like black and white squares. So it's like, you know, it's not like it's running a whole operating system or anything like that. But it does give you this perceptual effect. But what's really interesting when you get down to about that one millisecond, and when you look at this, it's almost feel like CG. You've never, ever touched, you were just basically just deep learning on legs. Like, you've never seen a touch screen that looks like this. So it should feel a little bit strange. It should almost be a little bit fake when you're watching it. And that's because you've never seen any training data like that. But when I got to experience this, what happens is when you get down to one millisecond, it actually feels different. Not like visually, but actually tactically feels different. It feels like I was almost sliding around a little square of paper. Because there was essentially no latency that my perceptual system could pick up. And it actually modified my sense of touch as well. It kind of, your brain, again, is doing a lot of sort of like upscaling. And it's changing what things you touch. It's part of this sort of like fusion of all these different things. I know that this is like a plastic kind of like cover. And I actually felt this very distinctly when I was using this touch screen between like the 15 milliseconds and one millisecond. It actually was modifying my sense of touch. Very, very cool. Okay. If you take a look at a current tablet, you can see the performance, as I said, is about 100 milliseconds. And this is most noticeable if you draw like a squiggly line on it. And you can see that in this example. And in fact, the line that is being traced at the ink is about a half cycle behind. Now, if you use our device and you take that down to about a millisecond where you can see, then of course that effect goes away. And it's almost like a finger painting. It feels like the ink is coming from the tip of your finger. And it's perceptually very, very different. Just to give you some better idea of what's going on here, we shot some high-speed footage. So this is slowed down by a factor of 8. So you can see the difference here. At 1 millisecond latency, you're right on it. And again, it appears quite noticeable when you're doing squiggly lines. And you can tell just how far behind you are. Of course, if you go down to 1 millisecond, then it feels like you're right on top of it. So what we've seen is that the current levels of latency... So it turns out, unfortunately, this is a really hard problem to solve. They used to produce this thing called TouchMark, but they basically stopped doing it after iPhone 5. But the distribution of how responsive touchscreens were used to be an actual selection factor. Now, most of them have gotten a lot better. They've sort of been more like that 15-millisecond range. It's very, very hard to get down really much below 30 milliseconds. And that's because the touch stack is really hot. So, you know, they're really long. So the touch panel typically runs like on... You can take any of your modern smartphones or kind of like whatever, Surface Books, whatever it may be. The touchscreen, for various performance reasons, like energy reasons as well, tends to run at about 60 frames per second. And so that's about, what, 33 milliseconds? What's 1,000 divided by 60? I think it's about 30. 15 milliseconds, right? So between individual flashes, you have 15, right? So 15 milliseconds. So if you touch down, on average, you'll touch down in the middle of the frame. But you're just going to have to wait at least 7 milliseconds before the touchscreen picks up at your finger. If you happen to land on time index 1, you'll have to wait 14 milliseconds. But on average, you're going to have to wait about 7. millisecond. If the touchscreen likes to collect two examples of your finger before producing a position estimate and they have to wait yet another 15 milliseconds. This is really hard to get down a fast unless you super clock up your touchscreens. So let's just say you only have to wait like 10 milliseconds. Well then you have to push some of the touchscreens up through whatever SPI or I2C to the application processor, up through the kernel, then you have to get to the event manager of Android, and then finally you make it to the application. But at this point you might already be about 20-30 milliseconds in before the application even knows that a finger is touched the screen. And then the application has to have some logic, like oh they clicked the print button, well now I need to like package this up as a PDF and like spool it off the printer, whatever it needs to be, and I'm going to turn the print button green or load some icon that says your finger is now printing. So you're going to push this down to the graphics framework. This can also take, now they're saying about 40 milliseconds, even if you're conservative it says you can do all that processing in like 10 milliseconds. Now you're up to like 20-30 milliseconds total. Then again you push it back down to the screen, you have to write all those graphics to the screen, and then your screen even if that is running at something like 60 frames per second, then again you know even after you push this double buffer, so you push that into one of the buffers and you're waiting for the screen to update, even if again you're sort of on average 7 milliseconds until the next redraw, now your total stack size is something on the order of like 30 milliseconds. And even if you really have to optimize all these things to get that to work well, but it's really really really difficult to get it down to something like 1 millisecond. Like basically every, there's no, there's not even really a single set in here that only takes 1 millisecond, so to get that all down to 1 millisecond is going to be exceptionally difficult. You probably have to move all both of these buffers up to like 1000 FPS. So that means your average latency is only half a millisecond, so those two together alone would be a millisecond, not including any actual CPU processing. So it's tough. We've got a way to go on that, but it would be cool. Okay, so here's a hypothetical for you, for our next topic. So let's say you have these two options, okay? computers. One is a computer that is really fast. It's like 20 cores, like 8 gigahertz, it's just like unbelievably fast. But for whatever reason, like when you're doing your homework on it, it just feels so slow. Like you open up the app and it's like loading, loading, loading, and it feels perceptually slow. But you get your homework done like an hour as opposed to three hours, okay. On the flip side is you have a computer that actually is really slow, right. It takes you six hours to do your homework instead of two or three. But for whatever reason, it's just like, boom, it's like everything you put is just there instantly, okay. What computer would you want? So how many people, how many people would want computer number one? The one that feels like brutal, but it's just your homework system, that's where you know it. Well, not really. It feels, it feels like it took all night, but it really took an hour. How many people want computer number two? Really? You're the slave over there, it's four in the morning before you go to bed, but it's felt so fast? Yeah? That's how, how people will tell you it's going to take two hours for something to finish downloading, and then it only takes 20 minutes. If they told you to start, that it was going to take 20 minutes, you'd kind of be frustrated. But when it's faster than you're expecting it to take 20 minutes, you're like, how slow is this? But that's, like, I don't, I think I would say number one, but then in reality, I'd actually be happier with number two. Yeah, what's a complicated question? Yeah? I've got a slow computer, and it's just brutal, going through all these orders, and it's feeling super slow, and it tries to do that all the time. Yeah, yeah, well, it's not to be underestimated, like, what the user, kind of, perceived, kind of, like, speed is. So, you know, luckily, this isn't purely hypothetical, because we don't need to have a computer that's actually slow. We can have a computer that feels fast, and is fast simultaneously, maybe even feels even faster. So, just again, some of my own research in this area is, we looked at progress bars. Progress bars are lovely little playgrounds, sort of like Quinn Lake. where we can manipulate their behavior and see how people react to them. And progress bars are sort of the canonical waiting on operation potential. So I'll tell you about my work on two of these. So here, what I'm going to do is I'm going to show you a screen with two progress bars, okay? I want you to stare at them, and I want you to tell me if one was longer than the other, or if they were equal in speed. So here, look at these progress bars. Okay, so how many people thought this one, the top one, was faster? How many people thought the second one was faster? How many people thought they were equal in speed? Okay, pretty good. So the reality is, and most of you guessed it right, they are equal in speed. Here are all the progress bar behaviors that we tried. And what you'll notice is they all start at the same time, and they all end at the same time, but how they get there varies, okay? So some have lots of pauses. Some have some accelerate. This one starts off really fast, and then as you get close to the end, it starts to go slower and slower until it's like painfully slow. Here's one that had lots of stops. There's one, this one at the top here, starts off exceedingly slow, and then it basically just accelerates, okay? So again, we made this little playground. We sort of ideated. We came up with all these different behaviors, lots of starts, slow at the beginning, fast at the end, et cetera, and then we basically brute force tested all these in a big sort of design study. And what we found, very interestingly, is that there is a strong effect. Even though they were all equal, they were all equal, right? In the interface, we could say if A faster than B faster, it was equal. And the correct answer would have been they're always equal. But the reality was that people had very strong preferences. Actually, most people did not say they were equal. Most people actually said A or B was wrong. So we had to very carefully counterbalance the design. We put A, we put this design first in some cases, and so on, to make sure that there was no novelty of actual ordering. effects going on. But what we found is that people generally like to look at the end of progress bars, they basically remember the experience looking at the end of things, and they really hate pauses. Any time there was a pause in the behavior at all, people rated it as being slower, even though they're all the same. Now we followed this up with a second project, similar type of effects. I'll let this guy narrate. Progress bars are typically used to monitor web downloads. These examples all take the same time to complete, but some of them seem to move faster than others. Here, pulses in the top bar become slower as it progresses. In the bottom bar, they become more frequent, which creates the illusion that it's moving faster. Other bars are filled with ripples moving left or right. Tests have shown that ripples heading left make a progress bar appear to move faster. By using an effective illusion, it can seem like a file is downloading 11% quicker than it really is. So, in this experiment, when we did it, when we followed this up, we said, our old methodology, we got basically like a directed graph of order, like A is faster than B, and B is faster than C, and B is, you know, basically we got like an order, but we didn't know how strong the perceptual effect was. So in the second paper, we got a bit more sophisticated, and every single time they put that A is faster or B is faster or equal speed, is we put it back into the queue with a slightly manipulated speed. So if someone said A was faster, whatever, the top one was faster, we'd add a little bit of time on it. So instead of being 5 seconds, it'd be more like 5.1 seconds, right? So now when you compare it again, you're hoping that they say they're equal because we basically added on time to that, the one that felt like it was shorter in duration. And they did this enough that basically it converged after like 4 or 5 cycles, where they were basically saying, everything feels like it's equal in time, but in reality, we manipulated them by little versus a little. the very best behavior compared to just a traditional kind of linear progress bar was about 5.5 seconds. They said that was as fast as a traditional linear progress bar does 5 seconds, or like 5.53, whatever 11% is. And so it is able to manipulate people's perception of time just by changing the graphical design of these icons, which is a really interesting result. Interesting enough that the New York Times did a great piece on it, and it turns out the only two people in the world that are experts on progress bars are me and Brad Myers, who wrote that paper that I mentioned in the beginning of the semester, who did the very first paper on progress bars. So it's not esoteric enough for the New York Times to report on it. So here's a really interesting study that we found after we did this research. So imagine that you're in this medical trial. You go to a pit for some sort of weird medical experiment, okay? And they have two different procedures that you have to go through, okay? In trial A, if you dump your hand into icy cold water, 14 degrees Celsius water, and you hold it in this sort of icy cold water for one minute. And it's not that it's like, you know, it had to pass IRB, so it isn't like it's going to kill you, but it wasn't very comfortable. Holding your hand in ice cold water for a minute was not very comfortable. And then in trial B, you got to dunk it in the same icy cold water for 60 seconds, but you had a bonus of 30 seconds of slightly less painful but still painful water, okay? And the procedure was as follows. Participants came in and they randomly said you're going to do A, and then B. And then the other half of the participants got B, and then A. But all participants got to try both of these. And again, it's painful for 60 seconds, but painful for 60 seconds, but slightly less painful for 30 seconds, okay? At the end of this, they said, hey, you've been like randomly selected, and you unfortunately need to repeat the experiment again. But you can pick. Do you want to do A, or do you want to do B, okay? So imagine that you're in there, and given this data that I've given you, what would you want to repeat? How many people would want to repeat A? How many people would want to repeat B? So someone who voted for A, why would you want to repeat A? Okay, yeah, I agree. How many people, how about for B? How many people want to vote for B, yeah? I thought that comparatively, the uncomfortable R might seem comfortable after being painful, so that should be more of a relief. So I can say that unfortunately, it was still the painful, that was a bummer. So what's really interesting about this is that actually most people did select B, but not because it was kind of comfortable after all that pain. It was still quite painful. I probably should change this from uncomfortable to be like, why'd you write painful to make it more clear? But basically, this one was just, if you summed up the pain, it was just pure, total, more pain, okay? And what they found is that, so when we dug into this literature, it turns out there's this effect in the medical community called peak and end effects, and it perfectly matched our experimental data from progress bars. And that is that people tend to judge experiences pretty much entirely on two things, the peak of an experience and the end of an experience. So when you're thinking about a recent movie that you saw, your brain is not remembering the whole thing. You don't have some sort of crazy moving average over time. You tend to compress experiences on a dinner you might have had at a good restaurant. It's gonna be the best thing you tasted and whatever they gave you last, like dessert, stuff like that. Or in a movie, it's like the awesome action sequence like halfway through the movie, and then the end. And if the end was a real stinker, then it's actually gonna degrade your movie experience. Or maybe the intro, you barely remember at all. And this turns out to be a way that humans cache experiences, because you have so many experiences. Like how good was your class freshman year? You probably remember like that really awesome lecture, and then like that final, and then that's it. You know what I mean? You cache those two things together, and that becomes basically your sort of cache of what experiences are like. So it's not based, and people have often thought about this, like psychologists and so on, like how do people characterize experiences? And it's not the total sum, it's like moving. average or aggregate or like integral under the total funness of an experience, it really turned out to be the peak and the end. And they tested this on many different things, there was people, one of the experiments was about like colonoscopies, I think, or maybe some sort of like broken bones, and it turned out that the most painful point of the surgery dominated over the total summation, like even though the procedure might have taken 10 hours, it had like 9 out of 10 pain, that was considered to be less painful than like the 30 minute procedure, it was 10 out of 10 for like only like one minute of it, and they see this repeated in many different types of surgeries, which is quite an interesting effect. So I linked this on paper here if you want to learn more about it, but if you think about that again, if you're trying to design a customer experience, you want to think about the peak and the end. Going back to your example earlier, when you're installing a piece of software, how many people have seen an installer? Like you're installing like Creative Cloud or like a new version of macOS, and it's like cranking through the install, and it pauses at 99%, and you're just like, what is happening? Like, am I out of disk space? Did like the thing crash? The only thing an installer should never do is pause at 99%, like you just bought like a subscription to this really expensive software package, like super excited user, you're watching this progress bar go by, you're like, oh my god, it's at 99%, like I'm ready to use it, and it's just like sits there for five minutes at 99%. It's terrible, terrible design. It can pause for 20 minutes at 1%, but it should never pause at 99%, which is interesting. Yeah? Have you had progress bars ever actually match their progress? Or is it just a... You know, I don't even know what progress is anymore. It's just such a set skip in the world. So most progress bars, if you ask a typical computer science, like computer science is like, what's the best progress bar? It's the one that's like perfectly linear, right? It's like I have a thousand files installed, and so if I've installed 100, it's like 1,000 and it pauses at 10%. Or, you know, whatever, total amount of data to be transmitted. I think that's assuming that humans are linear creatures, and that's how we want it to be, but I think people know. that humans want to be a little bit lied to, they understand how lies happen. Like, oh, you look so great today, that's amazing, you know? Like, you have to, sometimes you fluff it a little bit, and sometimes, because we know that we argue that our time reception is non-linear, that what we consider to be linear actually feels slow to us. And so I think if you ask most humans, or if you could find out in a more formative way than in our studies, is if you could actually tweak it a little bit to be better. But, you know, rationally, like a perfectly linear progress bar is the way to go, but we're not rational, unfortunately, so. So I think we should lie, and I think, actually, most interfaces shouldn't really lie. And what I propose in that paper is, it'd be great if operating systems like Mac OS and Windows, you could just instantiate two progress bars as a widget. There's like perceptual progress bars, and there's progress bars. And it's the same widget, and you can still say, I'm 50% done, but the perceptual progress bar might only render it at like 45. And since we know things like pauses are really detrimental to the experience, like you're downloading something, or you're installing something in it, any time a pause happens, we know, ends up being this peak, this sort of most intense and most salient point in the experience, is you basically just keep on trucking, no matter what, even if you have actually stopped. And eventually, you may have to sort of lie, because like you've showed it rendering beyond, but you really are stuck, or the download speeds have just taken a hit because you've moved the cellular or something. So at some point, it becomes like a really bad lie. And so, yeah, it's complicated. That's why there's more research to be done. But I feel like someone should go to Apple and do this for real. It'd be interesting. Okay, speaking of peak and end effects, let's do a quick quiz. And then we'll talk about the bake-off.  I Okay, 30 seconds, looks like most people have done this. Okay, how many people have done it, raise your hand if you've done it, okay, 5 seconds. Okay, start to coalesce those for me please. Do we have class on Thursday? Lucky you. Yes, I have them all, yes, yes, yes, final call. Okay, very quickly, before we move on to the bake-off, retina resolution on a smartphone generally means, it should be E, the screen has to be smaller than the eye can see. that D. Okay, this one's harder. Select all that apply. Two, our visual system is excellent at helping people put recognizing shapes and colors in parallel. Yes, those are all those examples of like, you know, like red boxes that were tilted and blue boxes. You can see those instantly. We're really good at doing that. Okay, how about recognizing the entire box of text? Nope, we're not good at that. That's the thing, like, I feel a sample example, like if I just slap you the entire paragraph. You don't ingest it in parallel. You ingest it serially. You have to read it really to understand. You don't just recognize the whole thing. Okay, unless you're amazing. If you can do that, I will give you an opportunity to redeem that point on the test. You can demonstrate it to me. I'll like flash my paper. You can like tell me what the F is. Okay, recognizing orientations of objects. I would say that's definitely perceptual strength. Determining the order and stacking of objects. Yes, I would say so. So like the kind of pizza boxes. Like if you have, I put all these papers kind of loosey-goosey on the table, you'd be able to very reliably tell me the order of these. Humans are great at that. Determining the quantity of objects and values below five. Yeah, this is a four and a half rule. Determining the, putting colors of different intensities into order. Yes, we're good at that. That's like really red, a little bit red, slightly red. And then finally putting colors of different hues into different order. No, we're not good at ordering like yellow ahead of brown ahead of purple. So there you go. Okay, how many people, okay, most people have guessed what is behind the box to the right. This is just my test to see if there's any aliens in the class. Oh no, okay. A is the one they cross directly behind. Yes, you don't know. Aliens would have put B or C. Okay, sorry, sorry. Did anyone put B or C? You put C? I have to make a call to the CIA. Okay, but the question is not what you would put, it's what most people would put. You're just outing yourself, man. Okay. I'm keeping an eye on you. Okay, change blindness is a tough one. I'm going to be a bit tricky about how I word this. Change blindness is a perceptual phenomenon that occurs. So, how many people put A, when a change in the visual stimulus occurs and the observer does not notice it? This is the best answer. It's not quite how we discussed in class, but this is the best answer. That's it. So, when an officer in the office moves, but the person doesn't notice, it's not really that very specific. When a user becomes temporarily blinded due to some acceleration, we didn't talk about that at all, so that shouldn't have rung a bell. The user does not notice a change because the difference is very subtle. This is probably the next best answer, but in those examples, it wasn't a subtle change. It wasn't like three pixels were moving in the corner, like the whole bar was moving in the background, right? So, it doesn't have to be a subtle change, like that whole street changed, right? That's not subtle. That's gigantic. Every pixel in that video changed, and yet we are still blind to those changes. A is by far the best answer. Okay, recognizing the little dots when moving are humans is an example of biological motion or strength in biological motion. Turn-off faces are those weird faces that you can manipulate all the eyebrows and eye size and mouth size, and so that's a method for visualizing multi-area data. That's it. Back a couple lectures, so camera probes. This is the one example I gave in class with that study about homeless people in Atlanta. They gave them a disposable camera, and they photographed all the things in their day, and then later they had an interview, and so that is using a camera to document objects and situations of interest, which can be stuff that are related to time and greater depth of see. Some people would see. And then finally, eight, in a study in which a message periodically pops up on your smartphone asking, how happy are you right now, you know, one through five. That's an example of how many people put experience sampling? How many people put life blogging? How many people put diary studies? How many people put delicious pastries studies? is experience sampling. You're sampling their experience throughout the day. Life logging is one of the automatic methods, like a 50. So it's fully automatic, there's no intervention. Diary studies, like at the end of the day, or after a trigger, like you're thinking about food, they fill out like a diary, so that's very different from just a single question. And elicitation studies, when you're asked to perform a gesture or tell you what's the voice command in terms of the volume of your car. It's definitely not elicited through experience sampling. You guys need to review. You guys are gonna be really upset if on that final bake-off day, I'm just like, and here's the final ones, all the pop quizzes and the continuum, and you have to do them all again. I might just beat you, but I'm not gonna do that. Okay, bake-off three. I wanted to end a little bit earlier today. We have 25 minutes, so I'm gonna talk about bake-off three, and I want you to meet your new teams for 15 minutes. You can start coming up with ideas, maybe even asking me some questions. Here are the bake-off three teams. I've also already posted to Canvas. If you happen to be in a team that's identical as before, again, I think that says something, you need to probably buy a lottery ticket, but that should be pretty astronomically small. So we'll, and we'll just, I'll put this back up within the class so you can go distribute yourselves. Okay, so that's bake-off three. So this bake-off, which I've done before, is gonna be on a smartwatch. We're gonna be doing a text entry mechanism that lives on a smartwatch. So I'm gonna give you a one inch by one inch area. We're gonna do it on smartphones, just because it's easier to give you a bunch of smartphones than this, give everyone a bunch of smartwatches. But it's gonna be an area, a screen size of one inch by one inch. And in that one inch by one inch area, which can be multi-touch now, I'm allowing multi-touch. It can be very small multi-touch, but you have to build a way to input characters for text entries. So I'm gonna give you a target phrase, like the easiest and the best class I've ever taken, and then you're gonna type it in on your little tiny keyboard, okay, to get it into the thing. Now I'll show you the scaffolding. So we're gonna... I'm going to give you Android phones if you don't have one, so one first thing you'll do with your team is check if someone has an Android phone they can develop on. If not, I will lend you some of my lab's smart phones. I warn you that even though these only cost about $16 each, they're very inexpensive smart phones. They're not amazing smart phones. But the problem is I can't get any more because Best Buy has a blacklist of me. I bought so many burner phones in such a short period of time, I'm pretty sure I'm on an FBI watch list. I got my grad students to start buying them, like five at a time, because we never activated them. We're just like stealing them essentially. And so now they're banned. So I can't replace these unless I go on a movie date or something. So just FYI, but I will want them back. But there should be enough for every team to do. Okay. So you're going to create this text entry method for a smart watch. It runs by one inch. You cannot use voice input. Again, I'm trying to give you less constraints. I want you to think creatively. The reason why I don't like this post all the rules is I would have a lot of people out there. So I want you to probe me to be creative. You're going to have a little bit of forgiveness. You can get up to 5% of the characters wrong. We're using a method called Levenshtein distance, which is already provided for you in the text. You can look it up if you want to get more details on the person on campus. So you're going to create this text entry method for a smart watch. It runs by one inch. You cannot use voice input. Again, I'm trying to give you less constraints. I want you to think creatively. The reason why I don't like this post all the rules is I would have a lot of people out there. So I want you to probe me to be creative. You're going to have a little bit of forgiveness. You can get up to 5% of the characters wrong. We're using a method called Levenshtein distance, which is already provided for you in the text. You can look it up if you want to get more details on the person on campus. the problem. There we go. So there you have it. I hope that helped you. There's some integrals as well, so it's kinda tricky, but it looks pretty good. Thank you for playing this time. with a phrase set. So here now on my screen it's way bigger than the page and that's because my screen, the screen resolution on this projector is like 72 DPI but on your smartphone it's going to be like 2-300 DPI so if you run this app on the smartphone it's going to be like super tiny and you will have to adjust the DPI on your, based on whatever your phone's model is. I might provide some links to help you get started. So time starts when you click the screen. So here we go. So the phrase here, one of two is a daring young man. So far I've entered none. Now here's how my method works. If you click here to move between the letters. Here's T and then I press the letter to enter it. And then to touch space I use this underscore. So once you're done, there is no automatically, I give you this next button that lives outside of your watch. You don't have to spend any real estate on that. You get a free, you get this for free, you can't modify this in any way. You can make the next button a little bit bigger but basically just keep it somewhere on the screen. I'll hit next. Here's another target phrase for you. Healthy food is good for you. Okay, fantastic. And I'll hit next and I'm finished. And then if we go into, it doesn't print on the screen but it prints it in here. You'll see that it gives you all the ways. So right here is, the total time taken was 58.9 seconds. I entered only 5 letters. I was expecting 48. So we know we have at least 43 errors. And it gives me a raw word per minute of 1.1. But with the penalty, I actually get 20 points. I'm sorry, what? Crazy errors equal to 4 photos a day. So with the penalty, well this doesn't make any sense. It's a negative 19 words per minute. So I actually typed like backwards in time. So obviously that's not going to work. opposite. Higher number is better, right? Because we want more words per minute. You know, 100 words per minute, that'd be amazing. And you'd be able to do a startup company to get 100 words per minute on your smartwatch, okay? So that is the entire thing. I will now let you break out into your groups to discuss. Any general questions? Any general questions about this thing? Yes? Okay, fine. It didn't help you last time. Last time you guys beat, for Big Up 2, you beat all previous semesters by a healthy margin, by almost like a .7 of a second out of like three, which is pretty substantial. But I will, this one's more similar to previous years, so that will help you, I think. Test 19. Okay, hold on, hold on. I'm pulling it up. Oh, watch this. Okay, so it looks to be the best teams, I'm not going to show you because in this thing it describes how their design works, but the best teams are in the winner, I'll tell you the winner, was 27 words per minute. The best mean was 16. 16 words per minute. So that's with the class. And obviously this one has a steep learning curve. You have to think about your design on this one. Have some kind of crazy gestural system or do some sort of Morse code with your eyes. Because they only have one trial, you know, to learn. So you have to think about how to make this work out of the box. We still get that one trial run. But yeah, the best team was 16.3, word per minute. And they, on their own design, got 27. OK. Enjoy. Let me put back up the people. I know that's scary. That's why I don't like to tell people. I never know, I just get freaked out. OK, so rearrange yourself in the room. Again, here to meet the front of the door. Rearrange yourself. Introduce yourself. If you need an Android phone, come get one from me. Sorry, I'm going to help you. If you wish, it will be answered. I'll have to make you a phone call. I don't have no idea which version you're talking about. I will make you a new copy of it. I'm talking to you. Or you can email me. I'm trying to get it. Email me to you. You know this again. It's the same thing. Do you want an iPhone or a Google Glass? AUDIENCE MEMBER 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 40, 41, Yeah, I'm checking up on that right now. I'm not super fast, right? Because it's like such a small screen. The last one's not weird. The last one's not weird? I don't know if it's weird. I will check out your answer. You can bring me the PowerPoint. I don't know. I will check out your answer. You can bring me the PowerPoint. There's a bracelet. I have you cannibized. Can I get a photo of you with the bracelet? No? But there is a bracelet. I do believe in the chemistry here. I'd like to meet a few faculty. But I can't. A few departments. A couple thousand and a few. And what is the difference? I have to get an authorization. Two weeks. Yes. You do? Can we use Google Translate? I don't know. I'll look into it. Yes. Yes. Thank you. Thank you. I've been very limited by it. I don't know how to start something like the next week. I don't know. I thought it was different. You know, I wish it was different. It's only two weeks. Since I've been pregnant. Maybe it just went to my baby. I don't know. It's been a bit unimportant. It's been a bit unimportant. I feel like no one starts these things. I've been doing it. I've done that every five weeks. I've done it like six. Yeah. I'm trying to find the best ones. I'm going to get my first... Maximum lumber group. Yeah. Yes, but it still has to be integrated. It's against someone else's name. We'll see. Oh yeah, that's right. It's a great interface. It's awesome. Yeah, it's awesome. Hey, so one more thing, members. It's awesome to be here today. Let's hear from Gina and Andrew. What are your programming side of things? You probably heard of the program. Yeah. We have an amazing team. Some of you, one of you at least, have some programming experience. Excuse me, I have like 15 minutes. I would say, I hate because you think it's going to be easy, let me know. We tend to be, the forms that you have we always have at least two people that have graded themselves at least a three. I think I gave myself a whole bunch of credits. Really? It's an issue. We also meet like next time for two minutes at the event. I would just grab the name up and email it to Candace. Send her up. You don't waste a week. Especially if you're not meeting them. I haven't actually met them. Just imagine. We didn't do it for weeks. Yes, but you can't base it on, it says it's a file, but it's 200 pages. I will actually go see the person I linked to some resources. It's called 100,000. common English grammar. Text, for example. That's text. Yeah, um... One, two, three, four, five, six, seven, eight, nine, ten. Okay, yeah, that's like what we want to... Oh, we have this one. That's sort of like, this is what you want to write in your head. Sort of, but that doesn't need to be on the screen. Oh, I didn't know that. Okay. Okay. Yes. Yes. Well, not voice. Okay. I'd rather them bind me to make sure it's not... I do want you to come up with something somewhat new. So, for example, I know that a lot of people when they try to do T9, you know, where you have the grid of numbers, so I'm just, that's not going to be me. Not because it isn't against the rule, but because I want you to do something that you invented. Okay. So, like, gesture language... Thank you. Um, I don't know if it's possible... It's possible, I think it's possible. For sure, really. Has anyone, like, tried this before? Because I feel like it could be, like, really bad and just, like, not working. It might be, like, not working. No team has ever worked with that before. Okay. But to be fair, it's also part of the implement than you might imagine. Okay, yeah. I think it could be. Okay, gotcha. And then I have another question. So, like, is it a thing where I can just complete it with... Yeah, what year are you? Um, a little later. I do know that they, as it goes up and up it gets harder and harder. Do you know who's the admissions committee? What do you know me? I'll write you a letter. I'll get into the hole as fast as I can. I may have already made admissions decisions. I don't know if it will help, but it can't hurt. I emailed you about this thing. Yes, sir. Yeah. I'm not going to run out. So, but like, I don't know, it's kind of difficult for me to make a decision because I don't know the people. Yeah, you might have to wait for another couple weeks. Yeah, it's kind of like, what I'm imagining may be like, I'm willing to take more responsibility in the first place, and do more work, and then like, maybe not come forward to take it, and leave early. Yeah, but I feel like there's, it's kind of like making an agreement with non-existent people, and kind of assuming that's going to work out. I've never seen it happen before. Normally people, the last fake-out's also more like fun, like I don't run a final, it's probably the most fun of the fake-out. Oh. And so they think like, oh, we're cool, we're chill. I wouldn't super worry about it. So as long as your team is happy, because you basically tell them right up front, hey, I've already done the comparison, I'm going to be away, you know. So I'm going to work extra hard ahead of time, so that when you guys get here, it's like all pretty good. I can't imagine what's going to happen. That'd be very strange. Oh, okay. Cool. As long as you're not working with the alien guy. Wait. It was going to be something along the lines of like a presentation issue. It'll be the same as the other thing. Okay. A little bit longer. We have a bit more time. Okay. And the other thing is like, because I'm going to be leaving the weekend, so I'm concerned that like, people might be waiting for it until that weekend. That's why you're going to have to get stuck in. Okay. It'll be during finals. It's basically a final process. Okay. I mean, we can talk more when you get your teams, but that would be like, I mean, what's your background? I mean, you may be like one of the people that's really helping out on the coding side of things. So as long as you are proactive, because in some respects, you know, all the iteration things are kind of blocked, I'm getting first in time to sign out. So as long as you're pushing that forward and you say, on my schedule, I have to get it done by this weekend, I can't imagine that it would be a problem. I think you're just going to have to take more of a leadership role. Right. Okay. Okay. Yeah, I think that's it. And if it becomes a problem... Oh, right. Is it possible for me to know those people? No, we haven't made that decision yet. Okay. That's a long time away. I wouldn't assume they are, but I've never seen this big of a problem. So like, people have to... Yeah, I mean, other people have to miss finals because they're graduating or like... Okay. So it's not like... You're not the only one. I mean, I would like everyone to see that, but I understand that it is a very late final, too. Right. So we all get a final, I think. Okay. Cool. Thank you. Hey, Chris. Yeah. I'm going to clean up the space. Okay. Sorry. It's all... Yeah, I know. Yeah, sorry about that. I just saw that there was a pile of loose sort of there. I just knocked it downstairs. Okay. Okay. Yeah, cool. Did everything make it? No, I think I need a few more weeks. I don't know. Not happy about it, but yeah. That is... Of course. Yeah. I'm glad you guys agreed to do this. Yeah. Yeah, I know. I have a lot of things to do. I'm very busy. I'll see you guys later. Yeah. Have a good night. All right. Sure. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Thank you very much. Thank you. Thank you.\",\n",
       " \"Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. I'm going to be back in a minute. All right, I'll see you over there. Good luck. Thanks. Have a good one. Okay, let's get rolling. So, let's start by talking about Bake Off 2, which was interesting, and the results were quite varied among the different designs. It still held true by the end of the Bake Off, but there was about 3, almost 4x difference between sort of the best and worst designs, which is pretty significant. If you went into the car business, and one car went 100 miles an hour, and the top speed went higher than 25 miles an hour top speed, like the 25 miles an hour top speed, no one would buy it at all. It would be a total fail. So, it does show you that even with all the same ideas, and I think a very similar level of intellect, there's still a diversity of ideas. And then maybe because you went down pathways, you got sort of trapped in local maxima that seemed good, but were never going to be maybe as good as some of these other ideas that sort of popped out in a different direction. I want to hear your thoughts, especially from participants who got to try many designs, so that's some commonalities they thought were strong. Who would like to volunteer? Okay. The other thing I saw was that a lot of the teams tried to match up the centers and then rotate and stuff. So that was one of the ideas. Another idea I saw that happened with the clinics was they had bars where you could open the bar. And you think that was successful or not successful? I don't think the bars worked at this time because it's kind of hard to... So if you think about it, you could have done this whole design as a series of sliders. You could have had a slider for X, a slider for Y, a slider for R, a slider for S. And because I let you specially decorate the target, you could have had little green stripes on all the bars. And if you kind of boil that down, what that essentially is is four Fitts' Law-style targeting tasks. It's like drag this to this, drag this to this, drag this to this, drag this to this. So you're setting four values by doing four translations. And you could actually model that with something like DOMs. You could even find basically like that Fitts' Law. We've even run a little Fitts' Law study and basically it's just four Fitts' Law tasks stacked on top of one another. You could use that to find the performance of a design that's like that. But there is a more clever way to do it than doing four sequential tasks. So what else did people notice? Does anyone else notice anything else? Okay. So why is it acting like this? So... Yes. So, why is it less work? So, in that previous design that I mentioned where you had four sliders, there's four actions to set four numbers, right? The nice thing about combining rotation and scale into a single operation is that you get two numbers in one triangulation, right? And this is sort of actually, I think, the big key in this bake-off, if you were kind of in the high performance or the low performance, or skewed higher, skewed lower, is the realization that if you have four numbers that you need to set, and the most efficient way to set four numbers is with two drags, right? If you think of the cursor on your screen, it's a 2D plane, you're getting X, Y movement. And so, with two drags, or if you think about it, potentially even one drag, you can set the value of four numbers. And that's essentially what some teams, the team that won, which had sort of the line was close, but you could imagine, and it depends on how you do the task, but if you go to a corner, and then you click and drag to the opposite corner, you're setting four values by using the upper left and sort of the bottom right, and that gives you all four values in basically one motion. Of course, you have to consider that it takes you time to navigate to the top left, so really, you never really get four numbers in one movement, you really need to get two numbers in two movements. But that's the fact, there's no faster way to set four values on an X, Y input plane, right? And so, any design that basically made you do unnecessary work in sort of like a drag a slider, where you're moving in 2D, but you're only getting one value out of it, those are always going to skew slower. Anyone else have any other insights? Yes? I think it would have been interesting to test, so I think a lot of groups that did a center and click and then a corner click, they had like a back button if you went to one of the center and clicks, I think it would have been interesting to test if instead of a back button, there was just another button that prompted you to click again. like, to accept the penalty, if that, like, half-fucking-penalty is still there, or if the fourth is, like... Yeah, it'd be interesting. It's just sort of the game-the-game. Because obviously, it'd be really weird if Photoshop's like, no, undo it, you know? You're like, what, did you do 11 or something? So it probably doesn't work well in practice, but yeah, if you could've, it would've been done. For the teams that did include an undo button, how many teams have some sort of a go-back button? Just one? Okay. So when you, or maybe two, when you tested it, did it actually help in time? Did it actually have a correction phase? Yeah, it did. Okay, interesting. Yeah, it was hard for me to guess exactly what the penalty should be, and I think that penalty is right around the mark, where it sort of starts to make sense. You got another idea? There's one thing that I noticed every time I've seen some things, is that a lot of them have some sort of confirm command. Yes. That range from, like, double-clicking to right-clicking and clicking outside the square, and I often, like, in the moment of trying to go fast, forgot what the confirm action was, so I'd, like, right-click and I'd still double-click, and I'd wonder why nothing was happening, and, or vice versa. Mm-hmm, mm-hmm, yep. There's another idea back here somewhere? No? Okay, so one other thing that I noticed that no one took advantage of this time, well, there's two things that I saw at previously popular designs. So one was... That's what I did. So one design that has happened in previous years, but I don't think anyone really implemented this year, but let's say this is your destination, and this is, like, your square, is people have previously put sort of two anchors like this on the square, and what you do is you pick up this one, and you move it here, and what happens is now your square sort of looks something like this, right? And then you grab the opposite corner, and you move it here, and it always guarantees a fit. So no one did... Some teams did sort of corner fitting, but it didn't happen with a lot. But if you think about, by grabbing two corners of a square, you can get x, y rotation and scale all in those same movements. And it becomes, again, you manipulate two x, y things to get your four values. The other thing that I saw, I don't think any team did this here, but if you look at the error checking code, it did vary a little bit in sort of the relaxation. So a lot of people on their squares, you know, they say, oh, I've got the molar definition, I'm going to put a little crosshair. But actually, crosshair is not necessarily the best here, because crosshair suggests there's only one value that's correct. And I think what would have been stronger is to have a little circle. And the circle is the kind of range of ambiguity that you can accept. So in that case, for rotation, I think rotation was a little bit more stringent. And so maybe if you had a little kind of quarter rotator, this size was actually smaller than this size. And then you basically do a lookup, you look at how the code works, and you set your sort of drop points, and the size of the target, obviously, is the size of the constraint. A lot of people just use the same size dots for rotation and centering, but actually, you could have had a little performance improvement if you had looked at the actual error bounds. Did any team do that? I don't remember any team doing this. Okay, so that was like another little tiny, tiny trick. Any other thoughts? How many people preferred Bake Off 1? Enjoyed Bake Off 1 more than Bake Off 2? How many people enjoyed Bake Off 2 more? Okay. Okay. I think Bake Off 1 is sort of elegant in its simplicity, but Bake Off 2 is certainly more interesting design space. And I feel like now, if I made you go back and do Bake Off 1 again, you'd be so much better at it this time. Now you're getting this practice of knocking out these sort of little interaction things. Okay. OK, well, anyway, as I mentioned at the end of last class, certainly previously, we were talking about bake-offs. Every bake-off is modeled after a little sandbox after a real-world example. So bake-off one was sort of like a very sparse desktop where you had the icons of your applications and files. This one's supposed to be sort of like Photoshop, Keynote, from PowerPoint, and then you'd just be able to do this sort of multi-degree manipulation simultaneously. And again, this is also very pertinent to things like AR, VR, where you might be doing multi-degree manipulations in space. Bake-off 3 is going to be highly applied. I'm not going to release any details about it yet, but it is coming. So it is on the horizon. And I will probably release it on Tuesday. And the teams for bake-off 3 will be formed this weekend. So you get to see who your new partners are going to be. Just also really quickly, it's peer-reviewed, too. I already sent out an email about it. It's already posted to Canvas. But a reminder that that is due this Friday by midnight. But it only takes 5, 10 minutes, hopefully, to fill these things out. So I would just make sure you get it in and don't be, something about, I don't know, three or four people got penalized with late submissions. It's sort of not a great way to, it's silly to lose points just because of being late. It also means that I have to hold up the whole class and follow up with people. So definitely get it done. Get it done after class, give it a moment. It's only going to take a little bit of time. And you'll be forming bake-off 3 teams very, very soon. OK. Just put in another milestone. So we've already looked at class milestone number one. So here's all the topics we sort of did in that first one quarter of the semester. Then we moved on to milestone two. So things like, we started talking about processing, shrapnel, usability goals. It's a shopping cart video. By the way, this guy's coming. I don't know if you saw, but David Kelly is coming back to get knowledge. or a doctorate, you know, the founder of IDEO, CMU alum. So anyway, but if you want to meet him or talk to him at the talk, he's going to be on campus. You know, ethnographic work, a master's in ethnology, I majored in history as well. And now, we're into kind of our third milestone, getting denser and denser over time. We talked a little bit about colorblindness and color protection, we talked about the difficult grouping principles, ICON languages, affordances, we're really packing in a lot of stuff for the semester. Even though I looked at the schedule and the semester is coming to a close more rapidly than I would have hoped or thought, we still have some interesting work to go ahead. I put together sort of a, I think, a fun, more lighthearted lecture to let you recover from Bake Off 2. But before we get into the lecture, we do a quick pop quiz. Now I will warn you, this is going to be a little bit harder because we're switching back in time. ... ... ... ... ... ... ... ... ... Look at the back, look at the back. Вот и все, как находится рим. Немного нависти перед вами. Наше предмет. Егославия. Амбалистична дерево. Амблистика. Если не говорится, что выбирайте все, то выбирайте один. Амблистика. Продолжение следует... Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Додайте 200 г цукру Okay, start to coalesce them if people are finishing, that's okay, but start to people that are done, gather them together. And look at the answers as they go by you, okay, gathering, I do, last one, last one, last one, last one, last one, last one, last one, last one. I have everything else? Okay, yes, final call? Okay, let's go over the answers really quick. Okay, the benefits of using an icon grid system, select best answer, a, a unified aesthetic and balance across icons. Yes, that I do believe is the best answer. B is also true, it does neatly align things in a row, but that's only one of the things it does. Minimalist color palette, eh, sort of, not necessarily, but the grid system doesn't necessarily do that. Icons with three edges, maybe, but the best answer here is definitely A. Okay, good icons used. How many people put words embedded in the icon? No, that's a no-no. How about consistent and minimal color palette? I would say that is true. Culturally specific metaphors, clearly that's not a great thing. Metaphors are good, but culturally specific, they're like only someone from like Zanzibar is going to understand. Very confusing, you know. You want it to be cross-pulled. And it makes a gradient red icon, clearly that's not consistent. Okay, small multiples. This is like Netflix, Rated R, Tomato kind of things. This is to help with the comparison between similar items. It wouldn't be like handbags or t-shirts. It would be like, okay, red, green, blue, yellow, pink, purple. To help you compare. That's why they're small and there's multiple of them. Okay. Proximity and connectedness. They are examples of select-all-that-apply. Some people put, it's called grouping principles. Yes. In fact, I showed you a slide with them on. Fitts' Law? Not Fitts' Law. Order effects? Small multiples? Hierarchies of size? It is just A. There's a trick question. Harrison. Using different shapes to facilitate grouping is an example of what distal grouping principles. I gave you this little icon. So how do you know those are four groupings? And the answer here is similarity. They're similar in shape. Okay. It's not synchrony. Synchrony is when they change together. Proximity is when they're closer together. Common region is where they're like enclosed in like a region that's common. And connectedness is when you draw lines between them. So similarity is when there's like a similar color, similar shape, some sort of thing that ties them together. That's often used in grouping. Okay. Human. This one was the hard one, I thought. So humans are fairly good at ordering objects when they have. So how many people put barren hues? So hue is like the red or the green or the blue component of it. And as we talked, it's very hard to put those into order. You have to sort of like use the rainbow. It's like, is orange above or below purple? It's like not super obvious. So humans are actually not very good at ordering hues. Are they good at ordering saturations? Yes. That's like the more red it becomes. comes like pink to dark red, people are pretty good at that. And intensities as well, people are good at. So the answers here are B and C. And we're weak at ordering cues. And then finally, designing interfaces in gray will help later on with, how many people put readability, how many people put grid spacing, occlusion, and small multiples. And most people apparently put nothing, but it is readability. If you do it because we're really good at intensity, if you make it in gray and then color it later, it will only help. Hopefully, from process of elimination. Let me get this together. OK. Moving on. So today, we're going to continue talking about humans, or our third lecture is on human principles and human cognition. And today, we're going to talk about visual perception, which is one of my favorite topics. There's so much cool stuff in human perception. So it's a great topic. I already pitched it once, but Professor Klatsky teaches a class, I think it's called Perception. I took it when I was a PhD student in grade, and Klatsky is an amazing professor. So I would highly recommend that if you like these sort of topics. We'll start with a classic, which is this one. So how many people see this as white and gold? Raise your hand. That weird distribution. And people who also like to sit on the right side of things. I don't think it's the projector, but that is kind of freaky. No one on here sees white and gold? So how many people, you see white and gold? How many people see blue and black? Yeah, OK. So, and the funny thing is about this image, is that I see this blue and black. I have no idea what these white and gold people are smoking, because I cannot, there's no ambiguity for me. I cannot see them at all. But it's a great example that even though we're staring at the same pixels, we know that there's an effect that's happening. It's really hard sometimes. Some people can move between the two interpretations, but I literally just have no idea what other people are doing. So we know that perception is weird. Visual perception in particular has lots of crazy stuff. And that's because there's a lot of compression going on essentially. So just a little refresher, maybe from your high school biology. The resolution of the eye, there's two things in the eye, broads and cones. Broads are really good for kind of intensity and change of gray. And then, so they have no ability to really do anything with color. And then we have cones, which are used to sense color, but a lot more, they tend to be smaller. But their distribution in the eye is very different. So in our eye, we have all these cones. You can see that the density goes up. And this little blob in the middle of the eye is our fovea, sort of this area of super high focus, because they're all packed in. You can see that, you know, of those 7,550 million, most are right here at the degrees. So it's within plus minus like one degree or two degrees. So there's this incredible density, and that's why we do color perception here. On the outside, we have all the broads, which are mostly intensity, and they're also really good at sensing motion. So, yeah, two degrees of this high optical acuity. What's interesting is that when you look around this room, like if you look at me, it looks like the whole room is equal resolution, but that's actually your brain doing some very clever upscaling. Actually, there's a reason why you can't read text sort of off to the side. Even though it looks sort of high resolution, like you can't quite read it, you really have to stare at the text in order for it to work. And that's because we're basically upscaling. But in that small two degree area, which is like smaller than your finger held at arm's length, it's about seven megapixel effective resolution. And the entire periphery of the eye, all the other, you can sense about 180 degrees. So the 178 of that is only one megapixel effective resolution, but it really doesn't look like you have seven megapixels right here and one megapixel here because your brain does such an amazing job of tricking you basically. The brain can trick itself. I don't know, it's sort of meta, like it is itself, right? So to us, the resolution that we're talking about, we don't see pixels in the world. If you sort of assume that because of this upscaling, this sort of basically it's like a deep learning upscaling, is that our effective resolution, or like our simulated resolution, resolution is more like 500 megapixels which is extremely common. So this is what it actually looks like in reality, this is what sort of the raw input to our brain is and they kind of call these like these foveated renderings and what's interesting here is if you look right in the center, you can actually see the blur. It should just look like, if you sort of just stare right at the center of that canyon, it should basically look like the canyon's all at equal resolution and that's because this is matching the resolution on your camera, you're not losing any detail even though in reality it is sliding off and that's because your brain will actually start to fill in little details on the rock and little details on the trees which is quite interesting and people propose using kind of techniques like this to make AR and VR experiences much more efficient, right? So if you want to have like a hall lens and you want to put an 8K image in front of each eye, that's pushing a lot of pixels, tens of millions of pixels, probably hundreds of millions of pixels per second but they were saying if we can track people's gaze at like 1000 FPS, then what we can do is we can just ship 7 megapixels right here and then all the rest of it can be pretty blurry, low resolution, terrible polygon count and so on and we can actually make it not only for higher frame rate but also more energy efficient as well and this is called foveated rendering. You just basically put the pixels that matter, you spend your compute cycles like your GPU cycles on the things that you're actually looking at. So in addition to kind of clever upscaling of the image, there's also incredible dynamic range. So dynamic range is the difference between the largest possible signal before you sort of like go blind and the smallest detectable signal. So basically the darkest dark and the lightest light and the human eye is unbelievable at this. So it has about an intense rate of several million to one, excuse me. So if you think about it, like you always, whenever you take a photo with like a camera, especially like a smartphone camera but certainly even like a point and shoot, you know that like you're at this like awesome party and like the lights are a little bit dark and you can see perfectly fine. It looks like awesome. Then you take a photo and your phone is like grainy and you can't see certain things. If you don't do a flash, like basically nothing comes out at all. That's because your eyes are so much better than even the best cameras we have right now and because the dynamic range is so high. So here is the range of light. So we can have all the way to pure solar. This is candles per meter squared. So if you did a measure of like the photons flying out of a candle landing on a square meter of ground. So in direct sunlight, like 900 million candles per meter squared, all the way down to like grass is starlight. You can see that there's a small fraction of one million of like a candle. And it's pretty unbelievable that we can basically experience all these things. You know, if you're out in the Sahara and the sun's beating down on you, you can see totally fine. You can see all the dunes and stuff. All the way down to like someone lighting just a single match like a kilometer away. Think about how many photons are actually reaching your eyes. That the photons are spreading out one over r squared in every direction in a sphere. And yet you can see that someone's striking that match under moonlight a kilometer away. The sensitivity of your eyes is just mind-blowing. We should also talk about DPI and PPI. So DPI is dots per inch. It's really a term from printing. More common is PPI, which is pixels per inch, because we do have dots on screens, but we think of them more as pixels. So your most regular displays, like if you just get a run-of-the-mill LCD off of like Amazon, it's going to be around something like 72 DPI. It's in the hundreds now. But if you look at some of these newer devices that are coming out, newer Samsungs, newer iPhones, they're up until like a 400 or 500 PPI resolution. So if you actually got out a ruler, like a magnifying glass, you open up your iPhone and you measure one ruler and you count all the pixels that were in that one inch, that would be, you know, 400 or 500 pixels every single inch. You just measure the antenna up or down. My laptop that I'm using right now, because I'm farther away, it's only 227, right? So if you think about the functional size of a pixel, if I'm here, I'm going to see the tiny pixels more than if I'm back here. And people tend to use their smartphone less close to their head and therefore need higher resolution. than a laptop. And so Apple was the first company to ship these kind of retina screens and sort of set the industry in motion way back when. I don't know what type of model it was. GPS? Or maybe something like that. And they have this formula that you basically take the distance, divide by 3.38, and basically it tells you what the effective size of the pixels have to be at 12 inches, such that basically you can't discern it. And so on the iPhone 6, if you take this number 12 inches away, 3.46, it means that you have to have a pixel size that's smaller than .035 inches. And in fact the iPhone 6 is .031, which means it's below the ability of your eyes to discern a single pixel. But of course if you bring the phone closer, or if you have better vision, like this, for example, this number right here is actually able to get you the phone to have like 40-20 vision or something. If it's 20-20 vision, the number is like in the 5,000s. That's kind of irrelevant. So there's little formulas like this to basically figure out if your device is retina. Now the other cool thing about your vision is that it's really high gamma. And it's reasonably low latency, too. So it's not super high latency in that, like, you know, the stoplight turns on and you want to, like, hit the gas. But it can ingest this incredible field of pixels at extremely high velocity. So you can glance at something on the order of 10 milliseconds, which is 1 100th of a second. You can actually extract information pretty reliably. But it's not, it's only useful in certain cases. We'll do a little experiment here. I'm going to flash something to your eye. It's probably more like I'm in the 200 millisecond range. And I want you to tell me what it is. Ready? One, two, three. Okay, so what was it? A paragraph about structure. A paragraph about structure? Okay, that's good. So you knew that it was text. So what else did you see in there? I saw a number. You saw a number. 200. 300. It's interesting because your visual system is really good at finding numbers. Faster than words, for example. So you found, you probably saw the numbers better. What else do people see? Can anyone tell me what it is? It's a paragraph describing something. With, okay, that's a good word. Doesn't give you much of a hint though. Huh? Wrought iron, okay. So it's like a 300 something with wrought iron and it's with something. Can anyone tell me what it is? Okay, go one more time. Here we go, ready? What do people see this time? Yeah? Well? Many years, okay. 18 something something, okay. 1889, wrought iron. Eiffel Tower, yes. Good job, you finally figured it out. So I mean, this is flashy. This is probably more like on the 100 millisecond, tenth of a second range. And there's no way you could read this whole thing. Some things pop out more like symbols and numbers are really good at reading. It's got so much training. But the other things would be very hard to see. So now what I'll do is I'll duplicate this experiment with an image instead, okay. Now this image is gonna have, in terms of just like bits of information, is way higher. But I want you to tell me what this image is. Ready? One, two, three. Now what's interesting about this, again, so like everyone probably just saw that it was the Eiffel Tower. So like this, again, folks have way more data in it. Way more data than that. There's no way you could compress this down to the equivalent of this. Even if you just did it in ASCII, it'd still be like, I don't know. How would it go? A hundred, so like maybe a hundred bytes or something like this. There's no way you're getting this image down to a hundred bytes. You probably blew a hundred bytes just on this area right here. And so your brain is just really good at seeing that. And if I had actually, I showed you this image now, but if I did this fast, not only would you know it's the Eiffel Tower, you'd be able to tell me what the weather was like, where the sun was coming from, were there people there, was the grass green. And again, that's incredible that you're getting all those details in a fraction of a second. And the reason why you can do this is that your visual system is doing all this sort of clever interpolation. Right? So there's all this sort of auto-correction. It auto-corrects things like lighting. It auto-corrects lines to be straight. It fills in missing info. These are good examples where, even though this is a very sparse representation, you sort of feel like there must be kind of a line here. Even though there's actually not. This is all just white, the same color. And yet, your brain is sort of making it feel like it can expect a kind of a continued boundary, and you're getting sort of this impression of a Z ordering as well. Here's another classic one that I scanned from a book. There is a point on this, because this is a gradient that goes this way, and this is a gradient that goes this way. There's a point on this where, basically, it's below the threshold of vision, and it basically would bleed into one another. Like, if your eye dropped into one of those feedback fills, it would escape out of here, because at some point, the gradient values are identical. And actually, they're actually identical. If you did kind of an A-B test where you just showed them the little lines that are these grads very different, are these grades very different, are these grades very different, probably for a significant region here, you would actually be able to say those are the same grade. It can't tell the grade apart. But when you look at it like this, you see a very strong line. Like, you probably can't even see really where it blends apart. You just sort of interpret it as a line. And again, your brain is basically doing like an auto-contrast, like edge-finding algorithm in real life. This is another super classic example. So we do, shadows are everywhere in the world, right? But we want to do a lot of comparisons. We're basically going to do small multiples on that. So, in this image, how many people think A is actually the same RGB color as B? Like, if your eye dropped with it, it's the same color. How many people think it's the same color? How many people think they have to be different colors? No, this fill color. Is this fill color the same as this fill color? So the answer is it's the same color. But even if you believe it, you're not going to really believe it. You can go in after class with the eyedropper tool and verify. receptor system is so good at warping it you can't see it any other way. To me it doesn't matter. B always looks lighter than A. You just can't untrick your kind of visual sensors. Here I'm actually drawing it. I just really put down a one color rectangle that covers both boxes. And again, you're probably seeing some sort of like, it looks like a gradient, right? But I assure you it's exactly the same color. If you came up here and just looked at this and then saw that, it is. They're non-different colors, I assure you. Go check it out for class. There's no way I can prove it to you because perception is your own reality. There's nothing I can tell you. Just go check it out. And that's because, again, it's not like you're having to think about it. It isn't like you're like, hold on, let me process this image. Let's write a little background thread and try to color correct this. You're just doing this on the fly all the time at 100 FPS. And it's doing it for much more complicated. You're doing it right now in this classroom. Basically color correcting everyone's t-shirt colors and stuff. But it's just absolutely incredible that this is done automatically. Go check it out later. So because you're doing so much auto correction, and essentially there's compression to be able to do it at that scale. There's no way you can do that realistically at scale. Times like a billion pixels. Eight million pixels. Seven even for your fovea. And one here, that's seven plus one megapixel. So eight million pixels. That's with no upscaling alone. That's just raw. It would be very hard to do that in real time. You don't have like a NVIDIA GPU, like a brain per se. It's all just being done with actually very slow chemical reactions. So yeah, it feels so instantaneous. But because there is all this kind of clever compression happening, is you get these illusions. This is where, with compression comes basically Gaussians. It isn't just freeze. All that photoshopping in real time is doing this. And basically we're doing augmented reality right now. The fact that seeing all these things manipulated is a form of augmented reality. Because when you take a photo, it never looks quite the same way. So those may not be visual illusions, those are more like kind of design things. So basically given that people know that there's going to be a bump on the skin, they sort of have a geometry in their minds that it looks just straight even when it's not. Again, I'm not a tattoo artist. So it's sort of like, you know the thing they just did with the Louvre just recently, right? They had all the pieces of paper put out and it's sort of this weird perspective thing and if you saw this art project it got destroyed in three hours. But anyway, if you see, you know the people that do the chalk things in 3D, if you basically have a sense of the good geometry and you stand at the right point or you know that people are generally going to be looking at my awesome biceps from this angle as opposed to here, basically you can do sort of a corrective transform. But it's more like artistry to make it look like it's visually corrected. My grandmother on my arm here. This looks amazing, right? It's not so much taking advantage of a visual illusion as it is so much just sort of artistry about how to actually make the curvy line appear to observers. So it isn't a trick though. So here's another great example. So when you look at this, how many people think the blue lines are tilted? How many think the blue lines are straight? They're parallel. And again, it's really hard to see, but the blue lines are indeed parallel. It's a really hard way to prove this to you. If I just highlight one with a box, you can see it still looks like it wants to sort of cant up. But if you just look at the boundary here, it is indeed actually totally straight. But it's very, very, very, very hard to not see. These little kind of boxes, these little tiles, this actually got discovered because I think the author was in like a cafe in Morocco that used this effect. And he's like, how did they lay out these tiles in such a crazy way? And then he measured them all. They're all the squares. And he's like, how is it possible that it looks tilted? But it's a very cool little illusion. Same here. These are parallel, but you just can't not see them as not. Again, it's just tricks like how you're compressing down vector fields. It's just wonky when you combine it with another vector. This one's another interesting one. So if you move your eyes around, how do people see the little black dots in the middle? It's hard for you to know what the scale is here. But you should see little edgingly derived on little black dots in the intersections. This is kind of weird. I don't really have a great explanation. I don't know totally how this works either. Yeah? Are there black dots there? No. No, if you just stand perfectly still, you won't see any black dots, but as soon as you start moving your eyes, you'll see the black dots. Oh, I think it's the same like you. I don't know. It happens with light, too. You get this little dot here. Yeah, sure. Yeah, I mean, there is some theory about why this happens, and the most theory is that once you focus on the white dot, your neurons basically saturate, saying, I'm seeing my white state basically conform to that. And then when you start to shift, now it's got black over it, like if you think about it, those pixels are now moving, and so now the black is like ultra contrast, it's like auto-contrasting, but there's some latency to your auto-contrasting, and so this is exposed here. Here's another one that you should sort of see that it's jittering, and people see that it's like jittering, the kind of middle part. No? It's kind of, I, the other thing, it's very hard to get the scale right. Let me see if I can make this smaller, if it's a more pronounced effect. So how about now, is it stronger for people or less strong? Stronger? OK. So it should just, yeah, very, a little bit of wobbling, and again, it's just sort of screwing with your vision. There's like a million examples of these. Here's another one, like everyone knows that those orange dots are the same size, like I can't trick you anymore, but you just can't, you can't, you can't unsee it. But they are indeed the same size, and what's crazy is that if you animate this, it really sort of makes the effect, so there they are, the same size, you add back on the stimuli. head back on the stimuli, and here we go. So it looks like it's changing, right? But the orange dot is not a, is actually not a change. So, here's another one, you know, this is the kind of, these are the classic ones, right? They're just, you know, all those people are the same size, but they're moving back in space, and it's really hard, even though, like we know that they're the same size, it's really hard to not see that one, the person in the back is just so much larger. Now what's interesting, so here's the, here's this classic optical illusion. What is it called again, a Mueller-Lyer illusion? So this one's probably the one you've seen, like, the farthest back, like elementary school, they'll show you this illusion, the popular illusion. What's really interesting about this is that it only works for some people, and it only tends to work for people that live in urban environments. And that's because when you think about where these sort of shapes exist, they exist in human square structures. So for example, this wall, right here, for these people, has this kind of mapping, where it's facing outwards, not a great example, but like, it's a corner of the room over here, looks like this, right? So imagine rotating this 90 degrees. Sarah, I want to rotate this 90 degrees, you want to apply that to this. Okay, fair. So if you think about, when you see this shape, it tends to be you're looking into the corner of a room, right, the corners of the room here. When you see this shape, it tends to be sort of like something jutting out into the room, like the corner of these windows, for example. And so this tends to be farther away, and this tends to be closer to you, and so we tend to actually perceive this as being longer, because it's sort of in the same effect as this person. But if it's farther away from you, and still the same size, it must be larger. So here's just an example of that. Here's like, you know, a wall where you see these shapes, and here's kind of a recessed corner where you see these shapes, and so we have this perceptual effect. But if you go to people that have round houses, or don't live in houses at all, which, you know, obviously in today's society is really hard, but 150 years ago, it was much easier to access communities. You show them this, and they're like, dude, it's the same, like, obviously. It's sort of like the Himba, where you're like, it's obviously. it as a blue dot they're like obviously that's the same length and all of us was like no it looks totally different lens so this is again an artifact of training we have training it every time you've encountered this shape it tends to be closer than the corresponding one of that size so you're just basically a gigantic pattern it's basically a big machine learning algorithm on legs and that's all your training it has ever been before. Okay so vision is highly parallel so we can again suck in that entire field of vision and it can do it for a lot of different things but again it tends to fail with text and audio because they're inherently sequential. So here I can give you a find me the largest number. Raise your hand if you think you found the largest number and sort of look at how you're doing this. Raise your hand if you think you found the largest number. Okay so how did you do that? What was your procedure for finding the largest number? Okay so the bottom line is basically read them all right and you sort of kept a little history like okay I saw 91, I saw 91. Yeah that's it. So you're gonna have to scan. So this would be called like linear scanning. You basically have to pass through every number in order to see. So 98 right there. Excuse me that's a tough one. Find me the real English word. Raise your hand when you see a real English word. Okay struggling for some of you. Too early to tell your finals yet. So it should be playground. So this is not too complicated because you have really good recognition of English words already. So this is a little bit harder. Find the adjective. Raise your hand when you find the adjective. So overall that took you more time. Why do you think that took you more time? Yeah so there's an extra processing step. Now you're doing like a look up in addition to scanning. You're like is it a word? Yes. Now I have to be like what's that again? Oh it's like a noun versus adjective. And so this is how you do it. has more positive overhead, and so it's going to slow you down. You could experimentally improve this. Now, if we do this for visual information, like shapes, it gets really easy. So what I'm going to do is I want you to raise your hand when you've located three items that are not like the other. I won't even give you a clue. Just three items that are not like the other. Raise your hand when you see all three. So that was instantaneous. Now, how did you do this? Did you have to start here? No, no, no, no. You didn't, right? You basically saw the whole field, and they popped out. It was pretty obvious. It didn't have to count. You just saw it immediately. Here's another example. Raise your hand when you see the two shapes that are different. So it's sort of basic, right? It's so obvious, you don't even have to think about it and get it instant, right? So this is really kind of the motivation behind information visualization. I used to do this a long time ago, when all these were like big charts full of data. And you explore, and you see the structure of the internet, or the GDP of countries over time. The idea is that you move data from a numerical or text form into some sort of visual representation. Therefore, you can ingest 1,000 data points or a million data points at once. As before, you gave people a million numbers in Excel. They're not going to be able to make quick comparisons about them. And that really is sort of the underlying difference. You can visualize large data sets. You can scan over them very quickly. You can analyze several dimensions at once. So you don't have to remember the number. You can actually see it all. So you can say, oh, wow, the GDP of the US is only now twice as large as the GDP of whatever, Germany or something like that. And you're going to see it, because they're literally two different sizes of balls, let's say, that represent the size of the GDP. So by not having to rely on memory, because you can do these instant comparisons, makes it so you can do much more sophisticated things. Why does an entire information visualization field, and why we have tools like Excel and graphing all the time. So here are some example graphs, attributes that it can manipulate, color, intensity and saturation, translucency, shape, orientation, and size. Good information visualizations take advantage of a lot of these different. effects, often several dimensions of light. So it might be that you use color to denote maybe a new language, because that's categorical. Then you basically use maybe size to do GDP or population size. And then you use translucency for some other methods. You're packing three numbers onto maybe 250 different countries simultaneously. So again, stacking, we're really good at ordering information. So even though things will overlap, we're really good at solving that. Here's an interesting one that I included. So shadows. So how many people see this, this section, these four, how many people see these as popping outwards? How many people see them as popping inwards? One person. Did you live upside down? So most people would see these as popping outwards, because that's where the sun normally comes from. Lights tend to be there, not under there. That's like, you only grew up in clubs or something, and it's only under, like, in your apartment. So you tend to, again, you're sort of machine learning. All the data you've ever seen, when you walk up to an object that looks like this, it tends to be sort of an out. That's when you touch it, like, oh, that is. It'd be very weird. You could screw with your kids and have them wear glasses and flip it upside down, and they just see the world totally differently. People have done that. People have actually worn optical mirror glasses that just invert the world or flip left to right. And it's super weird for, like, the first, like, month. Like, people are just like, every time they want to walk this way, they have to, like, actually walk this way. But actually, after about a month, you totally just get, like, your brain flips, and you're, like, fine with it. And you're just as good. You're, like, just as fast. And it realizes that there's no hard wiring in the brain from, like, left, right, and up and down. It's purely, in fact, the image that lands on our retina is actually upside down. And we do things, like, yeah, we don't see the world upside down, and so the brain has this plasticity to let you do all these things. But in this case, again, because we're just kind of built on a crazy data, most people don't see these as the sun coming from above. That's the solving of a routine. see. A lot of these things get applied to text so you've probably seen these kind of like cloud kind of text clouds and so on where you you take numerical data or maybe like image data and you basically kind of fold it or you italicize it and here we have these in bold and we do like nouns in italics or we do some sort of saturation effect on things and some people have argued you sort of get the best of both worlds but I would actually argue that a lot of these things are pretty kind of ugly like these are really popular five ten years ago and the fact is you sort of I think kind of you don't actually get the full benefits I think there's more elegant ways to do that. Here's some examples of occlusion and ordering again we're really really good at that because we lived in the real world we tend to like like base things on what we've seen so someone like drops off like pizza for you in two boxes and then you pull the two boxes apart and it looks like this like your mind would just no because I've never ever encountered that use case in your life right you'd expect it to be like this. And so again like this is why AI is such a time with computer vision is that actually logically you would expect sort of this. You wouldn't necessarily expect this. So it's all these things that you've found you know whatever 20 years of training data for from real world experience and it's really hard to kind of imbue that on an AI. Like they don't necessarily know that this link is always going to be behind. So babies struggle with this for many, many years. Another example. We're really good with lines. I asked you on the pop quiz what happens what happens behind this white box. So what do people think. What do those lines look like behind that white box. They stop at the box. What else can they do. They just go across. Most people I think would say they just go across like this. Right. But it's totally possible that this is reasonable. This is also reasonable if you're like you know kind of weird. And so what you do is you know when you get these kind of problems and you know up to my fingers. So I'm like, basically, you can see human perception get better and better at doing this sort of like, almost like a multiple-choice, like fill-in-the-blank kind of thing, and people get really good at picking out these lines. And this is another reason why information visualization often uses lines, is that, even when you have some sort of crazy network algorithms, like there's lines going everywhere, humans can ingest, like, that entire, like, noodle soup of lines, and we can actually understand where things are going at an incredible degree of accuracy. So, you know, this is just a very simple example, but I'm sure you've seen information visualization, where there's, like, lines going everywhere, like the social network graph of everyone at CMU is crazy. And yet, we can look at that and actually see trends, and we can actually pretty reliably connect them. That's one of the start-processional stresses. Back to Gestalt grouping principles, so you can actually kind of think about lines like little tiny segments, or like how little connected it falls, and you can override these in different ways. In this case, most people would, if I forced people to group this, I would group it as one whole thing, but you probably would also group these two lines as separate things. But if we use similarity, in this case we're using color, now most people would say that this is one object and this is one object. So again, there's sort of this connectedness, and there's also similarity in these different Gestalt grouping principles can combat each other to a win. It's also important to mention, like, one of the kind of clever tricks that your brain does is selective attention. So if I ask you to shift your attention to different colors, so for example, only look at the yellow. Focus on the yellow elements, raise your hand when you see the horizontal one. Have a look at the red elements when you see the horizontal red one. And what you should notice is that even though you can still see everything, you can sort of somehow emphasize or more availably hash the items. And this is what humans will tend to do. When I look at you, not all my pixels are sort of equally rated. I'm looking at your face, I'm looking at your body language. And so I'm emphasizing certain dimensions more than others. You can't do it in the naive way where all pixels are treated equally. Now this can fail in certain cases. Raise your hand when you see targets that are vertical blue and horizontal red. Vertical blue, horizontal red, there is two of them. Raise your hand when you see both. So vertical blue line, horizontal red. Okay, so what did you do to get this one? How many people just saw it instantly? Probably no one or very few people. So how did you actually do this task? What did you notice your eyeballs having to do? Yeah, so you're basically resorting to linear scan again. So if you give this task and you look at your eye movements, it's sort of like me asking you to find the largest number. Even though it's visual information, we've broken your perceptual strengths and you're now having to go back to linear scan. The nice thing, of course, about computers is that we can make interactive visualizations. So we can actually have things that move. So we can have actually data that comes and goes. We can change graphical attributes on the fly. We can change the capacity of things. And we can also explore different spatial configurations and so on. And if we incorporate animation as an extra kind of dimension that we can manipulate, it actually allows us to increase more information. So here is one of these other grouping principles that we haven't talked about, which is called correlated motion. And what happens is people tend to group things that belong together. So here you can't see that there's any kind of grouping in that field. But now if I run this animation, I would again ask people to group things. If I put this in a group, if I put this in a group, if I put this in a group, maybe there's a background move as well. So this isn't changing the color. This isn't changing. It's sort of changing a little bit of the proximity. It's all in a common region. But now the correlated motion is actually a strong cue that our visual perception uses to put things together. Here's another example. Here's actually one I included on the pop quiz, I think, illegally, which is synchrony. So instead of actually moving, they just change at the same time. So it's still a temporal effect. you look at this, most people would say this is a group and this is a group. They're changing instantaneously. Here's another example. So, common correlated movements when they're physically moving spatially together, like a school of fish, in synchrony is when there's just deltas that are happening, like they both change shape at the same time. And again, we kind of correlate that together. Okay, that's it of all the, just all the principles that we're going to cover. Here's another kind of cute one. And again, look at how you're actually doing this task. So find all the differences between these two images, okay? There are at least four. The scan is a little bit wonky, so their obvious difference is not like, that's a little bit darker, or it's cropped a little bit differently. Raise your hand when you find four things that are different between the two images. And again, look at how you're actually doing this task. Okay, starting to get there. Raise your hand if you have about three, three or more. Okay, so what are some of them? Now, so that's the task. So basically, you're going to look at a patch, and then you're going to kind of bring your eyes over here and see if there's like a delta. You're going to go back, right, back, right, back. What are the four? Just to repeat people. I found the window thing on the right. There's windows going missing. And again, the important thing here is you're going back and forth, okay? Now, this is a very common behavior and interactive task. When you're basically, maybe you're looking at some sample code online, and you're looking at like your processing code, and you're just like, oh, what is wrong between these two like function calls, right? behavior of moving back and forth is a very common way of comparing. And so you'll often see this is a shopfront building software where there would be two different JPEG compressions. And so you know there's two different images. One's going to be more highly compressed, you know, 60-80% compression versus, you know, 80% compression. And the way that you compare like is a artifact. You glance back and forth. So can you imagine what might be a better way to show people or facilitate comparisons across like items? Yeah. Like one over the other and change the opacity. Yeah, that's definitely a good one. So you basically have one over the other and you can either change the opacity, which is sort of interesting, but that may be hard to see like JPEG artifacts, but you could just toggle one on and off, right? Is there any other way that you could do comparisons, yeah? They put them like three or two. Okay, but you still have to go like up, down, up, down, right? Yeah. But yeah, you could try to simplify it. Yeah. So you can like change the opacity of one on top of the other? Yeah, kind of like change which one's on top. Oh, I see. So you can toggle which one's on top. Yeah, and actually this is what the common way to do it is. Is you have, you know, if you're like an image and you have kind of a little preview about an effect, let's say this emboss effect, you put a preview on it. You can just toggle it. On and off, on and off, on and off. So this is basically, I would say, the industry's best solution for doing these comparisons. They moved away from basically a small multiple comparison to basically this toggleable comparison. But they make a really dumb mistake in a lot of software, and once you see it, you'll never make this mistake. So what happens is because for something like an emboss, it takes a split second to render that preview, and especially in more complicated software where you want to turn like, I don't know, ray tracing versus phong shading or something like that, is there's this little momentary blip where often they deallocate the previous one and they basically slap on the new graphics, like the emboss effect. a little tiny interval where either it's in some intermediate state or there's no graphics there at all. And sometimes it's only like a small fraction of a second. And this blanking interval really screws with your visual perception. So here's an example. So what I want to do is raise your hand when you see the change. Raise your hand when you see a change. It's not a small change. It is a big change in this image. Raise your hand nice and high when you notice what is changing in this image. You find it? Yeah, actually, this is the opposite. Raise your hand if you still don't see the change. Raise your hand if you don't see it. You can put your hands down. Okay, so still can't see it? I know, right? It's so hard. Now, all I'm doing here is I'm inserting one gray frame, okay? Now, if I take it off, it's obvious, okay? And if you just actually do it well where there's no blanking interval, it's really good. I don't know why that's where you find it there. Okay, so now you guys understand the game. Let's see how good you are at it. So here's another one. Okay, so raise your hand when you see the also very large change happening. Ooh, this is a hard one. So raise your hand when you see it. I mean, it's literally just looking at two images. Now, some people can just sit here for like half an hour. I've seen it. I just let some of my students just run off. Okay, so how many people still don't see it? Raise your hand. So more than half the class still doesn't see it. So okay, now when I do it like this, do you see it? It seems so subtle. It's a very, very, very subtle difference, but it makes a huge difference. I mean, it's like instantaneous recognition versus people who started for half an hour and still just never see it. OK, here's another one. So raise your hand when you see the change. Raise your hand when you see the change. The other weird thing about this is that it's actually hard to get better at it, too. It's not like training improves it. Raise your hand when you see the obvious change. OK, we're going to move on. Just because it's brutal. So if you think about it, there's so many software packages are like, let me just toggle on this layer in Photoshop. Let me toggle on an effect in Blender, whatever it may be. You'd be amazed how many interfaces actually get that very small difference. If you want to do true A-B testing in situ, you really want to make sure that there's absolutely no blanking interval, that it should overlay it. And then it's really obvious. But they've gotten a lot better at this. Because it's a very kind of weird, esoteric perceptual effect that happens to manifest itself in interfaces. And it used to be much worse when computers were slower. Like, if you used Photoshop 2 15 years ago, you'd literally be like, turn on effect, and it'd be like, deallocate, then it'd be like, reallocate. And you're like, I don't know if I can see anything changing at all. OK, here's a cute little ad that takes advantage of this effect. I'm going to turn off the lights so you can see this. To test just how much attention the attention-stealing design of the new Skoda Fabia actually steals, we left one parked on this ordinary road in West Humps. We wanted to see if its sharp, crystalline shapes, bold lines, and lower, wider profile would attract the desired level of attention. Will the 17-inch black alloy wheels stop passers-by in their tracks? Will the angular headlights attract the attention of other road users? crowd gather to check out its fresh sporty look. Well, not quite. But did the attention still in design distract you from noticing that the entire street has been changing right before your very eyes? Don't believe us? Have another look. Did you spot the baron changing to a taxi? How about the scooter changing to a pair of bicycles? Or the lady holding a pig? Let alone the fact that the entire street is now completely different. Didn't think so. So there we have it. Proof that the new Skoda Fabia is truly attention stealing. Okay, so the other thing we should talk about is face processing. Hopefully you can recognize this as me, when I was your age. I don't think I look all that different from you, but the hair may be a little bit less hair now. And so we're so good at recognizing faces, I've probably seen them everywhere. So much of like, not like mythology, but so many times in human history, we've tried to find faces on clouds, on the moon, and so on. That's because we're so, so, so good at finding faces. We have a lot of training data. We often stare at people's faces, and so we have thousands and thousands, millions of hours probably, of faces. You can see these examples where we can find faces even at very low resolution. This is probably the classic example, if you don't know what this is. You may have seen this image before, but yes, it's a very good shot of Lincoln. What's cool about this is that if you actually squint your eyes and blur your eyes, you'll actually, your brain will start filling you with information. It'll actually start looking more photorealistic. Yeah, it's kind of weird. Yeah, you'll actually start basically pulling from your cache of what Lincoln looks like and try to intel the data. And you do this all the time. We're also pretty good at squished faces, so can people tell us who's this? It's like pop culture variation. So it's weirdly enough, we're pretty good at this, even when faces are squished, and we don't quite know why. Because we have no training data like this. And I'll show you examples of other training data fails in face processing. But because we're so good at faces, there's this proposal by Herman Chernoff, and he says, well, you know, faces are this great multivariate thing, like I can change my eyebrows, and my eye size, and my nose height, and helping them smiling, and where my ears are, and my hair color. So he says, why don't we just treat this like an icon language? And we can vary all these different parameters. So here's, for example, some of the terms that you can change. The shape of the face, the height of the nose, the length of the eyes, the angle. And you can sort of make these different things like this. And so if you wanted to render, for example, like the GDP of countries, and their population size, and their pollution, and how many fish they catch, and whatever kind of dimensions you want to, you could pick all these different dimensions. Like, how big their head is, how big their population is, how big their mouth is, like how much food they eat, how big their nose is, like how much pollution there is in the atmosphere. And you can basically have like, here's like the US, here's Mexico, here's Peru, or whatever you want to do, and you basically get in this multivariate, iconified sort of thing. The problem is, it turns out that faces, as you might imagine, are not kind of linearly separable. Like, when I do a frowny face, my eyebrows often do something, and they're sort of correlated emotionally. And so they're not totally separable, and so therefore they're not really good at getting for conveying data. So, faces are first off non-linear, like a smile isn't just parameterized by how much my lips move up. Like, the more I do this, there's like an extra weighting on smiles. They're non-linear, and then there's also these interactions between the different features like eyeball size and surprise. You can't be like, surprised and sad all simultaneously. Like, no one really has any data like that. And it turns out that it ends up compressing what should be a very nice visual field sort of suggestion back into a serial task. We're having to look at every face and be like, let me look at that for some time. that person knows, that person knows, and you're basically back to the text of that. So even though it's kind of heralded as this maybe breakthrough into a data science and information visualization back in the 70s, basically no one uses it anymore because it was a total fail from a perception standpoint. Now, there are some notable weaknesses of face processing as well. So who is that? Yeah, but no, it is Obama. It's inverted. And it's a little bit harder. If you time people's error and you look at their recognition time, it's much worse. And that's because while we do maybe see sometimes people's faces compressed, especially today on LCD TVs. Like if I look at a flat screen, I look at a TV from the side, I essentially get a compressed view. And so it's very hard to know if that compressed view would have been true for people that had never seen motion pictures like two or 300 years ago. But they'd be like, what's up with that guy's squish face? Never seen that person before. But nonetheless, no one is going around and seeing the world with an inverted vision. And so therefore you don't really have any training data on this. And so we tend to be much worse. These are a bit of esoteric examples here. Does anyone know who this is? Over here. Jim Carrey. Wow, good job. Yeah, someone said it over here. Who said that? Okay. No. It's not Jim from The Office. It's Kevin Costner. So it's like I got this out of a textbook from one of the fees who helped us out. Unfortunately, it's over. So what happens is if you take out the high frequency, or you only leave in the high frequency spatial information, so when you get rid of all the kind of shading, it's actually quite hard to recognize who these people are. And again, you don't know who these people are, so that also affects your vision. How about this? Who's this? It's a famous person. So how about if I give you the eyebrows and not the eyes? No one knows who this is? How about now? It's Nixo. Yeah. So it turns out that we also, and again, like no, I was not alive when Nixon was thrown out, but like, you really, it looks like faces are for lists, so you can't remove certain parts of it and still have the recognition. We actually recommend, we kind of cache this in our brain with all of the dimensions included. So like our hash function for brains is there, for faces, is all together. So even though people can guess it's Nixon from certain facial features, we're so much faster if we get to see it all. And if you start to kind of composite people's faces together, it does slow down the recognition time, even for famous people. So can anyone, does anyone know who the top is? Elvis. So these are famous, yeah. Yeah, this is Elvis. So if you start splitting them apart, it should make, it starts making it a little bit easier, actually. But yeah, so again, we don't see composite faces very often. Here's maybe my favorite example. So these images, it just looks like an upside down head, right? So if we rotate this, we can see it's an upside down head. But again, even though you know that this exists, like when you see it like this, it doesn't look that weird. And again, that's because you very rarely see people like hanging upside down on monkey bars, right? And so you just have so little data that you don't know what a face should look like when you're upside down. You know, but like if obviously there was no mouth, you'd be like, that's not normal. But you sort of know what mouths look like, you know what eyes look like, and so it seems like it should all work out fine until of course you rotate it. So this is a really great example of just lack of data. You know, probably less than 1%, probably 0.001% of our training data is looked on that basis. Here's another one. So how about this? How many people think the left hand side one is correct? How many of you think the right hand side one is correct? Left, vote you're left, vote you're right. And most people think the one closest. Hold on, you're whatever, this side, my side. But yes, it's... There's about one third of you who voted for this, and two thirds, but most of you got it right. But clearly you like it's very, very different. Okay. Another really amazing thing that our eyes do is with motion. This is how we take advantage of things like synchrony and coordinate motion. There is like an outline of a bird in here. And I could give this to you, and you would never find this bird. But if I animate said bird, then you see it instantly. Again, this is... If I pause, even though you know it's there, it's actually quite hard to trace that bird accurately again. But as soon as it starts moving again, it's obvious. And so again, this is one of those things that we're just so good at. You can sort of think about this in an evolutionary perspective that I want to see that saber-toothed cat walking through the grass while I'm down getting water from the river. And I'm only going to see maybe 20% of its body. But I can see all the parts moving together. And I can even recognize what that animal is, how it's moving, is it stalking me, is it going to leap out of the bushes, whatever it may be. And we're just so fantastically good at assembling all that sparse information. You can think about this from a computer vision standpoint. Here's another one. So in this case, there's a circular arrangement of squares. Basically impossible to see here. But if I animate it, it's pretty obvious. So another, probably one of the strongest examples, most interesting, are these ones. So can anyone—here's like an arrangement of dots. Can anyone tell me what this is? Any guesses? It's big and a little different. It is not. It is not stars. It is a thing on Earth. It's actually— It's a lion. It's a lion? Any—no guesses? It's like that Florida map I gave you. Anyone want to guess what type of object it is at least? It is an animal. A cat? Bird? Nope. Okay, so now— I will animate it, same data, but now I'm just giving you temporal information. And now, you can clearly see, it is two CMU students attempting to dance. So what they find would be, so these are called point light walkers, or in this case, it's a perception of biological motion. And what they find is that people can recognize, number one, you can see this is a human, instantly. You've never seen a human like this, so we know that your impression of what human movement is like maps onto this, because you've never really seen humans like this, and then you can recognize that they're humans. You can recognize what they're doing, and you can even, in some cases, people are really good when they can actually determine age and gender of these point light walkers. So again, very sparse representation, but we know that this must be true to how it works with asthma, by the fact that we're able to still recognize it. So here's another example. In this case, it's someone doing, I don't know, a few dips or something. I don't know. And again, you'd be able to easily recognize that this is a human, and they're doing some sort of exercise or performance. So this is this notion of biological motion. You can do this on many things. If we put on a lion, and a line walking, or an elephant, you'd be able to instantly see what that is. It doesn't work as well for mechanical objects. Even if I had a school bus, and I put some on the wheels and some of the things, you'd probably be able to guess it's a vehicle, but it would be much harder to actually do reliably. But there's something about, and again, I'm sure we have dedicated hardware for this, is that we've been trained to take the sparse information and actually map it onto a biological kind of instance. Okay, very good. I don't have enough time to finish this. I need probably one more minute, but I'll just talk about it super quickly. So one of the things that I did in my own research is saying, well, since we have all this great cognitive hardware for processing motion, can we do some of that iconographic motion? And we have icons that are graphical. We have sound icons, like a little shutter sound. But we don't really have the notion of iconographic motion. But we know that humans are really good at it. And there are existing behaviors like this. When you click the dock in MacOS, it bounces. You have this flip kind of card look, and there's a wobbly effect in iOS. So there are examples of people using motion to convey a state, which is essentially what Icon is. And so we said, well, why don't we, this is a very limited vocabulary. No motion's powerful. Can we somehow use it in a clearer way? And so we did this project called Kineticons. I did this project called Kineticons. And we took these random, start-up-y kind of styles, kind of icons, and we gave them these different behaviors. We sort of brainstormed, so I'll show you the list. We did a design crowd-source study where we came up with all these different behaviors that you could take any icon, whether that be a progress bar, a menu in a menu bar, or just an application icon like this, and we animated. So it's basically a transformation, motion transformation that you could apply to anything. Here's a sign that's sort of flapping in the breeze in 3D. Here is a rumbling motion. This is actually very close to iOS's, sort of what they would say is movable, which is a very weird mapping. Here's a frantic camera running. Alarm clocks is sort of that ring, ring, ring. So you can read the whole paper on this if you're curious. And what we found is that there's certain behaviors that are very iconic, but in very different ways. Actually, that rumbling behavior, most people, when we saw the big crowd-source study, is that rumbling behavior conveyed to people, it needs your attention. It's not that it's able to be moved, that it's like something is broken, it needs an update, it's out of memory, something like that. The one that actually conveyed the best, that I'm able to be sort of dragged and repositioned, was actually that 2D hanging in the sun, sort of that kind of blowing leaf in the wind. And so we published this paper and started to convince people that you should adopt sort of a motion icon language and you can apply it systematically. There's one other project I want to show you, but we only have four seconds left, so have a great weekend and look forward to your new make-up.\",\n",
       " \"... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... I don't want you to be excited about that, but I don't want you to be sad about that. I don't want you to be sad about that. I don't want you to be sad about that. I don't want you to be sad about that. I don't want you to be sad about that. I don't want you to be sad about that. I don't want you to be sad about that. I probably got a list of ideas from 90% of the teams, but not all the teams, which is a little bit worrying. How many teams have a working first prototype that's like possibly bringable to the big office? Okay, about half, maybe two thirds. Okay, that's good. I would say if you don't have sort of a first round prototype that's working by like Saturday night, the latest, there's pretty much no way you're gonna converge on that reasonable design in time. I would say really by end of today or Friday, you really should have something that you can start testing and iterating on. Otherwise, you're basically, you're gonna miss that iteration cycle, which is probably the most important step in the whole process. As for clarifications, because I've been seeing a whole bunch of things come in, what are the main themes? I guess they're fairly generic. Nothing is standing out to me. Does anyone have any questions? So one thing that I did see, hold on one second, is again, just to reiterate is you can't have buttons that only, or like, you know, options that only appear sometimes. Like people are asking me, hey, can we have a button for like count? Like if you have to rotate it counterclockwise, can the button for counterclockwise appear? Yes, but clockwise should also appear because everything has to be equally accessible. So you can't have only the buttons that you need available. You should be able to get to everything. So just like, you know, you're in Excel, you can move like arrow keys anywhere you want. You should always be able to do that. You can highlight them potentially in a more unique way, but try to make it fair. You know, you can point, sort of kind of illuminate the path to the right answer, but you shouldn't really only make one possible thing happen. The other thing that I saw is like, you know, can we have a button for like rotation, let's say, and it's like, you click and go, and then it basically sticks when the value is correct. Clearly, I mean, that'd be awesome if computers could read our minds and they would just automatically stick to our destination, but that clearly makes that particular value unfair. So there should be no notion of auto-locking or auto-sticking or anything like that. All values should be fairly achieved. Now, you can do highlighting or some sort of visual feedback like, you are correct, but it shouldn't lock or stick or somehow bias towards any particular values. Same with when you get a value correct. Let's say you're scaffolding your design so that you can do, like, you move first and then you do rotation. When you get the move correct, it can't automatically move to a rotation phase because, again, that's biasing towards the correct values as well as the incorrect values. You need to find a way that makes it so you can move to the rotation even when you're wrong and there's no, basically, bias in the design. I don't want to say too much because you have to think about a clever way to do this. It basically gives you a bias towards the correct thing, but without actually being biased itself, and that's sort of the optimal technique. The other last thing that I should mention, because this has come up, I think, in two or three emails, is you can't have any, like, teleporting. Like, click and something jumps. This is meant to be a continuous manipulation task, so every value that you're modifying should be a continuous manipulation, like a drag of the mouse or a holding down of a button. You could have a clockwise button and you click and hold down and it goes, like that. That's also a continuous manipulation. It shouldn't be like, I click and any value in your code goes from 100 to 515 instantly. So there's no, like, teleporting or jumping. You have to kind of move it from 200 to 515 by doing some sort of an action and the values update over time. Okay? Okay. Question? As you said, non-bolting has to wait. The two fingers doesn't do anything. One finger is like, if you want to click and drag something, it's usually just a lot easier to click one finger and drag the other. As opposed to, like, I can't drag a single finger even though it's happening. a different sort of capability, what's your stance on that? Yeah, so I am allowing, if people prefer to click with one finger, but drag with the other, I am gonna allow that, because that is still a single touch application, because you're not using the X, Y value from your clicking finger. So I am permitting that, that is not in violation of multi-touch. Multi-touch is where you're getting inputs from two or more fingers, yeah. So if your target square is gonna follow the mouse, the mouse has to be clipped down, basically you can't just follow it by moving the mouse. You can click it and it can stick, you don't have to have your finger in front. You don't have to click it once, then it starts tracking it, and then it flips or like when you say that, it continues to make a list, but then it's just following the mouse hand. Any other questions? Okay, well you can come ask me after class too. Okay, so today we're gonna continue some discussion on visual design. I wanted to talk a little bit about like grid systems and color and things like that. We're gonna continue to do good practice visual design and also introduce you to some more vocabulary. And specifically today we're gonna be talking about design patterns, metaphors, affordances, and skeuomorphs. Okay, we'll learn about all of those, how are those different from one another. So first up, let's talk about design patterns, which is a word that people probably should have encountered maybe in their other coursework. How many of you have heard of the term design patterns before, outside of this class? Okay, you're in soft, you're in like computer science, almost certainly have heard this term. Can anyone give me an example of a computer science design pattern, an example of one? Yeah, in the back. Can we see if you have another example? I'm afraid not, I don't actually know what that would do. I think programming language would mean, but yeah, I can see what this does. design pattern. And then Model-Do-Controller is a good one, right? You can also have like consumer producers and other common design patterns. So, okay, given that you had your top two examples in computer science, can you tell me what a design pattern is in general beyond the physical design? What is a design pattern in computer science at a high level? A format that you're following. A format that you're following? Okay. Does anyone want to add to that definition? Why follow that format? Yeah? Okay, so there's like a stylistic component. Why else would you want to follow this format? Okay, partially. So why do people say use Model-Do-Controller for the computer scientists in the room? Why not use the Chris Harrison view method? Yeah? I think they achieve results that are consistent with the coaching model. Yes, so the key is that these design patterns, like Model-Do-Controller in programming, is that these are good methods. They've been shown to be successful. It isn't that we're copying a terrible idea, we're actually copying something that's been vetted and is good. And this is true for design patterns in general. Design patterns exist beyond computer science. Design patterns live everywhere. So I've included this quote here that Picasso said, good artists copy, but great artists steal. And if you have good ideas, if there are good ideas out there, you should essentially steal them, in quotes. So the core idea of design patterns is that you're going to adapt a good design from one or more existing designs. So you're going to take this good pattern, this good design, and you're going to apply it to many things. So the key is you don't want to reinvent the wheel. The design patterns often work. or a proven design, so you actually know that they work, in the case that was mentioned before, that users will often have familiarity with the mechanism. So there's a particular way for a volume controller to work. Or let's say a calendar, like a calendar widget. You could invent your own crazy calendar thing that's shaped like a little time machine and you move these dials or something to change the date, but no one, if any, has yet used it. Where if you pick a standard template, a standard design of what a calendar widget looks like, it doesn't matter if you're going to Expedia or Kayak or Orbitz or whatever, you're like, I understand how this works because they're using the same design. And that's good, that promotes ease of use across different platforms and improves consistency, which is one of our real secure aspects. So design is really about finding solutions. I think there is a tendency, and I talked about this earlier, is that designers, as kind of creative professionals, often have this urge to redesign, want to come up with some snazzy new thing that may be cutting edge. And that's not necessarily always the most effective strategy. Often the design you're trying to sell, you have to keep cutting across, and you're really trying to solve a solution for your, a problem for your client. And so if you can draw on existing good ideas, that's good, there's nothing dirty about drawing and leveraging existing ideas. For people with their laptops out, last morning, you put your laptops away. Okay, so design patterns originated with this book, I believe it was from the 70s, and it actually was from architecture, okay? And in architecture, there's lots of examples of where you want to leverage a design pattern. You don't want to have to reinvent a building pattern. The things that work in buildings really well, like doors, like a little slab of a wall, or some hinges that let you partition spaces, it's a really good idea. That's why the Romans had them, that's why we have them. Like 2,000 year separation, everyone has doors, everyone has desks, everyone has chairs, right? Even classrooms, like if we brought like Aristotle in here or something, he'd be like, yeah, it's a classroom. Like it'd be high technology, but he'd be like, I can figure out what this fits, right? So a lot of these things haven't changed, and that's because they're good. So for example, I design a waiting room in a hospital, I design a bus stop, these can follow particular design patterns. So this is one example I took from this original book, which introduced this notion of design patterns. So this is like a hall, like a type of socialization hall. So somewhere in the community, there's at least one big place where 100 people can gather, to hear one music, and perhaps a half dozen activities. And it's designed in such a way that people are constantly crisscrossing, so they intersect and it facilitates these sort of social connections. And the elements of this language are called patterns. Each pattern describes a problem that occurs over and over again in our environment, or HCI. And it describes the core of the solution to that problem. This is the really beautiful part of this phrasing in one of the authors. So you can describe the core of a solution to the problem in such a way that you can use this solution a million times over, without ever doing it the same way twice. So it isn't that when you leverage this idea, it has to just be a regurgitation of this stale old idea. But you can take the essence of an idea and apply it. And even if you use it a million times, nothing has to be identical, which is a really nice way of saying it. So it's not that it's a direct copy, necessarily, but you can take inspiration of the best parts. Very beautiful quote. So patterns come from successful exemplars, web popular websites that are so successful that I feel like a lot of people say, if you're going to build an e-commerce website, you probably could model it off of Amazon. If you're building a new website that's selling some newfangled thing, you don't have to reinvent the wheel on your e-commerce site so when people come on, they have to learn how to do online shopping again. The thing you want to sell, the problem is, I want to sell whatever, Chris's crazy new five-wheeled bicycle. We want to make that the focus of the selling, not the fact that they have to reinvent this whole kind of website experience. So you might as well just copy it as much as possible. So there's a great metaphor, and we'll talk about this later, that works across many websites, is the notion of shopping carts, right? That's not the only way to check out through an e-commerce store. You don't have to have a shopping cart. But it's so ubiquitous, it's kind of a language that everyone knows, that it's a great thing to leverage. And I can just use that on my new website. So here are. One example, so these are all benches, and they all follow, even though they're all very different designs, they follow a key design pattern. Can anyone tell me what is common among all these designs, are they all in one? Yes. They all have a section. And why is that? Because people like to have their own space. Okay, that's part of it. So by having sort of defined seating areas, that each person can tuck into like their own little personal zone. That's definitely a good thing. There's another design paradigm that's going on here. They're in groups, sure. Yeah, so you want to see lots of people. I guess that is also a design paradigm. There's another unique thing about the design. Yeah. So why is the flat surface where your things are? Sure, that's a design pattern for chairs in general. Yeah. Perfect. Is the design in a regular form or is it coordinated? Yes, so this is the one I was with. All of those are true, but this is a main facet of these kind of designs is that you don't want, it's unsocial and not a very good effective use of civic resources if you can lay flat and like take over 10 benches, right? Like and people will sit down and go to sleep on these things, especially like at airports. I am guilty of this. Like I definitely do stranded in airports overnight and I'm totally one of those people that like try, attempt at least to sleep like in it, but they're always shaped like this. So you have to sort of like fit your body like a Tetris piece into the chair in order to get any sleep at all. So yeah, almost all of these public benches, you can look around in Pittsburgh as well, is they tend to not facilitate people sleeping there because obviously it's not a great look to have. It's like, you don't want, you want the people already to be living rough on the street. You want to try to bring them into shelters, not incentivize for them sleeping on the street, right? So you'll often see, and it's done in many ways. This has like discontinuities. A lot of people have their personal space here. It's very difficult to sleep on that. This one's totally impossible. This one, they put the bars there. Here's some opportunity to maybe just like sleep here, but nonetheless sort of have this personal space. So they're all. expressing these design patterns and again it's like how complicated is a park bench but there's experts in park benches essentially that are going to rope in probably four or five of these different design patterns and again none of them you could design a park bench a million different ways but the essence is the same and that's a good idea of design patterns. And there's design patterns for spaces too so here I just grabbed a couple examples of like hipster coffee shop kind of design patterns and of course you need like a high top bench right like I remember when the Starbucks on the corner here like redesigned it and they like put in that high top bench in the middle it wasn't always like that that happened three years ago. You almost always need like the chalkboard menu. I feel like this is this emergent design kind of trend that you need to be white on black, you have to have like a wood cabinetry, you kind of got the bearded barista behind the bar there so like all these things like so when you walk in it doesn't matter if it's like Pittsburgh, Portland, whatever, you walk in like yeah I know this kind of coffee shop. So pros and cons to using design patterns is that they're good for new users that are familiar with design patterns. I can walk into that coffee shop and I get what the experience is going to be here, I can land on an e-commerce website and I'm like oh there's a shopping cart, like I understand immediately how to use that e-commerce website despite having never visited. And of course for the designers they don't have to reinvent what their coffee shop looks like or reinvent what e-commerce looks like, they can really draw on those existing design patterns. Now of course something that works doesn't mean necessarily that it's good. This is sort of that survivorship bias of ideas. Like the fact is maybe there is a better way to do an e-commerce website but everyone's stuck in this design pattern that we're sort of limited now. If you try to do like that cover cart or like idea or some sort of crazy thing that you can't even think about, it may not, it may be better but then no one understands how it works. So people will argue that it sometimes can stifle creativity if you're just copying everyone else's idea. And if you implement the design pattern wrong, like you don't quite understand what it means to have a shopping cart. So like when you take things out of your shopping cart and you're like new shopping cart paradigm, When you click on something to remove it, it removes it from the upper basket, but it puts it in the lower shelf if it's a large item or some sort of weird metaphor, then people might get confused. So it may be totally possible that a lot of our design patterns just look optimal, and if we really thought about it deeply, we could come up with something so much better. But that comes with a cost of being, again, not familiar to users. So it can lead to conventional designs. We may not have the best solution. Sometimes it will take you time to find the appropriate design pattern, so if you need to make a park bench, you have to do your research to basically figure it out. And then sometimes there are these, but inappropriate technologies, so clearly, especially in the technological world and the HCI world, technology is moving so fast, you don't want to be stuck with old paradigms, and maybe that technology is opening up entire new paradigms that we should be looking at. Maybe all of our shopping should be in AR, and we actually have a real shopping cart, and we go through the aisles of this virtual Amazon warehouse and put them into our baskets. Who knows, right? So that's one example. The other good thing, and this was sort of mentioned earlier, is that design patterns are useful shorthand. They're kind of a vocabulary for design. So I say, hey, we're going to build a new e-commerce website, and I need you to build a shopping cart mechanism. I don't need to tell you how that'll work. You'll instantly understand it. It's shorthand for this incredibly complicated thing. It's going to be cookies, that storage, it's going to be a database, it's going to have a checkout procedure. I'm just going to have a checkout. I put these into the shopping cart, and then I get my credit card, and blah, blah, blah. So you understand immediately. And if you know design patterns really well, it's a very useful kind of shorthand. So shared language, right? The other big thing here is that they also work at different levels. So you can have design patterns that are for neighborhood scale, for building scale, for single rooms, for single gardens, and so on. What they did in that original book is they capitalized the design patterns so that they stood out as sort of this, almost like a syntax. So here is an example of one of the design patterns they have in this book. And so there's like this beer hall, sort of the center thing, but there's also like a pattern for nightlife, and there's like a promenade, and there's like these subcultures, and there's like the actual building material, and then inside the beer hall there's also going to be this like alcoves. And again, these all exist in different strata, sort of room scale, local scale, city scale kind of things, and you can integrate all these different elements. And these are, sometimes they're physical, and sometimes they're structural, like how do you facilitate like a good city with interesting nightlife? That isn't like you built the room. It's going to be a different set of policies, for example. So design patterns exist in many different walks of life. Here are some just quick ones I just like took screenshots of. So here is like animated transitions where you open up like a little corner box, like it animates out as opposed to popping out. Here are some sort of mobile experiences, so you'll often see sort of timers that are like this where you sort of scroll up and down on the individual digits. That's a very particular way to represent time. You don't have it as a text entry field, but nonetheless you see those as being quite ubiquitous. Password strength meter, you know when you type it in, you often see that little kind of red, yellow, green bar. It's not like 500 websites co-invented that. Like someone sort of invented it. It started to get used. People saw it as a good idea. It was a good design pattern, and it sort of slowly became more ubiquitous. This one's a bit more meta, lazy registration. So some websites let you go. I feel like Pinterest is a good example of this. Like I don't have a Pinterest account, but I usually end up on that site. And like you can surf for a while, and then eventually like it bugs you to register. Like you know there's certain features that sometimes you go down some sort of link, and then that's when you have to register. But it sort of lets you explore and maybe even add items to a shopping cart without having an account. And other places have sort of strong registration where you really can't even get into the website without having an account. Facebook is sort of like this too. You can nominally explore before they kind of trap you and make you sign up for an account. whole book on design patterns to the web was actually written or contributed to by Gates Pong, who's back over here at CMU. You can take his class and so on, yep. I mean, I think it's a great area. I'm not sure, like, where design patterns necessarily have to begin and end. I would say there's definitely design patterns for how you structure books. Like, there's this protagonist, and there's a dilemma, and there's a climax, and whatever. So I feel like there probably, you could say that there are design patterns for writing good books, and certainly movies call design patterns what the storyline has to sort of be. It seems like every fight sequence I've ever seen in a movie last five years is the two people meet, and initially they're losing, and you're like, oh my gosh, and somehow they do some ninja flip, or some sort of trick, and they win, and it's like, no one ever loses. They always get the last minute somehow, like I say. So yeah, they probably, I'm not sure if authors would consider design patterns, like, the notion of design patterns has been around forever. Like, the Romans were using design patterns in the studios that we're in. So every kind of field has essentially come up with templates that are successful. So they probably just don't use that terminology. This book really formalized, well, not this book, but the previous book really formalized it as a kind of a syntax, a strategy for design. And I'm not sure authors would say they're necessarily designing books, they're like creating books. So, different vocabulary, sort of related, but yeah, it's an interesting point. So, you know, millions of books have been written on it, hundreds of books have been written about design patterns for all these different areas, and obviously the web has this particular set of design patterns, and it's sort of like almost like neocentristic. There's this huge kind of like web of hierarchy of different things, like how do you facilitate trust and credibility? What is a homepage? What are the design patterns that make up a homepage? What are the design patterns that facilitate trust? search. So let's go into one of these here. This is making navigation easy. So under that category, which is K, there's all these items, right? So a tab row is very common, a navigation bar is very common. Buttons with actions like order or add to cart, those are action buttons. High visibility action buttons, like Amazon is different, often called action buttons, in orange. That's like that, you want to click the orange button to push through to the next level. Familiar language, preventing errors, we've talked about this. You want to prevent errors as much as possible. When they go to a page that's not found, you say page not found. In the early days of the web, it just said like 404 error would make the page not found, but it just said 404. And you're just like, what is a 404? Normal people would not have any idea what a 404 error was. Now they just say like, sorry, something's wrong, or page not found. These are all things that you're probably familiar with, even if you didn't realize they were explicit design patterns. So here is K2, a navigation bar, which you see everywhere, having a string of tabs at the top of the screen. And the problem is that you need a structured way of basically organizing the main kind of directories of a website. First level navigation is often up here, and second level navigation is often down here. I'm sure you've seen this design pattern played out a million times. Here's another category in Hong's book on website kind of design patterns. This is L, or I, I guess. So this is effective page load. So using grid, which we already talked about, this notion of above the fold, below the fold. I'll show you an example in a second. Clear, first read. So what do you look at first? This is when we talked about the New York Times web page. There's a hierarchy of scale. So the biggest items are what you attend to first, and then it kind of whittles down to tiny things. That's what you'd read first. Expanding screen width. So this is the basic response layout. Big screen width is kind of a lazy person's strategy to make your website look everywhere, but it can be effective. Consistent sidebars. So when you guys navigate around, it isn't constantly changing. So now here's like Amazon. I know in the top left corner we get the logo and we often get like a little promotional thing in the top right corner here. The cart is often in the top right corner and it's shaped like a little tiny cart and it even shows how many items are in there. Then you often have like account information here, so the sort of expanded things around the cart. So your account on Amazon. Search field in the center, very common. Then you have your sort of first row, your kind of navigation bar of main things. So in this case it is like local deals, coupons, gold box, I guess it's all like a geohierarchy. Go to like app.com, you can see it's pretty similar. We have cart in the top corner, my account in the middle, search bar, logo in the top left, navigation bar, pretty much the same. As we get down to second level headings, so now we're doing this basically just search by category. So this is, we're in, this is an example of a breadcrumb, we're in clothes shoes and jewelry, and then we go to clothes shoes, handbags, accessories, luggage, jewelry, watches, and there's drop downs for all of those. Then you get down to the actual breadcrumb trail to the page you're on right now. So we're in clothing, accessories, luggage and bag, luggage, and then travel modes. It's showing us how we got this thing. Interestingly, Amazon also wastes screen real estate, very valuable real estate, actually regurgitating it on the left bar. It's kind of neat. It's kind of cool. It's kind of cool. It's kind of cool. It's kind of cool. It's kind of cool. It's kind of cool. It's kind of cool. And then another common design pattern that you see on most commerce websites and even search websites is sort of a filterization kind of thing. I only want to look at the ones that are made of leather, and so on. So like on Newegg, very similar set of kind of things. Again, Carb up here, login information here, search, tab, logo, search results, search modification. It's basically a carbon copy. And even though they're essentially competitors, they're basically duplicating each other. Social media is a little bit different. This is an older shot of Facebook, but again, you have search, logo, account information. This is sort of like secondary navigation. There's no, in this case, there's no kind of title bar. This is assumed to be just from Google+. It would finally die, but again, it's like, you know, search in the middle, logo, accounts. In this case, there is a little bit of sort of like a, kind of a modifiable bar here, but it's not quite the same as like a search field. You know, totally different things being search again, you know, account, search, logo. There is often, now I think they do have a modification that has sort of special links here. But again, very, you know, following similar things. Here's inside of an Amazon page. So again, you can see the action buttons are, there's a lot of things you can click on here, but the action button, the add to cart or buy now, they're verbs, they're highlighted in a different color. They want to basically draw your attention to that. You can see that example of small multiples here. So basically you can take a look, you can do a quick A-E kind of comparison of things to pick the color that you want. And this is, so this is when you get to Amazon, it looks like this. This notion of what's in your first view, it's called above the fold, right? Does anyone know the origin of that term above the fold? It's the thing that you first see when you get to a website. Let me go back. Exactly. So the above the fold is like, you know how newspapers, there's a big central fold, like what's on the newsstand, it's folded like this, like a sandwich, right? So when they say above the fold, that's the thing that's going to sell the newspaper. This is like Mueller report release, right? You're like, oh my gosh, like I have to read this right now. So you want to put all the really crazy stuff that's going to get you to sell newspapers above the fold. So if you're going to have an above the fold article, it's like a big deal, like on the New York Times, because it means you're like, not just front page, but like you're the front page that's on every newsstand in America. Below the fold is all the content that exists below here that you basically don't see, like you explicitly open up the newspaper and go inside. So everything that's inside the newspaper essentially is below the fold, right? Even if it's first page, it's still below the fold. So this is above the fold. It's designed from, they call it from the blueprint industry. Here's below the whole stuff. So the next most important information is like, oh, you scroll down because you didn't like those options. Well, let me show you some other options. Let me tell you some details about the bags and some other things. Then you sort of get the reviews. Then you just go all the way down. You probably get the stuff that you never even heard of on Amazon, like Shopbop or Book.com or Katta, Bookworm.com, I didn't even realize Amazon had so many of these web properties, but they do. And probably you would never miss it anyway. If we go down the action buttons, right? So we click on these action buttons. It's actually quite interesting what they do. It makes you follow a really nice template here. The one is they tell you your stuff right away. They tell you that if they give you confirmation that they can color that, you can add it to the cart. And then again, action buttons. More salient is proceed to checkout. They don't really want you to edit your cart so much. They really want your money at this point. So they're highlighting the proceed to checkout. They do make you sign in. Now look at the difference between these two pages. This has many options to read, right? You can go into other items. You can scroll up and down. You can go in all together. There's a Valentine's Day gift. There's crazy stuff. There's like probably a hundred links on this page to navigate you away. But as soon as you click on proceed to checkout, Amazon knows they've got you. And they strip away a lot of stuff, right? It goes to this. Everyone, I'm sure everyone's seen this Amazon screen, right? And now there's really, besides like some required like privacy notices and if you provide your password, everything is stripped. There's not even a search box to add anything else in your shopping cart. You probably can't see it, but there is a little kind of like breadcrumb trail of what you have to do to finish the checkout process. It's washed out, unfortunately. You can see it on this one, but not on the picture up there. And so you're at the beginning. It highlighted the very first one. And then as you go through this process, again, big buttons that show you where to go. I love how they give you shipping and handling right away. How many people have been to an e-commerce website, like some smaller e-commerce website where you want to buy some like weird servo motor or some unusual thing. And you have to type in all your credit card information, everything, before it tells you how much the shipping is. And it turns out the shipping's like $40. I've definitely gone to a lot of those sites, and it's so frustrating. Tell people you can estimate shipping with a zip code. You don't need their credit card to estimate shipping. It's like such a terrible, it's a violation of a very good design pattern. Just tell people how much it's going to cost in total, before they waste their time writing lots of little numbers in this one. So anyway, just follow this procedure, and then you hit that place your order, and you're done. So it's a very clever, deliberate action. Amazon does not really want you to deviate away from this thing. Once they go in, they want you to finish and get to the checkout. There's lots of things that make up a basic e-commerce website. Quick, slow checkout, really good, clean product details. It doesn't have to be insane. Shopping carts are a ubiquitous mechanism now. Shipping method selection, and all these things are super obvious, right? Even some meta ones, like easy return. So this is it. If you want to run a successful e-commerce website, make it so it's easy to return. Just have a little button, like Amazon does. Amazon makes it so easy that probably now, because I spend so much money on Amazon, buying crazy stuff from my lab, when I click return product, often they say, keep it. They don't even bother printing out a label anymore. They just say, we've made so much money on you, that we're just going to dislike it. It's like a decision tree in there. And half the time, I try to return something that says, that's not even worth shipping it back. So that's pretty interesting. They've really made it so easy that I just keep the things that they send me that are defective. I've also been getting a lot of weird packages from Amazon and stuff that have not worked. Has anyone experienced that in the last three months? We're getting some weird stuff. You get any of that? Yeah. Their whole business practice is all about efficiency. So I'm just getting these random boxes of stuff, really bizarre personal products and things. Yeah. It's a totally legitimate shipping label and everything. And then I open it up, and I'm expecting to get some batteries or something. And I open it up, and it's like, what was the last one I got? It was an ultrasonic wrinkle reducer. Maybe Amazon is telling me something. Maybe their machine learning has figured out something. I don't know. I mean, there is a great example that Target, right, I don't know if you've heard this example maybe like three, four years ago, but this guy sued Target because his daughter, who was like 14, was getting all these like pregnancy-related ads, like you can buy this sort of clothing and these like dietary supplements and all this sort of stuff, and he's like, this is so inappropriate, my daughter's only 14 years old, so he sued them, and then it turned out she was actually pregnant, and then machine learning could actually look at her spending habits and alter to fit a pattern of someone who was basically like early in their pregnancy, and so it came out in court that like we'll have to do a pregnancy test to like show, it's like, here, look, she's not even pregnant, they're like, oh, wait, like Target figured it out before the dad, you know, which is pretty crazy, so maybe it's just part of the new marketing strategy, like, oh, wow, this like really works, like I look 20 years younger, that's incredible, here's another good example, floating windows, you see these all the time, so the problem is you need to be able to show the customer extra information, but you want to maintain the context to keep the customer based on that same page, and you see this all the time in interfaces like this, where like Netflix or I don't know, like Apple TV or something, where they want to show you all the fancy titles to like grab your eye, but it'd be really annoying if you wanted to look at the reviews or the synopsis or that's the one with Tom Hanks in it, and you have to like navigate it and navigate down, navigate it and navigate down, so a pop-up, like a little pop, even though this sort of breaks the metaphor of a web page, now it's like a web page with like a third dimension, it's sort of turning into more like a desktop experience, there's really no notion of Z-ordering in a web browser, but nonetheless, it was a very successful mechanism to basically show you extra information for free. I'm not going to go through this in any great detail, but obviously there's lots of different organization patterns, so you can do alphabetical, you can do like task-based, where it's like I want to look for restaurants, not home goods stores and so on, right, so obviously there's many different strategies for how you organize information. So, if you're interested in going into a career in web design that links to some good books... kind of classic books especially on this, so much more reading if you want to. And then there's even some good websites on a website. Okay, moving on to metaphors. So what is a metaphor? Hopefully more people know metaphors than design patterns. How many people have heard of the word metaphor before? Okay, so does anyone want to say what a metaphor is or a simile? We can group them together. Yes. Okay, yeah, that's part. I don't know if it has to be always exaggerated, but yes. Yeah, a comparable. That's actually a great way to phrase them, right? So so I put the definition. So this transference of the relationship between one set of objects, one item to another for the purposes of a brief explanation, right? So this notion of metaphors has been really well explored. We humans use metaphors in the language and in their mental models all the time. And there's this great book called Metaphors That We Live By, and it's very much, they say that all the people we've encountered are very much like an example of metaphors. There's lots of examples in language, you know, when you attack someone's weak argument, but you're not attacking them like physically, right? You know, things that are right on target and that's far in terms of like, you know, like archery or something. Yeah, I know relationships that are going in the wrong direction, or things took a U-turn, like, these are all metaphors, right? Like, you don't like actually do a U-turn in your relationship, like someone's like, turns around in the room, like, you know, so. So these are all getting drawn on things. And certainly, not only in kind of culture and language, but we use metaphor a lot in HCI systems. We use metaphors to leverage existing knowledge about how the world works. So it's basically sort of like a mental model. So probably the most famous metaphor in computing is the destiny. So does anyone want to venture a guess on what the desktop metaphor is? We talked about it once this semester. Yes? So what does it mean in the context of computing? What is the desktop metaphor? Do you have like a little trash can at your desk? Or do you have a computer? Do you have pencils? Do you have a mouse in your real desk? Or do you have a pencil? A lot of people have calendars in their desks. Or they have a calendar on their desktop. Yes, so the desktop metaphor are all those things. Number one is we borrow objects from the real desk. It isn't that it's an exact photorealistic duplicate of a desk. There's some parts of this sort of comparison, breakdown. It's sort of a simplified version of a desk. It's a metaphor of a desk, not an exact copy. In addition to having items that we've drawn from the real world, like calendars, and folders, and documents, we also have this notion of things that overlap. Windows overlap, just like sheets of paper would on a desk. In the early days of desktops, I think it was graphic user interfaces, like Angular 4, if people thought this was the best, there was limited screen room, especially on these lower resolution monitors that start. And the most effective thing to do was tile. And you still see these in a lot of expert interfaces like IEs or Photoshop. There was this notion of tiling interfaces. And there was this leap. This was not so flexible. It didn't promote that freedom and control. And so there was this leap where we said, let's just treat them like they are paper, as basically a metaphor of paper where I can have lots of papers that overlap on my desk. If you show people two windows overlapping like this, they would say that's some sort of a graphics error. that the user wants to be able to see. But if you explain it through a metaphor, then imagine this is like papers on your desk, and they can overlap, and they have a Z ordering, where they have like a stacking order. So one on top will occlude or cover the other. Then people go, oh, yeah, I understand why they're like that. And obviously, if I kind of grab this one and move it up, then it'll cover the other one. So once you understand the mechanics, it was obvious. But importantly, again, we are the wrong demographics, because we've only grown up in the age of graph-based interfaces. If you give people a metaphor to latch onto, they'll understand the physics of that universe, or the laws of that universe. In this case, it's like a little mini-universe. And if you break them, then it falls apart. But if you tell people it's like a desktop, then they understand. So here is the example of this is the first commercially, medium-commercially successful computer with a desktop metaphor. Again, there was the 3Rivers Perk, which was a startup here in Pittsburgh that shipped the first commercial one. But that was like a $100,000 machine. This inflation adjusted the average of like $25,000. So it's still expensive as hell. But nonetheless, people were buying them, and they were mass-produced. And this is what that desktop metaphor looked like. So you see there's little tiny stacks of paper. You see this little icon that looks like a clipboard. There is that little waste basket here. And again, we start having overlapping because we even try to have this sort of synthetic shadow here to show that there is even that C-word, and to emphasize that C-word. There's many other metaphors that exist in these things that are basically barred from the rules. How pound does work, how roll of x works, how 2D does work, the whole notion of a file hierarchy. You could have made, it would have been maybe computation difficult. But there's no reason to only have a hierarchical file system. You could have a database file system where everything is done by a search. You just type in three keywords, and it shows you what those three keywords are. Or instead of putting it into a folder, is you label all your files with basically meta tags. So I say this is the NCS 2019 spring lecture on visual schema, like comma, comma, comma, comma. And basically, just like I would. Right now, the way that DHCS is organized in my computer, it just has like, DHCS, you know, spring, 2019. So I just made the hierarchy, but there's no reason why you have to have the hierarchy. It could just be a flat thing with just kind of a keyword, a search term, and that's it. Other metaphors that exist in modern interfaces, like cut, copy, and paste, those are definitely drawn from the real world. They're metaphors. And obviously, the computer isn't inside, like actually cutting it out with like a little pair of scissors or something. It's just all graphical operations, but we understand how they work by applying this And I would suspect, other than maybe elementary school, no one has cut, copied, and pasted in a very long time. So, what are radio buttons? Okay, what is a radio button? Can anyone just tell me what that is before we get to question number two? In the back. It's like one position. Sort of. That's one maybe stylation of it. Stylational word, I don't know. Yeah? Right, so they tend to be kind of round, and they're often in a set of like four. And radio buttons are different from checkboxes in that you can only select one, right? So checkboxes, you can select four checkboxes, like you want block, and intervenes, and barbicode. They're not exclusive. They wouldn't be using radio buttons, okay? Where does that term come from? We know why we call checkboxes checkboxes, because you literally put a check inside the box. But why do you call the other type radio buttons? I'm not so sure why. I think it's kind of like old radios. When you want to play, pause, stop, you have to press like 100 times. Yeah, I mean, that is almost exactly where they come from. Often on old radios, you can also only tune to one radio station. So there's a mutual exclusivity. I can't turn to 82.9 FM and 86.3 FM. I can only select one at a time. And likewise, your example of I can't pause and play, or like stop and play at the same time. And so in these old mechanical sets, it's like if I press this, the other ones pop out. If I press that one, then that one pops out. And that's where radio buttons come from. They literally came from radios, because radios had to have mutual selection as part of their mechanical interface. And so we still call them radio buttons today, because I bet a lot of people have not operated a radio like this in also a very long time. And indeed, many of the widgets that we have, which are sliders, come from mechanical sliders. Tabs come from actual tabs. I mean, the way that Chrome even stylized it is very tab-like. It isn't even sort of like a modern interpretation. It literally looks like real tabs, and there's even a Z ordering among those little tabs. So many of these things are drawn from the real world. But they're not exact clones. They can do crazy things. And because they're digital, they can be more flexible. But nonetheless, they're drawing on real-world metaphors. This is that example I keep on bringing up, which is this metaphor really makes no sense, where if I want to eject it, if I wanted to eject this and hold it back and drag it through the trash, that's a very weird way. Putting something in the trash is not like eject it. Eject it from my light, but it doesn't eject it from my computer so I can access it later when it's gone in a trash can. So this is an ineffective metaphor. Now, people started thinking at the birthplace of kind of graph-based interfaces in the desktop metaphor with things like the Xerox Star. I remember we watched that video where they kind of walked people through what all the metaphors they were drawing on. People thought the future was going all in on metaphors. They're like, this is working so well. People understand that we can stack papers. Maybe we should really embrace it and take it to an extreme. But here's an example of a system called Magic App which is an alternate desktop experience. And you really had life with desktops. You saw your desktop in sort of reality. It wasn't even, there's a blurring line between a metaphor and almost a real thing. And on your desktop, you have the various apps. So you have a way to make calls and have like a contact so that you can have a way to write letters. And then you take your little letter and you put it in the outbox. and an inbox comes in here, there's your clock, it isn't like up here, it actually is like a clock, and here's I think a photo album, so there's an album, but it's like appeared as an album, right? And then of course there's other things down here, and that's because you can actually, if you wanted to do something else that was not office related, you have to leave your office, go out on the street and move back to, for example, your home, or go to, like this directory of people in your company. So you'd actually have to leave your little metaphorical room, like go, I don't know if you have to get a bus or something, but at least like walk down the street or something, and then get into your next thing. So here's actually leaving your office, leaving your desk, you go to the library to access your system controls, and then you'd be leaving it and go and explore the neighborhood. So this started to blur the lines between the benefits of a metaphor and really taking it all the way to an extreme. And this was a massive failure, a magic trap, but Microsoft, in their infinite wisdom, said we're still going on in this. So in parallel with the development of Windows 95, which was really their breakout desktop system that really cemented their kind of monopoly over Apple in the 90s, they built this thing called Project Utopia. In their own project name, they're like, this is the future of computing, this is the utopia of computing, and it looked like this. So this was the ultimate Windows 95 interface that they co-developed, and you have the emergence of basically Clippy. They needed an agent because you couldn't just have a weird AI in your environment. If you're going to make everything realistic, you need to be personified in some way. You could build a dog and eventually build that paper plane. But here, again, you have all the little bookshelves where you store your stuff, you have your little crackling fireplace, here's a clock, here's a communication terminal, this is the web, here's your inbox with your mail coming in. I don't know if it's biological will, but for time reason, I think there are other doors as well. It has the same sort of thing where you leave this room, you go to other parts of your house and have different functions, like the boiler room to modify the settings or something. a massive failure. It's actually a real stain on Microsoft's record. No one at Microsoft ever talked. They didn't call it Project Utopia. They renamed it Microsoft Bob, which also I think is a massive marketing fail. What a weird name for an operating system. But luckily they've gotten better at things. And a lot of people jumped on the bandwagon. Even after Microsoft Bob, IBM was trying to produce all these metaphors, all these objects, they really focused on the game and making it easier. But what's nice about metaphors is that you can simplify things. I'm trying to think on the spot of a good metaphor, but you can take some really complicated human behavior, and I don't know, I don't have a good metaphor. It's impossible to come up with one. But if someone was withering away like a rose that you didn't water or something, there's a lot of imagery packed into something like that. Metaphors are effective because they can simplify. They boil it down to the essence. And it doesn't become effective if your metaphor is now like, oh, they were a red rose, and I didn't give them the fertilizer on Monday, and so then it got kind of dry, and then the whole point of the metaphor falls away. And that's sort of what happened with these desktop metaphors, is that they got so hyper-realistic and so complicated that it lost the benefit of being a metaphor. And so now we don't really see it nearly as much. It certainly did continue. Apple's design trend was until really iOS 6, I think. It was very picture-perfect. You have a library of books in your iPad, even on macOS, and it was like a little set of books sitting on little shelves, like a real library. And it wasn't even a metaphor of a library. Right now we have a library of songs in iTunes, and it doesn't look like a library. Apple is actually making it look like a real library. And then save here, it's starting to become less effective. You still have this little floppy disk icon for save. Clearly that's a metaphor that is sort of long, long went, long gone, and yet we still recognize it today. Nonetheless, the shopping cart is an effective metaphor that you pick with us on e-commerce websites. OK, so highly related to metaphors are affordances. So the best way to think about this is, when you visit a website for the first time, or you open up a friend's operating system for the first time, how do you know what is a button? I gave you a preview copy of iOS 20. You've never seen it before, or whatever. New Mozilla, like Linux, whatever version of a touch interface that you've never seen before. How do you know, generally, what's a button and what's not? Someone new, OK? You all do this. It's not like you've not clicked buttons before. Yeah? There's more of a button when I hold it. OK. And what is it? It's like a little bit of a fringing of darkness. But what does that tell you? It's like a shadow that's elevated. Yeah, so it feels sort of elevated, like it's kind of popping out of the screen a little bit with a little bit of shading. Yeah, that's a good cue. Yeah? It's not bringing you to a world of it. OK, so you know it's an action. It isn't just like, cool. Right? I mean, I guess that could be like a bird. But yeah, do you have an idea? It's probably bringing you to a world where it's easy to communicate, but it's not easy to do in a computer. It's like on macOS, the cursor turns just from an arrow. It's like a little tiny finger, right? So all those are good clues, right? It may be that only one or two or three of those possible designs are incorporated, but they all give you a sense. An estimation that something is a button. So those are all great examples of how you can figure out something is a button, even if you've never used an interface before. So this guy named Jim Gibson, who was a psychologist, he introduced this theory in 1937 called the theory of affordances. And it's evolved a little bit over the time, but it was the start of this idea. So affordances of an environment is what it offers to the animal, in this case users, of what it provides or furnishes for either good or evil. So affordances are basically properties of an environment that are. independent of our perception. So there are things that are expressed by the environment, but they're not necessarily perceptual. We can perceive them, but they're not like some artifact of our perception or our cognitions. Now, later, Don Norman, really interesting guy, he's now basically like Emeritus at UCSD, he extended this notion of perceived affordances. So the term affordance refers to the perceived and actual properties of a thing, primarily those fundamental properties that determine just how the thing could possibly be used. Affordances provide strong cues to the operation of things. So in the case of that button, that sort of notion, we sort of darken it or we give it some sort of like, maybe like a pseudo three kind of look, maybe this little shadow, that notion that sort of raised from the surface, you sort of want to go in there and push it in, is that visual cue that's telling us that this is probably clickable. And same way that we can, for example, a cursor, so it becomes like a little icon that we know that, oh, that looks like sort of like a little figure clicking on it, that's also going to be a cue to understanding what its operations, right? Yeah, I'll show you some other examples. So he published this book, which is really interesting and a highlight, a really good read, is The Design of Everyday Things. And really what it's doing is that if you take advantage of good affordances, is that a user knows what to do just by looking at it with no experience on the computer. And there's affordances everywhere in graphic user interfaces that make it so it's easy to use, and metaphor is an important part of this. So, you know, one of the most, and it's actually called a Norman Door, is we probably all experienced this example, okay? So how many people have walked up to a door, including at CMU, where they have a handle on it and you pull it, but it turns out it's a push door? How many of you have experienced that at least once in their life? If you're not raising your hand, you're a liar. I encounter it like at least once a semester, like trying to be like, I can see people inside the store and they're like, oh, and then it's like, oh, and that's because it's designed for this kind of grip, right? What it affords is you to wrap your fingers around it, and why you do this is so you can pull, right? It's meant to pull. It's meant to facilitate and express to you that to wrap... your hands around and when you're resting your hands on it, you pull. It'd be very weird to walk up to a door with a flat plate because you'd have to like get your fingernail behind it and like pull it open. It doesn't afford the notion of opening. What a flat plate does afford you is to put your hand like this and if you put your hand like this against something, you tend to want to push it, not somehow weirdly pull it. So when you find good design doors, and there are many, this is an example of a door that is a bad design. The only visual key you get on our door is that there's a door opener on the inside and that does give you a little bit of a visual cue. But good design doors are behaved like this. I hate doors where they like, the employees have to like stick a like a sheet of paper on it that says like pull instead of push. It's like we just think you're trying to like get into this like restaurant and it's really confusing. So when they get it wrong, when you find an example of a door that gets it wrong, it's called a Norman door. Anyway, so they're really about the clues of the operation that make it a therian. It promotes certain actions. This does not promote pulling. This promotes pulling and that's therefore its importance. Now here's another example of a door, okay, a little roundy knob like that, and actually the one in our classroom is a good example too. What does this facilitate? It actually facilitates two things. There's two clues here about the operation of the door. Yes, twist and pull, right? So number one, you know it's a pull if you didn't get your hand behind it, which you know is going to facilitate pulling, but it's also a circular object which sort of conveys that this might be a twisty behavior, right? If it was a square or something, the fact that it's sort of out and there's an axis of rotation sort of suggests that it can be. So you know the classic example of like a teapot, right, how to hold a teapot, this is a big importance that I'm going to grab a teapot like this, probably because the contents are hot. I could grab it like this, I could grab it from the bottom, I could grab it from the spout, but those are not the logical ways to grab a teapot, even if you had no idea what a teapot is, if you just gave it to someone who'd never seen a teapot in their life, they're much more likely to find that it's a teapot. design of this object to take this up by here and probably take up this top by here. Yeah? Is there a key handle for returning to it? No. It's basically the same. And for that reason, that's why on these sort of doors, the drop is in less than a situation. It's pretty much net. So they get the design of the axis of rotation right, but with not a great conveyance of wonderful, wonderful push. Because the problem is, you'd have to have something that doesn't facilitate your wrapping around. So if you do want to have a twist and push, it'd probably be something that doesn't have this relief. It'd be like a cylinder that you can twist, that you can't get behind. Once you get behind anything, it almost always conveys some sort of force is going to be needed in this direction, as opposed to a flat surface. So yeah, it's not always perfect. So here are two examples. You can tell me what these convey to you, right? Because a good object, an object with good affordances, I don't have to explain it. You just can talk to it, speak to it. So what, when you see something like this, what do you want to do with it? Show me what you want to do. Yeah, so you just want to, and you're not going in like this. Everyone would go like this, right? It's like, the baby thought you wanted to pound that thing, right? Like emergency alarm. How about this one? What does this convey to you? So it's twisting, and what else is it? So why do you think it's sort of knobbly like this? So it tends to want to show you that it needs some energy, right? It could be hard to twist. If it was perfectly smooth, you'd walk up to the bike, random study where it had like, this was like sort of a non-grippy thing. You'd probably apply less force initially. But if you convey it like this with sort of a grip to it, people are going to naturally apply more. They're going to grab it a little bit harder, and they're going to twist expecting it to have more force. It's sort of like, the way that you can, like sometimes you can see this. People do modulate their grip strength all the time. How many people have picked up like a milk container that they thought was full, but it turned out to be empty? Right? I've done this. You're like, whoa. And your body is automatically thinking it's like a. a gallon of milk in it, but it turns out it's like your roommate's a drunk and all, right? So that's an example of your body doing a lot of this automatically based on what it perceives is correct. And if you ran that experiment, you would apply a lot more twist towards here. So again, in the kind of digital domain, originally there was, people knew they needed something like a hat, right? But it wasn't like they figured it out from the beginning. So they knew that they needed to segment lots of different options into little packets, because otherwise there's just too many things. So originally, Mac OS had these weird scroll bars, and you click the one, and it would basically change the view of this. So it's almost like a radio button, like an icon radio button style. And what they realized is that it was a very weird way of displaying things. And so what they did is later on in Mac OS and subsequent operating systems in general, is they said, well, basically this is the same thing as tabs. If I have tabs, I can open up, like in a phone book, there's tabs as well, and I can open up a key, and I see all the people with last name T. And so they ended up borrowing and adding tabs as a much later thing. Mac OS 8, I think, introduced widget for tabs. So it wasn't like they nailed it right from the get-go. There was no tabs, for example, in the start. There was no tabs in the release, and it came actually much later. So these metaphors have been slowly creeping in over time. And this is a good example, because people understand that if I click this, that this becomes the active tab. And also really nice in this design is that this tab is connected to this thing. So it's very clear to me that themes is the active tab. And if you click appearance, this does not restore. Appearance would say where it is, but this bottom line would disappear, and it would be glued on. So that, I think, is a really nice, effective design. It's basically what Chrome has used 20 years later. This is the new version of tabs in the later versions of Mac OS, and then they get it worse. So this is their notion of a new tab bar. So there is a sort of container, which is sort of tab-like, and then you have sort of This is no longer a metaphor. It's the same mechanism, but it's a different design. But when I have tabs in the real world, they don't indent a tab and it pops to the top. So this is actually a broken metaphor. We get five because we understand what tabs are in general. We even still call this widget a tab bar. But it's actually less intuitive than I think the older designs. And of course, as Mac OS, I'm sorry, iOS has progressed over the ages. What are we on now, 12? 12, yeah. It's around seven. Like, they've changed it substantially. So John and I took over from, what was that guy's name? Scott or something. Does anyone remember these? Scott Pruessler? Forrester? Good memory. So everything in Mac OS was definitely much more like traditional designs. So buttons looked like this. And then when that guy got fired and John and I took over, he went for this super minimalist look. And the new button design became like this. If you turn on accessibility options, it will render this. But it didn't go back to this, which is what people were complaining about. For example, in that Microsoft 8 video that I showed you in the sort of debrief after I showed you that video, you have no idea what to click. A heading and a button basically look like the same thing. And it basically has lost all of its affordances that it is a button. And again, the only reason why most of us are able to make this trend is because, number one, we thought it was confusing, so we kind of pushed through it. But also because we kind of recognize where buttons are and generally what they say. So we lost sort of the declaration. We still understood it to be a button. But if you're a new person, like if an alien came and tried to use this interface, they'd be much more confused if they've not seen this before. Yes? And then just like in New York, the design practice of instead of being like setting it in the real world, our new system networks, like remember when iOS 7 is a thing. Is it something like this? Because now we're using a bunch of that. Yeah. I mean, I think it is happening. And that's essentially why we moved from iOS 6 and before to iOS 7 is that we're able to leverage our memory. So as design trends are changing, like our mental model can basically survive beyond. So I think that's the reason why we've been able to make those upgrades that really could get into the web. definitely see people confused by newer versions of IOS. So yes, you can make those design trends when it's incremental and when the design is reasonably good. But in the case of Windows 8, it lost all these affordances. Buttons are invisible, and there's no idea where to click, and then they moved so far ahead of what the Windows XP lineage is, like Windows 7 lineage, that people just could not catch up. And there was literally people like that woman who just could literally not even close an application because it was so foreign there. So it has to be done very carefully, very strategically, in order to work. But yes, it may be that when we're all super old, we'll be remembering sort of the mental models that we built as kids using things like iPads. Or we'll be super confused, you know, like, who knows? We'll have to see. You guys are on the vanguard of designing those interfaces, so please make it easy for old people like me. Yeah? I'm looking forward to you making better, like, testing in the real world rather than, like, the digital world that we've created. OK. I'll get to that at the end of the lecture. OK. So this is not getting good. So here's another good example. OK. So when you see this, OK, what does it mean? OK. This sort of icon exists in many different interfaces. No one tell me what it means. What is it conveying to you? Yes? Resetting. OK. That is what it functionally does. What else is it? What do you think its affordances? Yeah? I mean, like, gripping. Gripping. Yes. Exactly. Right? So it says, grab me here to do something, because it's exposing, like, little grippy marks. Right? So the affordance and the function are sort of related. But, like, the affordance of a door is like, I can wrap my fingers around it, which is going to facilitate me, like, pulling it open. So they're related. And yes, this generally is thought to be that it's basically facilitating grip. And it comes from your first best IT word of the day, knurling. So the tools. is have these, you know, I haven't seen this on power tools, but also on hand tools, that they, where they want you to grip, they've added special grippy things. This is called NURB, and it increases friction. So when you see this, like a chuck, I think, for a drill, is where they want you to grab it and tighten it, is they're going to grab here. It's sort of iconic, that I know that's where to grip it, but the importance is that this area affords a greater grip, and it's a logical place to turn. And that's actually what they borrowed this thing into. Not to interface this. Here's the second essay three words of the year, or of the day. Okay, so skeuomorphism. Who's heard the word skeuomorphs or skeuomorphism before? Okay. So do you want to venture a definition of skeuomorphism? Yeah, so it's when you are borrowing a design, a language, off of objects, and you're bringing it into a digital world. That's how we, the modern usage of it, but actually the term dates back to the 1890s. So what they actually found in this great book, it originates in archaeology, is they were digging up, I think, ceramics in Mesopotamia, and what they found is the original vessels were made of basketry. They were basically woven baskets to hold rice grain, or wheat, or fruit, whatever it may be. Then they started making ceramic versions, so clay, fire clay, ceramic versions of these pots. And what they found is that the decoration of the pots looked like it was woven. So what they were actually doing is basically imprinting sort of like rope into it, so it looked sort of like the old version. And this was purely decorative, so it was inheriting this style of this basketry, even though it was totally ornamental. They didn't make the ceramic any better to make it look like it was sort of woven. like the pseudo-Rothitka kind of like design, it was purely ornamental. And that is what skeuomorphism is, when you adopt the design elements of something for a purely, well not necessarily purely aesthetic, but basically you're designing the ornamentation, you're borrowing the ornamentation and applying it to something different that does not need it. And so you can see some things like this, where like you have these cars with these wooden panels on it, sort of like, everyone's always like, why would you ever make this car with a wooden panel on it? And this is hearkening back to like caravans that were kind of Oregon Trail style things, and they often applied to wagons and cars that are labeled with wagons for that reason. So this is not, it's not necessary to have wood on this like Ford car, but they're kind of borrowing this design element previously. I just got a copy at this weird little place in Bloomfield recently, and I noticed that the straw was metal, which is kind of cool, but it had these little design accents on it because it wanted to replicate a bendy straw. It was not bendy, they were purely decorative, but it's an example of skeuomorphism, where they've made a metal straw and they've borrowed sort of a design element from the plastic straws of yesteryear. It's kind of, I think it's a recycler, they want to reuse all those straws. So skeuomorphism was everywhere in iOS 7 and before, and even in parts of macOS, so I'll show you some examples. So this is what iPhones looked like pre-iOS 7, right, which, how many people owned an iPhone pre-iOS 7? That's probably like seven years old or something ridiculous, right, so, but you probably don't really super remember, but it seems like some of you are. Maybe I'm just getting time warped, but a lot of things look like this, but your notepad application actually had like a yellow tablet with a little line and a little line to help on it, and of course the lines on notepads, what are they there for? To help humans write in straight rows and have a constant margin. The computer doesn't need this, right, the computer can write straight lines, it knows what it does, it takes a coordinate system. So this is purely a skeuomorphic design orientation. Here in the games application, this is sort of like their, you know, it's kind of, I guess like a precursor to Apple or Tables they announced, but this is actually got sort of like a casino style, so if the hat got And back here, if you look closely, it's actually green felt, like you would have on a billiards table or a casino table. And this actually is a wood grain, like you might find on a green flag or something. So all this design recommendation is totally not necessary. These are just gold and brass accents on all of the widgets, just to give it that kind of Trumpian feel. Here's a library example where we have little books on little tiny shadows, little tiny drop shadows, that your digital library is like a little supermarket library. Here on the iPad was the contact book, so you actually have the little thumb holes, and you put your finger on J and you can lift up all the pages. And there's even a little ribbon here. There's even little stitching. There's little leather stitching on things as well, which is pretty crazy. Here, even on the bind, look at the bind there. It has little springs going through little holes to keep your digital-bound book together. So this is all fluff. It's not even required for the metaphor to hold true. You can have a much simpler design, and people would still understand this to be a contact book. But Apple was superimposing all this skeuomorphism on it. And this exists everywhere. It's like everyone for a long time had these little cardboard volumes. The notion of page turning to some action, or some e-book readers have this swipe and you have to peel a page over. There's no pages in the book. In fact, there's no reason to have pages at all. It could just be a continuously scrolling thing. We purposely borrowed the metaphor of pages because it's a very convenient way to, when you're reading a book at night, you don't just scroll through all of Harry Potter. It's nice to have something like a unit of pages, a nice human unit, and we can turn the page over. And that's definitely skeuomorphism. There's no reason to have that. Other than it's a nice crutch for people to think about. So skeuomorphs also don't have to be visual. We talk only exclusively about visual skeuomorphs. Can anyone think of any auditory skeuomorphs? So things that sound a particular way, but it's purely essentially ornamental. There's no reason for them to sound like that. They're built on things from like yesteryear. Yeah? Like a digital alarm clock. Yeah, or like a phone call, right? Ding! Yeah, there's no reason for that. That's a great example. That's probably the most classic one. There's a lot of other ones, though. Yeah? Yeah, exactly. It's for the mechanical clicking sound. Yep, that's another good one. What else? Yeah? So that would be an example of a haptic skew more. Like you're trying to simulate like it's a mechanical keyboard, but that, you know, it could be something totally different. It could just scream at you or something when you press the button. Yeah? For like an alarm clock, too? Yeah, that's really like 1800s. Yeah, where like digital envelopes are flying through the air? Yeah, that's a good one. Any old timey boys? That would probably not be considered a skew more per se. Yes, because it's not that it has to be retro. It's that it's not really an auditory ornamentation, right? You're applying the skew. So it would be an accident. That's an interesting idea. It'd be weird. Like would you want Siri to talk to you like in an old timey accent or a Shakespearean? I'm born in the UK, but I'm very sad I lost my British accent. Yes? This is a little bit old, but dial up that tone. Do you have a way to link up to like, you know, an app? Yeah, and also touch tone. Like in your app, if it was like beep, beep, beep, beep, beep, like that's not necessary. Like there's no reason for it to be a different sound. That was because literally the different sound was a way to encode to the switchboard that you're pressing different numbers, right? Yeah, one couple more examples. Yes, that is a synthetic. Yes, so that is a holder, right? Mm-hmm. Yeah? Doorbells have ding-dong. Sure, right. So if you get a doorbell, yeah, right. So don't make such a ding-dong sound. So the other good one, just so we can move on, is when you take a photo, you often get that click of the shutter. But digital cameras don't have, you know, things like mirrors, don't have a shutter. Your smartphone certainly doesn't have a little mechanical shutter, but you often get that click, you know, and it makes a little sound, right? Last one. Yeah, yeah. Which examples are those? I was having the conference day. It would be weird to use totally different sounds, people would probably find that strange. So yes, to be a skeuomorph, it does mean you take something from the past, or at least some other object, and you're transposing it for, not just aesthetic, but as a design overlay on top of something that does not need it. So it's not required. You could build the thing without it. I could build a pot without rope braiding. I could build an alarm clock that does beeps and beeps and doesn't have that sound in the ring. But nonetheless, adding this design element makes it a richer experience, but it's not a requirement. That's really the essence of a skeuomorph. It's purely to make the experience nicer, but it's not a functional requirement. So there's been this huge debate about skeuomorphism. Number one is that it's much more difficult to design, because you're going to build this contact book with little stitching and leather accents and so on, that's much harder to design with a skeuomorph. The second is that you're going to have to design it in a way that's not graphical. It's like a plain white background. It's a simple elements. They tend to take up a lot more screen space. By nature of superimposing on all these decorative, ornamental things, you're just taking up pixels and making it more cluttered, not necessarily drawing people's attention to the right thing. Early on, we were doing Numeric. feedback with things like rotary knobs. Early versions, I think early versions of QuickTime did have actually like a rotary knob for I think sound. And it was just, it was really confusing because like you don't need a rotary knob on a computer. You should get a photo of this because it is really interesting little space. And we do have a holdover that the sound thing that they made in iOS, which is now sort of a common design pattern where you sort of move like up, you set sort of the digits this way to set like a time is sort of really barred from the skew-morph thing. And it's not necessarily the ideal way to set a time. There also is increasingly true that people are no longer having experiences with the devices that we're trying to emulate, right? So like having like a Rolodex, for example, like I've never owned a Rolodex. Probably very few of you have owned like a real Rolodex, like everyone has a little card that sort of spins around in a circle alphabetized. And so like, that's a metaphor that's just not working so much as it used to, right? Or even like a physical like calendar or like any of those sort of like note-taking things. Like we've just moved into a digital world. A lot of those items are just irrelevant, right? And touch tones and stuff. We don't even know necessarily what those are. And more importantly, what people would level against it is that skew-morphism really limits the creativity, right? Like we're no longer bound by having to build interfaces that are built on the physical world, right? Digital innovation is this breakthrough. We can think of totally new paradigms. We don't have to think about only using shopping carts. That's like an old school way of thinking about shopping. We don't have to only think about, you know, ringtones that, you know, make little bell sounds and so on. We can build entirely new experiences, entirely new metaphors, not even metaphors at all, just entirely new ways of doing things that might be free. So we don't, you know, the physical world is truly limited because it has to constrain the physics. The digital world does not have that. On the other hand, you know, grounding experiences and physical things gives us that knowledge of how to latch on and use things. It offers those affordances. Like if I see something that's sort of shaped like something I know, I'm going to be able to learn it so much faster, right? So I think, you know, even though interfaces like both Android and iOS, Mac OS and Windows have really pushed far into this minimalist, flat design view, I think actually that Pentium is almost going too far. I think iOS went too far in stripping out basically all the affordances that I think made it actually a good operating system. So, I think the reaction is really overblown, and we still, if you actually look deeply enough, you realize that we still have a lot of these things. There's still sliders, there's still radio buttons, there's still tab bars, there's a lot of actually increasing skew work that's in things like parallax and blur. So, you know, as you have items that are behind, they're actually getting sort of like faded out of blur. We don't need to do that. That's purely ornamental to sort of give you that sense of depth to help you understand that this is like a stacking in Z. And, you know, indeed, like, Google was going to actually, luckily, to talk to, what's the guy's name? Mathias Dorthang. He's the chief Android designer. He's actually from Google. And he was the guy who was responsible for basically doing the redesign for this material redesign. And I asked him point blank. I'm like, is material design skeuomorphic? And he gave me this huge smile. Because basically, their whole design process was to make Android much more skeuomorphic. And then that skeuomorphism got toxic sort of three years ago. They changed it to material design. But it's really a nod towards skeuomorphism, because you're saying you're building design out of like real-world materials over this card-style interface in Android today. There's shadows, there's divisions. There's even looks of like little index cards. Their whole strategy of how to design these minimalist interfaces follow a lot of things that we see in the real world. So I would say that, actually, we're still in a very skeuomorphic era. And, actually, the best way to do it is somewhere in between, where it's sort of minimalist and clean and friendly to use, but still leverages what we know. OK, we will talk more about. So now I've got negative two seconds left. It's the beginning of next class. OK, fine. You go. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class. Next class.\",\n",
       " \"Okay, a lot to talk about today, but we'll kick off with a little bit of a recap of what Final last gasp questions on MAKEOP 2. Hopefully you saw my clarification email, where I kind of revised my stance on having to sort of pick up the logo square at every opportunity. You can sort of think more of this task as you have a logo in your website, for example, and you're gonna manipulate it into those. That's sort of the framing here, what the kind of task is. And again, simplifying it, but you can think about moving a logo around between those various spots. Where I initially said, imagine as many logos on the screen. That's just imagine as one logo, but obviously many possible destinations. The second thing that you wanna make sure that your code, definitely make sure your code can run with different numbers of trials. I believe the scaffold code is set to 12, but it should also work for one, or 100, or whatever we choose. Leave the DPI at 72, unless, especially on Windows, sometimes it does honor it a little bit differently, but it should be plausible, it should look similar to the other people's code. And generally you should leave it at 72, because at least on Mac, processing is already doing a scaler to account for things like RedNet. But if you're finding on Windows, you're getting ones that are like this big, like the head of a pin, then you probably do need to change the size of the DPI. And then finally, make sure you're using the very latest target generation code. I updated the scaffold code in email clarification, where we changed it to the minimum target size to 0.25 instead of 0.15, which is a gift, because every previous bake-off that I've run this on has had to use 0.15, so I've made it easier for you guys, and I'm expecting super fast times. I already have pulled a couple teams, previous bake-off times on this, so I feel like the entire, everyone, it's not identical, because I changed it this year to make it a more realistic task, but it's similar. And last year, the best teams were sort of in the two to three second range, and the worst teams were like. in the eight, nine, 10 second range, okay? So, let that stew on you for a little bit. I'm trying to think if there's anything else. I like best times. So, any questions on the VACOM? How many people are feeling, how many people were feeling confident before I just told them those times? I was after it. You could, I already know there are some teams that are in the three second range. So, that's definitely possible with the current rules. So, you should be okay. There is chairs down here if you wanna drop on down there. Okay, I wanna just very quickly mention the peer review code. Are you sure you wanna be down there? Grab a desk down there if you want. You might need a desk in like 30. So, just a quick note on the peer review. I posted everyone's peer review scores. I did this myself. I went through everyone's comments. I did not give this to the graders because I wanted to see how, just a little bit of detail of where I start. The very first thing I do before I even write even one shred of a comment is I do the following. Is I plot what you, when it's like one, is I find it here, one means this is the score, like the average score that you gave yourself. So, this is averaging over all those columns. And then I look at what your peers rated yourself. Okay, it's an obvious place to start. And what I do is I look at the difference between these. So, in this case, if it's a negative score, like this negative .5, did that person actually rate themselves below what their peers did? And actually, overall, you guys are very humble. The average, if I average across all students, most people actually evaluate themselves less than their peers. But occasionally, there are people like this person who rate themselves a full one point on average higher. That's like the team gave them two, two, two, two, two, and you gave yourself three, three, three, three, three. That's pretty. And those are people that I dig into and I read all the comments in quite a lot of detail. So you should be honest and reasonable. It's not that this is necessarily a wrong score, but it does mean that I scrutinize and I actually end up sending probably something like a fifth of the class emails clarifying some of their strategies. And again, it was our first peer review, and so people sometimes misunderstood the questions. But again, it's not that I'm looking. If you have a team that says 33333 for everyone in your group, and it was a couple groups that did this, I emailed them and said, so you're telling me that at every moment of every day, there was three hands on that laptop making the video, all three people were coding in parallel. This is not possible. A good team, as we talked about in group work, is you're different people, you have different skills. And even if you had identical skills, you would still divide up the tasks. One person's going to take more of the leadership role in this and less of the leadership role in that. And that's natural. That's good. That's called effective teamwork. If all three of you are typing on that keyboard, that's the dumbest approach possible to putting together a video. So I'm not expecting that. To get full points, essentially, on the peer review, I'm looking that you had a substantial or significant contribution in two or maybe three of the seven categories. If you're good at three in all seven categories, then there's a tear coming out of my eye for this. It's either a lie or it's a very dysfunctional group. So keep that in mind. Same here. I'm looking, and I'm having to concoct you again the what percentage you think that person did the work. And I saw this in my collab here with the team. I thought that person did around 33%, so about a third. But they thought they did 80% of the work. That is a bit of a mismatch, because that means the other two people only did 20% collectively. So again, just to give you a bit of insight, it's just to get people to be honest, don't try to game the system. I will warn you that the only people that have been ejected out of this class for disciplinary action, it's not because they cheated on the box quiz or anything. It's because there was clear collusion among the teammates, where it was so consistent that they were like, I'm going to get you. I'm going to get you. I'm going to get you. I'm going to get you. I'm going to get you. I'm going to get you. to get penalized, you can look at your grade, almost everyone got D's or A's for the period. Okay, moving on, and you're welcome to ask me more questions in the peer review or send me an email if you have any questions about your grade. The last lecture, we sort of blew through these three topics, HCI topics, affordances, metaphors, and skeuomorphs. And I wanted to review them just so you have a clear picture in mind, okay? So number one, affordances, these are those cues that an object or an interface or something gives you that tells you how it works, right? So the classic example is a kettle that has a handle on it that affords that notion of that's where it's going to be where you drip it, right? On a door, it has a push plate, that's the design of that door has an affordance that facilitates pushing. And then the classic example in an interface is that you have a button or even a radio button that sort of looks button-like, you sort of want to go in there and push it in because it's design, it's graphical design, tells you that it's sort of meant to be squished in. The other one we talked about is that knurling on the corner of the screen, that that sort of facilitates that gripping, right? Metaphors are a little bit different, that's where you take something that's how something works in the real world and you're sort of transporting its functionality into the digital world. So like how your desktop works where you can lay lots of papers down is how it works in the real world. And we sort of transported that idea into the digital world by having sort of this notion of overlapping windows. So on the desktop, I can't necessarily have like an Adobe Photoshop running, but I could have this notion of each one sort of like an interactive piece of paper that I can pile up. So in this case, it's not like quite exactly a metaphor like we use in English language, when we talk about metaphors in the HCI world, I mean the sort of sucking of the conceptual nature, like the essence of a function of an idea and transporting it into the digital world, like a trash can. There's no trash can on your computer. It isn't like when you delete a file, like some process picks it up and it doesn't even move it anywhere. in the file system, it literally, on the file system, it just marks a bit that says, like, delete. It doesn't even copy it into a new folder, essentially, right, so the trash can is purely a metaphor for us, the user. There is no trash can in your computer, it's just an idea, it's a concept that was stolen from the real world, and it's a handy sort of shorthand. People understand that if you put stuff in the trash can, or especially like in Windows with the recycle bin, it's like, someone at some point may come along and suck it out of that recycle bin in this concept. You wouldn't store a million dollars in a trash can, and then like three weeks later you're like, what, like, God, because you put it in the trash can, all right, people understand that's not gonna, especially if those things can be taken away. And then finally, skeuomorphs is a slightly different definition. Skeuomorphs are when you adopt a design sort of language from something that tends to be ornamental that's not necessary. So the trash can metaphor is sort of necessary. I couldn't, I have to make it a trash can for people to understand that metaphor. If it's a skeuomorph, it means it's actually gonna adopt that design appearance, right? So in the case of the trash can in macOS, you could argue it's skeuomorph because it even looks like a little trash can. In fact, it used to be like a little rusty trash can, and when it was full, it would actually like bulge out of the sides. So you can actually argue that the original trash can is kind of skeuomorphic in that it actually adopts what trash cans look like in the real world. But sort of the classic ones are like the shutter on a camera. There's no reason to have the little mechanical click shutter on it. It could just be a beep, but we've adopted it, and therefore it is a skeuomorph. So we just had a camera app that just goes beep whenever you take a photo. That's fine. It's not a skeuomorph. If I go click, then that's a skeuomorph. So skeuomorphs are not required at all, but they are a design strategy. Now, I pick an example, like a shopping cart, okay? So what do people think a shopping cart is? Is a shopping cart an affordance? Is a shopping cart a metaphor? Raise your hand if you think so. How many people think a shopping cart is a skeuomorph? So the correct answer here is that it's a metaphor, right? Like, if there's a shopping cart in the real world, and I'm making sort of a version of it, it isn't quite like a shopping cart, but we can use it as a crutch for understanding. And so it would be a skeuomorph. Now, how you design that shopping cart is what needs to come into play. So you could, for example, to make a shopping cart skeuomorphic on Amazon is when I click on the shopping cart logo. Right now, it's just like a list of objects that are in my virtual shopping cart. You could make it a skeuomorphic shopping cart, but when I go to that page, I see like a little shopping cart, and it's full of like boxes of the stuff I ordered. So it's taking on this sort of ornamentation and visual design of shopping carts in the real world. And that's definitely not what you see on Amazon. So it's a metaphor, but not a skeuomorph. And then in that design, you might have affordances that basically show how the shopping cart is going to be. It's a little bit ambiguous how you would have affordance you need in a shopping cart. Like, obviously, a shopping cart in the real world has a handle that facilitates pushing, so it has like a physical affordance. Or probably, like, affordances are lower level in general in shopping carts, yeah. Would you say that often skeuomorphs can be metaphors? Often, but the camera click is not a metaphor. Like, so when it goes like this, it's not, like, if a beep is also, yeah, I wouldn't call that a metaphor for how a camera works. Yeah, that beep, yeah. But it's definitely a skeuomorph there. Metaphors have to be more conceptual. They're not just like a tiny, single example. They're like how a whole desktop works, or how a file system works. OK, so hopefully that makes a little bit more sense. Any questions on these? I don't have a corner case. Would it be fun for them? So these aren't like a stack. They're not even like, they're just like a bender, and they're three totally separate, but sort of related entities. OK, you're going to wish you asked a question after you see this pop quiz. I've been saving these up. This is a mega-puppet. This is the longest puppet of the semester. So there is a back. I've been saving these up. I've been saving these up. I've been saving these up. 1.5 кг. тушки цукрової пудри 2.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри 3.5 кг. тушки цукрової пудри Afiyet olsun... Ви можете використовувати цей курс на www.marcoparet.com Ви можете використовувати цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Ви можете використовывать цей курс на www.marcoparet.com Okay, start passing those in. Pass those in, pass those in, if you're wrapping up, that's okay. How many people want to go over the answers? Okay, we'll go over the answers. Okay, do I have all of them? Final, final call, okay, very quickly then, so, an e-book interface with virtual books on virtual shelves, best answer, it should be skewed more, it's a little tiny, there's no reason why an interface has to have little books on little shelves, a library could just be a list of stuff. So the best answer here is definitely that it's skewed more, it's a design of like stealing shelves from a library. Okay, that little neural region in the corner, what is it affording? It should be affording sort of gripping or dragging. or pulling or resizing. It's the physical conveyance. What is it offering to you? And that extra grip is kind of making you get a grip. Any kind of words like that, it'll be acceptable. Okay, floating windows. We talked about this, I think, two lectures ago. It's an example of a design pattern, like a Netflix uses it to have to stop people navigating away from the page. It's a convenient thing that has worked in many websites. We went over this one already. The shopping cart mechanism on a website. The best answer here is a metaphor. Can anyone guess what the next best answer is? It is a, it's like all that apply. It is also a design pattern. So it is not a skew in work unless you made it as such. It's not an affordance because affordance is a very specific thing, like the knurling pattern. You wouldn't call that a metaphor or necessarily a design pattern. It may have affordances inside of it, but in general, a shopping cart is not an affordance. Okay, using design, yes. So, question for the shopping cart. Was there, does that also reflect all that apply? It is. Would you consider affordance for that as well? It depends on the design of the floating window. Like, just floating window is not an affordance. If you like saying it's a car, an affordance, like, it could have affordances in it, but I wouldn't say it. Just that description would probably, I would say it's just a design pattern. Trick question. Okay, using design patterns is beneficial because, like all that applies. How many people put, you don't need to reinvent the wheel? That's a definite benefit of design patterns. How about optimum proven design? Yes. Users will have familiarity with the mechanism. That's also a good thing. And promotes interface consistency across apps and platforms, I would say yes. So all, it's like all that apply. Pick Hyman Law from last week. The best answer here is C, the time to choose between a set of options. It might be, you might have put in D for the number of items in the submenu. That's one application of Pick Hyman Law. But what Pick Hyman Law is about is choosing time. How many people put C? Okay, this one I saw a surprising number of answers. So the four-and-a-half rule, which is in sort of the name, like the gif is in the name. It should look like 5 around, then your time goes like this. There has to be this inflection point where this is linear time, and this is basically linear time. I saw one like this, I saw one like this. Those are definitely wrong. So if you have some sort of a king, the key thing is that if there's non-linearity in human perception, if there's some sort of a bendy thing around 4, 5, or 6, we'll give you points. If it's like a bell curve, it's going to be hard to give you any points on that. Okay, remembering something from your mind without a cue would be an example of recall. Remembering something in your mind, or using a sensory cue to reach for something in your mind would be an example of recognition, because you're using a cue. And then having a little printer on the print icon would be an example of facilitating recognition. It is not increasing your mental model, that's kind of a gobbledygook answer. It shouldn't increase your cognitive load, we haven't really talked about cognitive load, but a good icon does not increase your cognitive load, it should decrease it, if anything. So the best answer here should be recognition, that's why it's a little print icon. How many people have put C, recognition? Ooh, wow. So how many people put A, mental model? How many people put B, cognitive load? C, recognition? D, recall? And people didn't get that far in the quiz? It should be recall. You just have to recognize that it is a little printer, then you know that's probably the print icon. Model human processor, MHP, the primary purpose of it was a method to compute how long it takes users to perform tasks. That's what we did with Project Ernstein in the call center for 9x. is it's a method to figure out how long things take. It was not trying to make a computer microprocessor that simulates human cognition. That's an interesting goal, but it didn't make any processor. And it's not a unified system for handling brain inputs either. It's just a, basically, it's almost like a design method. When we use the term below the fold, unless any of you are in the newspaper industry, it should be the area of an interface that appears outside the immediate view of the user. When you go to a web page, what you see is above the fold. All the other content is below the fold. So it should be A, here. In not us, in the other world of newspaper people, it would be D, the bottom part of the front page of a newspaper, but again, I don't think anyone here is in the print business. And then finally, I gave you a recall question. And you probably are like, mobile places are so much easier. So in an information-rich world, what is the scarcity source? Human attention, yeah. So if you wrote attention or human attention, like brain power or something that is about the humans, you would get that. OK, that sounded brutal. OK, so today we're going to talk about observational methods. We've already done one lecture on observational methods. And we also did a lecture that sort of talked about things like contextual inquiry. We're going to resume and talk about some other methods, just to give you a sampling that these are not the only methods that we used. Obviously, beyond surveys and contextual inquiry, there's other methods we're going to do sort of a whirlwind tour of those today. So this is a reminder for contextual inquiry, the one that follows that master-apprentice model. And you want to understand requirements by actually going to the place where the behavior exists, whether it's a place of work at someone's house. And you're going to observe the users performing those tasks similar to how they'd be performing it with your intervention, with your assistants. You're seeing how they document insurance claims, and you're going to build it. Once you understand that process, you can sort of start. strike out and build a new process for them. And the reason why you go in person is that you can remind people to tell you what they think or how they approach tasks outside of that environment. You can bring them into your lab or bring them into your interview room and you try to have them explain what their job is like. It's just gonna be really terrible. But in context, like after the actual play, it's much easier for people to point that and relate to what they're doing. And they can do things like think about it. They can say, oh, we're responding to a fire. Oh, this plane just came in. And you can ask probing questions right there and stick to it. It tends to be much more reliable. Now, we also talked about designing the thing right versus designing the right thing, which is a little bit complicated to unpack and good thing it wasn't in that pop quiz. But the key thing is that you wanna know if you're designing things right. Because if you wanna gauge that, you have to understand what your metrics are. We've had a whole lecture on how do you pick the right metrics for knowing that you're improving the funness or the intuitiveness of your script. We don't have units like news or brand, or we have to think about how to quantify them. So you can have things like, do we care about utility? Is the system functional? Is it easy to use? Is that usability? The aesthetics, is it pleasurable to use? Is it attractive? Do you enjoy using it? You can also have meta things like does the person, does the user identify with their product? A lot of Apple products are about building sort of a self-identity there. So they're trying to grab you and sort of build their own kind of brand around like an internal brand. A simulation, once the user is inspired, or they're wowed by the product, it does, Adobe Photoshop make it, not only a creative tool for designing, but it's the act of the software making you create it as well. And there are value systems behind the brand as well. And to apply more to kind of fashion, but also the technology companies have sort of a value that they convey to users. So we've talked a little bit about how you do summative versus process data. The main kind of subjective, which is sort of perceived happiness and all the kind of qualitative. things that are about qualities of a thing. So the fondness of an art taking, we talked about feelings of support. Very hard to quantify that into numbers, but you can try. And then there are objectives or quantifiable methods. So the percentage of users who found jobs, that's a number, it's 72%, right? Or the number of times they clicked order over six months, there's a number there. It isn't that you have to interpret that data, it is itself a number. So subjective is sort of numberless, but you can try to code it into a number. An objective typically has a unit, like number of clicks, number of errors, number of times they hit back, percentage, what incoming salary, whatever it may be, you can quantify those into there. There's two types of approaches about how you can think about how things are done right. One, which is sort of the one favored by economists, is that you study people's choices. So you assume that people are rational and they do things that benefit them. And so therefore, if you look at what they do, you can try to infer sort of their value system and so on that they're selecting on. And then, you know, the kind of more psychological approach is that you go ask them directly. So you have to self-report, you go to people and say, why did you buy the latest iPhone? And they will tell you, and of course then you have to interpret them. So because this is a kind of a messy science, is that there's a really strong need to have a whole bunch of techniques for measuring experience, right? We want to know about the funness of an arcade game. I challenged you in this class over like five minutes to come up with a method to probe this. People are like, well, we can count the number of quarters, or we can stopwatch the time, or how much they smile, et cetera. And so we want a lot, we want formal methods, things that are vetted, like basically design patterns of methods to be able to experience, to measure user experience, right? And you have to sort of go with this under the name that people don't have a great sense of it themselves. You know, if you tell them, if you just ask them afterwards about something, they're going to give you a very subjective and possibly careful response. There are other constraints in these methods, and we're going to go through a bunch of methods today, and we'll talk about each one of these. I would say there's four or five main splits. So one is participant effort, right? How invasive is it to you? Does it require a lot of effort on behalf of the user, or is it basically invisible to the user? How intrusive is the experiment itself? You can collect two types of data, quantitative data, and also qualitative data, so how are you feeling versus how many times have you eaten food today? You can have where it's deployed. Now, obviously, sometimes you have to run things in the lab, a lot of the technologies I create are sort of so fragile and there's so much hockey in putting them together, there's no way to give it to someone and expect it to come back a day later, so we run almost all of our things in the lab, occasionally we run things outside the lab. But of course, that's gonna be much more fake than in the real world, in their home, out when they're out with friends and so on. There's granularity, right? So if you ask them only once at the end of a one-month study, that's gonna be very low granularity. On the flip side, if you ask them once, sort of like a survey every minute, that's very high granularity, but of course, you're trading it against invisible versus invasive. So the three categories are generally momentary, so that it's just an instance in time. Episodic, so it happens basically, it's a momentary, it's like the end of the day. Episodic is like after an episode of something, like after every single time you eat a meal, I want you to fill out a survey, it continues, it's like every 10 minutes, it's just gonna prompt you to have you eaten, have you eaten, have you eaten, have you eaten, right? And then finally, you can have things like food compensation, yes, no, dot, dot, dot, there's all these things that are all trade-offs. A couple of different methods, and again, this is not super critical, but this is to say there's also a variety of different ways that you can test things. So the most common that you often see is repeated sampling, so you ask someone at the beginning, like you give them an iPhone, before you give them the iPhone, you make them fill out a survey about how they use smartphones. And then you give an iPhone to them for a month, and then at the end, you basically interview them again and see if their smartphone has a choice. There's also this notion of longitudinal, which tends to be over a long period of time. longitudinal HCI, it tends to be over a year, even decades of use. And so it's many, many times, or maybe once a year for 10 years, and it tends to be smaller numbers of people. You can also do things that are cross-sectional. So let's say we want to test how 4th graders, 5th graders, 6th graders, 7th graders, 8th graders use smartphones, let's say. We're not going to find a grade of people and then have to wait 10 years for them to go from 2nd grade to 12th grade, and it's just going to take too long. So what you do is you do it cross-sectionally, where you interview 1st, 2nd, 3rd, 4th, 5th, all the way to the 12th grade, and basically each one is a cross-section, and you do it simultaneously at the same incident time. And then there's a whole bunch of retrospective ones where at the end of something, after you've used the smartphone, you say, hey, how did your smartphone habits change over the last month? And of course, this is probably the weakest methodologically, because humans, again, are really bad at this. Okay, so here are the methods that we're going to review today. That's lifelogging, diary studies, camera pros, experience sampling, data reconstruction methods, elicitation studies, and crowdsourcing. And some of these you'll already be familiar with. It's actually fancy games. Let's start with lifelogging. So the mantra of lifelogging is that I want millions of data points about the minute details of how you're moving, for example, with far more numeric accuracy than you're technically able to tell me directly. So the classic example of lifelogging is like the pitman, where no one's going to imagine one to report how many steps people take. You would never build an app that's just like, I took a step, I took a step, I took a step. That'd be one way to do it, but it'd be totally bonkers. And so you have a technology that does the logging for you, and it's basically, I can get very minute details. I know exactly at 12.02 p.m. how many steps I took because this software was doing it or this hardware was doing it, and I can collect millions of data points that way. So it's great. Okay? So logs, in general, there are technologies that log your activity. This is an example of one that I took a photograph, I think every minute, and at the end of the day, you should look at this photo timeline of all the people you interacted with and when you were eating lunch. And this, again, is logging. your bike, and it doesn't have to just be step-counting your photos or your geolocations or your charts, there's also one that basically records all your video inputs. So all these are examples of life-logging. And then what you can do with that data is you can see how different external factors basically intersect with that data. So for example, if we're tracking your step-count, we can correlate that with weather in Pittsburgh and see does weather affect my step-count. I run less, but thus it is more important, for example. And you can also do things like demographics, gender, movement, depression. Every single time in the photo stream, if you're doing, let's say, face recognition automatically, every time you come to DHCS and it's a top quiz, your mood just goes down by two. We can track that with life-logging, but you wouldn't have to explicitly necessarily type something in it. We had a way to automate, for example, life-logging your movement. Or let's say, pulse rate, like your pulse always goes up when you see that blue slide from on the screen. So the pros of life-logging is that it's automatic. The user shouldn't really have to input anything. It logs it all simultaneously. And with the advent of smartwatches that can collect all this biometric information, it's really advancing quite a lot. And it's really useful for data to be really tedious or careful. Humans are not going to, every one minute, type in how many steps they have or measure their pulse rate. And so for life-logging, it's a really serious pro. Of course, the cons is that you need to make sure that it's not interrupting people's natural behaviors. We had a, maybe I've mentioned it before, but I had sort of a life-logging-esque sort of study that we put glowing lights on people's shoes. And the more they walk, the more they glow. And the idea was that you show up to a party on a Friday, and your shoes are glowing, like they're beaming with light. And it was supposed to be like a social crush. People are like, hey, what's the deal with your shoes? Like, oh, I reached my step count this week, and we felt like, high five, everyone's going to love it. So we did this interesting social feedback cycle. It turns out it was true for like 8 out of 10 of our participants, or whatever, 18 out of our 20 participants. participants that were terrified. They were like, I'm trying to walk home through a dark parking lot in Pittsburgh at night, and my shoes are like blinking in blue, like everyone's looking at me, I'm not really scared. So that is not what you want people to relate to. You have to like drop them out of the study because they're like terrified, and then you have to get back in the parking lot. One of those examples where you can't always think about everything, like are you committing that IRB? Did you think that was kids at all? No. I feel like they'd love it. It really depends on your personality. You need to really make sure that it's on a regular basis. Ideally, I can kind of put on a script and it runs, and probably the weakest part is you get very little context. Like it may be like, wow, all of a sudden at 2 p.m. there was so much step counting, there was so much heart rate, but because you didn't have anything from the user, you have very little knowledge about why. Like why did their heart shoot up? So sometimes you have really rich, fine-grained data and really no way to understand why that's happening if you don't have that sort of back story. So in sort of summary here, generally the effort is pretty visible. It sort of just runs with the person going. It's very quantitative. It's like heart rate, step count, calories or something like that. That's great. It's not very qualitative. You never have to ask the user anything. It can work in the field of the physical lab, and it's continuous, which is really nice. It's fine-grained data every minute of the day now. In contrast, we have diary studies, and these are just like keeping a journal of census. So I want to hear all the stories that you think are important and relevant to, and you basically have a context like this kitchen over a given time period, exactly as they're happening. So every time you're thinking of a kitchen, or are you passing a grocery store and a bar, are you hungry? That's the trigger. It's basically like, ooh, you're right. I want you to tell me everything. The more content, the better. You log interactions. You can do it in a rimmed diary, but you can also have an app that pops up on your phone, and typically gives a trigger to prompt the diary. I need to fill out a diary episode, essentially, for that day. And it's quite invasive, because obviously every single time you get on the bus, or every time you jump in an Uber, you have to fill out this two-page survey. That's quite invasive to your day. It tends to be very qualitative in nature, because if you're trying to get this sort of, like, why did you have to take this transport, how do you feel about it, are you reflecting on your carbon emissions, whatever it may be. It's definitely a field study, because, again, you can give people this app, or a stack of papers, and they can filter that anywhere. And instead of being fully contiguous, it's not episodic, because it's like the episode of when you sat down to have lunch with your friends, or when you went to the grocery store and you shopped for 30 minutes at John Peel, and so it is episodic, as opposed to momentary working through this. So definitely they recommend doing seven to ten people in a diary study, and you want to do it over the course of a week, because, again, it's so infrequent, like, how often do you guys go grocery shopping? Some of you, it's like once a semester, so it's not going to be a very good trigger, right? And so you want to have enough time that basically the events, the triggers that happen, like maybe you only take public transportation on Friday to get to some club or something, I don't even know what people do anymore. And so that's going to be so infrequent, you need a long enough time to answer that. It does really pair well with interviews, so what you do is after people return your diary, or you're seeing them upload to your server, is at the end of the study you can say, hey, tell me about this episode, like, last Friday, you sent like an e-mail to Laura to tell me what happened, right? People, as I mentioned, paper, there's mobile apps, you can also do it by voicemail, so you can actually just call into a phone number and basically just record it and then sort of digitize later. That's pretty cool. This is an interesting study, I don't know, I don't remember exactly all the details, but early on, so this was, what, 2008, so this was right around the time of iPhone, one iPhone was released in 2007, so smartphones were just sort of taking off. is when people wanted information on the go. And so every single time, this is again, imagine you didn't have a smartphone and if you wanted to look up something on Wikipedia, you basically log that as an episode. You're like, oh, we're in like a debate about, you know, when Charlie Chaplin's like peak career was, and I wish I could have gone on Wikipedia then. Again, this is previous to the smartphone for most of the planet. And so they basically log, they go directly, where they log what the information they wanted is, how they would have access to it if they had wireless connectivity. And it basically was used to inform how you might do search on the go in a mobile context. So the pros are that it's very useful for processes that happen at times and places that researchers cannot observe. If it's like, you're hungry at three in the morning, or you can't sleep at three in the morning, you're not gonna have a researcher like, looking like in your bedroom to see if you're awake or not, right? So you're gonna have to do it in the, the binary study lets you basically capture all these things that happen serendipitously, which is really, really important. And you can't put people in the lab. Unpredictable times and so on, right? Like, good for looking at variables that change over time, like mood, and you can also really complicate one to capture with just some light logging. There's user-defined data, where the user sort of expresses their own, sort of creating their own value system on the data they're giving you, so that can be interesting. And what's important here is that because it's not retrospective, like you could ask some of these questions at the end of the study, but it's likely to be much more errorful. And so this is a way to capture that sort of qualitative interview-like data, but right after the thing that happened, which is gonna be more reliable. Cons, of course, is that if you make your trigger too frequent, like someone's just a really hungry person, they're gonna be filling out these diaries all the time, and that's gonna be pretty intrusive. Periods, people would also say that, you know, because they're filling in the blank, that the self-report is really unreliable, and why are they eating now? They may have their own opinion on that. And with many of those, they're all sort of fill-in-the-blank kind of questions, so it can be quite time-consuming for the experimenters. to recruit participants because it's so much more invasive to people's time than a life-logging study. And often what you'll see is you deploy a diary study and they're really excited about it for the first two days, and by the time you're like one month in, let's say, people are just not filling their mouths. They just get bored. And it's like often it's the same diary every single time, and the novelty of just being in the experiment is sort of that Hawthorne observer effect that sort of slid down. So you get a high dropout rate or just no participation. Okay. A very cool one. This is actually one of my favorites. It's kind of has this interesting property. It's called a camera probe. So they want to hear that you take photos of things that are important throughout your day, and then we'll meet later to go over them. It's trying to balance a couple of different things. So here's a very interesting, well, I guess I pulled this out of this little paper from CHI. It's an amazing DCI conference. So we undertook a qualitative study of the homeless population in metropolitan U.S. City. We tried to understand what it means to be homeless, how technology from cell phones or bus passes affect the day. We provided a variety of opportunities for technological interventions that can empower the homeless population. Now what they did, okay, is they met with these people that were homeless, and they gave them one of those disposable cameras, those little kind of Kodak, you know, paper, you know, disposable cameras. It comes like a big box or something like that. Again, this was sort of pre, the homeless population did not have a smartphone, especially when this was published in the fall of 2008. So it's the beginning of this mobile era. And the instructions for what, you know, and they're on the camera probes. Take photos of places or situations where you need the help. Take photos of things you use, telephone, buses, radio, television. Take photos of your daily activities. There's no such thing as a bad day. There's no such thing as a bad day. There's no such thing as a bad day. There's no such thing as a bad day. There's no such thing as a bad day. There's no such thing as a bad day. It was really a way for people to document their loss, so they just took photos of things that they thought were interesting. It's not like a realistic world. And so this was a way to probe, to see what people thought were important in their lives. And so if you think about it on that spectrum of sort of invasive to non-invasive, taking a photo is really quick. Like it's just like a snap. It's an HM machine that I always go to, snap, I take a photograph of it. And it's a very quick, you can have it in the field, it can be sort of, it's sort of episodic too because you're basically taking photos at periods of time that are important to you. And then at the end, the interviewer, and the, in this case, like a homeless human subject kind of participant, come and you go through the photos. And then you get all the rich details, like why did you take a photograph of whatever, this intersection, like oh, this is where I always meet my friends every week, and we have a support group, blah, blah, blah, blah, blah. I don't remember all the details of the study. But an interesting sort of approach. Now you could probably do this with something like a smartphone app, right? We just take photographs, we can just look at people's like Twitter feeds or Instagram feeds and be able to reflect on that later. Yeah? Do you think the fact that it was something the user could interview before meeting though was beneficial? Yes, because it probably made it more candid. Where smartphones definitely. Like, curated. Yes, yeah, I think so. So I'm not sure exactly how well this would work now. I think I'd probably still give people disposable cameras just because it's more real. So I'm sure they're almost dead now. Probably pretty hard to find them. Another really important method, which I've used myself, is experience sampling. So in this case, the mantra is, I want to know what they're doing all day long. It's okay, it doesn't have to be interesting to them. Did you eat lunch yet? Was it good? Are you happy? And typically you see it is done on kind of smartphones or cell phones. And what they'll do is it'll periodically interrupt the user and basically have them answer a small kind of questionnaire or question. So it invokes a set of, a set times or random times. It could be right after lunch, every single day it says. you eating lunch at, and what cuisine did you eat? And basically, you're sampling their experience with where they get to work, experiment sampling. So it's some sort of a cross between an automated logging and a diary study. It's like a mini diary study, but that happens continuously when a client logs in. So it can be about information about the context, like are you with friends right now, or are you inside, or are you outside? And it can be kind of daily life, like what activity are you doing right now, how are you feeling? So it's much more reliable than asking hypothetical questions at the end of the day, like how are you feeling at the end of the ACS? Like, oh, I feel like I was doing OK, versus asking right then to get out of class. So here are just some examples. In this case, it says, where are you now? And it says, in transit, at office, at home, somewhere else now probably has a donut lane. This is the how do you feel right now, compare that to very good, and it was a multi-part survey. It's a very simple example. So what's great about this is that it's mediumly invasive. I wouldn't say it's invisible, because clearly it's prompting the user to fill out something, but it's much less invasive than other methods. You can get qualitative and quantitative data. You can even have it so that the app records things like step count at that instant in time. But you can have them fill out, like, are they eating right now? It's not really subjective. Like, it's a yes or no question, essentially. So that's pretty quantitative. And you can also get subjective things, like your mood. It definitely happens in the field, and some of the things are momentary. It may happen, but every 15 minutes. It's kind of a discrete interval in time. Like, it's only sampling that one second or that one minute. And so it's sort of momentary and continuous all at once. So this was an interesting study. This was, again, like, it was done on a PEA, but it was, you can imagine this on a smartphone. And what it did is it prompted people with, at a random time of the day, or at a random time, like every 15 minutes or every half an hour, every hour, something like that, it prompted you with a person from your contact list. you to share a particular level of geospatial detail. So imagine like I pop up during spring break on your phone and it says are you willing to show Professor Harrison where you are. Now you can say I'm you know in Shadyside, I'm in Pittsburgh, I'm in Pennsylvania, I'm in the continental US, I am on Earth. You basically do the different rings and again who you show and when. Like when do you want to show your parents or something or when do you want to show your friend from high school that you're in like this exact house in Shadyside may be different. You may say hey whatever I don't mind if I let them know that I'm in Pittsburgh but I don't need to let them know like I'm in Florence or something, right? And so they did this repeatedly with random people from your contacts and random people and it was as random where you were at that instant in time and they're able to draw conclusions about how people think about privacy and what level of granularity of like geospatial data they want to show to people, right? Kind of interesting idea that it'd be very hard to get that through an interview. It's a very clever study. In this other example they look at how interruptive people are. So basically on your desktop computer, this was a study run at CMU, is periodically on your desktop computer a little window would pop up like a floating window would pop up and it would say how interruptible were you 30 seconds ago, 10 seconds ago, something like that. And you say well I'm frantically trying to get like this homework done, I'm not interruptible. Of course the problem with this study is that by interrupting them you basically like you've sort of made it so they're interruptible, right? But nonetheless people are basically saying like right now I don't want to be interrupted or right now you're just like surfing on Facebook you're like sure you can interrupt me all you want. So they evaluated how interruptible they were with this little experience sampling method. And what they were doing in the paper is they were collecting sensor data about the noise level in the room, like through a microphone, whether the door was open or closed, and they were recording on the laptop, they were mousing and typing and what they were able to do is take all that data which is now been sort of labeled from machine learning and able to model so that your computer has a machine learning model of how interruptible they were. you are at any given moment in time. And they actually benchmarked this with an A-B test. They gave humans a video camera in the room, like at people's office at CMU. They gave them a video feed, and they asked the human observers, guess how interruptible this person is right now? They were like, I don't know, maybe like three out of five. And then the algorithm would predict. And often, they showed that actually computers are better at guessing user interruptibility than human observers, which is a really cool result. Because as we know right now, interruptibility is ridiculous. My smartphone will happily ring in this class. And as you already heard, all semester, my PhD students are messaging me. There's no notion of interruptibility. This computer always delivers messages, regardless if I'm stressed out, or in a bad mood, or in a happy mood, or lots of time in the hand. It's just an equal opportunity offender. And it shouldn't be. They really should have a notion of interruptibility. Another just quick one to look at is quickly, this was an experiment that was done by a CMU colleague at Microsoft. Every 30 seconds, what it did is the phone would wake up and record all of the different sensors. And so on, with gyro, mic, camera, touchscreen, you know, the handy, every sensor that was in the phone. Then the screen would wake up, and it would vibrate the phone. And the user would basically take out their phone and ask, where was I being stored? So you'd say, oh, that was in my pocket 10 seconds ago, or that was on my desk, or that was on my night table, or that was in my backpack, or in my handbag, whatever it would be. And what it is, again, they use that data to build a model so that phones recognize where they're being stored. So you can imagine modifying the behavior of your phone, of where, if it's in your pocket, versus it's on your nightstand, right? Like, for example, when it rings, I don't need, like, vibration is really important when it's in my pocket, because it's right against my skin. But I don't need the screen to flash. Like, no one checks that their phone is ringing, by going like, this, the screen is on, right? Like, you'd feel it, and you would hear the sound. Conversely, if it's on a table, facing upwards, the screen's probably more important. Like, it'll flash, like, for who the incoming caller is. And vibration's probably not so effective. Vibration takes a long time. It's a lot of power unnecessarily. You probably just want to use the sound, right? You don't need to have a chatter along the table. So phones know where they're placed. You can actually not only do better battery saving, but also basically modify the experience to be more like a friend. Here's a study that I ran. So what we did, we built this little tiny device. It was a very small, little $1 device, had a little tiny microcontroller on it. And it's a coin cell. And it fits in a phone and an LED. And what the behavior was, is that it would go to sleep anywhere between like seven minutes and 25 minutes, randomly. And then the behavior was, it would start to blink the light. Blinking, not just on, but blinking. And we recorded how long it took the user to press this button to turn it off. And people wore, I think like seven or eight of these little devices. So there was a couple students a couple years ago, they're walking around on campus for like the whole day. We did it one day, many different people. And basically it was a way to see how often they noticed different parts of their body. So when you're sitting like you guys right now, there could be a light blinking on your shoe. Let's say whatever reason, like a text message that arrives on your shoe. You're never gonna see that because you probably can't even see your feet. But if you're like walking frantically between class and your feet are peeking out in front of you, you probably would see that light. And so in this paper, you're walking through the paper, we actually modeled the visibility from people's eyes of all these different points on their body. So basically inform how you could have like a body interface and what parts are visible. It turns out, unsurprisingly, the wrist is actually the best. It tends to swing in front of you. And probably most of you, if you don't move your eyes right now, how many people can, without moving their eyes, how many people can see their wrist in their field of view? And then like, yeah, I guess making you raise your hand is kind of difficult. But I would guess that probably 20% of you could have seen your wrist without moving it. And you might be able to see your shoulders. But actually most of these points would be invisible. And that's sort of what we wanna see. Most of the time you don't see your body, you just sort of see the world. So this is again, one form of experiencing. So it can be very accurate. It minimizes the effect of retrospection because it makes it kind of. between right then and there at that time. You can ensure a good distribution of points all the day. Unlike a diary study where you're relying on the participant to be the trigger, here it's like a random number generator. You know every roughly 15 minutes it's going to be set on and you can get a wide variety of data. So it's a really powerful stuff. The big problem is that it literally interrupts people in the middle of their tasks, which is going to be annoying and people are going to drop out and the higher you crank it up, like if you make it every 20 seconds, something is happening, you're just going to not do the study at all. So there is this kind of danger. Okay, let me just skip one method. It's not as easy as setting one. Okay, here is a cool one. So elicitation studies are about, it's basically like a guessing. So here's a model. I'm going to show you an action and you're going to tell me how to do it or what caused that action. Okay, it's a little bit confusing. So elicitation studies comes from the word elicit, which is like a vote to draw out a response from someone. So I might elicit from you answers by letting you wait in silence while I'm going to ask you a question. And the idea is that you present the effect of something, like the effect of a gesture and you get the participants to design what caused that event to happen. So it's very much like a think-aloud protocol because they're basically inventing it as they go. So let me just explain an example here. So this was a great little study. You had people sit in front of a TV. It was a dumb TV, okay? It was all Wizard of Oz. And what they said is, we want you to design a gesture for increasing volume. Right, so they're like, this is a smart TV. We're lying to you, but it's like a discussion study. This is a smart TV. Please go ahead and like turn it on. And they're like, I don't know. You know, and then it's like do an action and they invented on the spot. And then they're like, okay. So then like the researcher turns it on and say, now, like go to the next channel. And they'll go like, I don't know, like this or this or this or something. They'll perform some action. And you're getting them. You're not telling them anything about the design. They're basically inventing. it or guessing it as they go along. And if you do this with enough people, you actually build up a really interesting corpus of data. And what you're looking for are commonalities. Everyone does this for this channel and this to back and this for up and this for down volume, for example. And you're like, oh, everyone gives you that by default. Basically, it means that people know how to do it. But you don't have to train them because they already instinctively know that that's the gesture. Wouldn't that be the best interface of all then? The one you walk up to and you already know how to use it despite never seeing it. That's where they go. It's more like a graphical user interface. And then at the end, you collect all this data and you co-link this to 20, 30, 40 people. And then you basically say, we're putting forward good design. We're putting forward a design that we think is good. So in this case, increase in volume was move your hand up. You can sort of think about where you had guessed that the design was some kind of interesting to you. I mean, you cover your mouth, like this, like that, or like that, makes you quiet. So again, it's all about agreement. I'll show you a graph in a second. Here's another one that was really interesting. They had a multi-touch table. And they're trying to get people to guess the actions. Like, if you wanted to pan, you wanted to slide this whole thing, not this one object, but you wanted to slide the whole background, like sort of Google Maps style, what gesture would you prefer to perform? So some people are going to probably put a single finger on a move. Some people may put their whole hand or even two hands on. You might also say, I want you to select that square. How would you select that square? Probably most people just sort of click it with their finger, right? And so again, you do this for all these different things. How do you enlarge? How do you delete? How do you lift something off the surface and put it in your clipboard? Like, what's a cut gesture in multi-touch? That one we don't really have. And so they did this experiment. And again, they collated all those answers. And they put it all together. And they're able to put together a recommended thing. So hey, look, rotate, grab corner. People have already thought about some good ways to do a fake out, too. You can think about undo. This notion of sort of scratching out something to undo, kind of interesting. Here's some more. So enlarge. multiple methods. So there's the pull hand. So this is enlarge and shrink by pinching. And this is the enlarge and shrink by swaying the fingers. In this case, they say both methods should be supported. Because not everyone does it like this. Some people are going to do it like this. And so maybe both gestures should be appropriate. Now, here's what these graphs look like. So here is all of the gestures that they test. This is way more than what you have in iPads. So here is, for example, cut and undo and out and switch tasks and close and enlarge and maximize, delete, shrink, and so on. And there's a couple, like move a little and move a lot. And you can see that the agreement is basically 100%. All of the participants in this study did it exactly the same way. They probably put a finger on it and tried to move it. But as you get to these more exotic ones, like rotate, now you're in the game of 50% agreement. So 50% of people came up with one gesture, and then all the other 50 people may have come up with a second gesture, or even like 20 different possible gestures. And as you get up to really exotic behaviors like cut, like some people may be trying to scissor hands it on the display. And some people try to karate chop it into small pieces. You have no idea. And so the best design that came up was around 15% of participants had a common design. And that means that 85% of participants are going to be confused if your design only implements one of those things. But nonetheless, it gives you really interesting data. And they've actually found that if you put these two techniques together, so you go get 20 non-expert users, just like people off the street, and you do an elicitation study for things like multi-touch gestures. And then you go get like five expert designers, like PhD in design, and you have them make a set of gestures for something. Then you go get a third group of people and you test that design on them. Generally, the elicitation study gives you better results than the professional designers. It's kind of an interesting result. Of course, it requires more people and maybe even more time than having like HCI experts come in. But it does show you that wisdom of the crowds is like a... real, real thing. The con here, of course, is that you may be biased by past experiences. When they ran that study for multi-touch gestures, some of those participants had already seen smart phones and tablets, even though it was only a couple years in. Until a pinch to zoom, it's probably because they had a phone that supported pinch to zoom. So now you're asking people to invent a new method or elicit a new method when they may have already been exposed to one method. So it's something to get track, and regular people are not going to be able to give you new things. So you have to keep in mind that regular people are not designers, and you have to keep that in mind. But probably no one has used a gesture... Has anyone in this class used a gesture-sensitive TV that you could wave in the air? So that would be a good example. And none of us are sort of biased. We have sort of our own guesses of what that might be, but because we haven't used one. But I'm sure if we used... If some Samsung TV came out that did it, we'd probably all end up using those gestures. It's basically like a design pattern. We all start converging on the same sort of things. And then, you know, because there is this uniform agreement that only 50% of people think a gesture is a good one, are you ready to make the decision that that is a good general solution? Tough to say. Okay, so crowdsourcing is another very common method to get good data from many, many people. So instead of a million data points from one person, I want one data point from a million people. How many people have done a crowdsourcing something? Experiment or application? How many people know what crowdsourcing is, like Mechanical Turk? Okay. So just a quick review. Mechanical Turk, which is Amazon's version of this, is you can basically post to this gigantic website and there's, I don't know, tens of thousands or hundreds of thousands of what they call workers, and you give them often a small task. So here's two tasks that I just pulled off. So this is a classifier receipt. So you're going to see a photo of a receipt that someone has scanned it on their smartphone app, and then the worker is going to spend like two minutes saying, This is a receipt from Macy's, and it's for clothing or accessories, right? Or here's one from U of India, that's a restaurant, okay? And so they're going to show them an image, maybe like 20 radio buttons, and they're going to click the right one. And for that, you would get three cents. That's a tool if anyone wants to sign up for a mechanical chart, supplement your CMU tuition, which is paying my salary. There you go. Here's another one. So, you know, find a doctor's home page URL, and you get four cents with this one. It says you can pick up in 10 minutes, but generally it doesn't. So they're going to give you some details like Dr. Chris Harrison, you know, HCI, not a physician. And that's their client, his personal web page. And so they'll Google search, and then they'll type it in to get four cents. You can just bang these out really, really fast. They're sort of micro tasks. Generally, like when we do mechanical chart studies, we always pay the equivalent of US minimum wage at the very least. So we sort of, our hits tend to get gobbled up quickly because we're offering more like 25, even a dollar per hit. And so we, but these people are just always talking stuff. So here's an example of a study that I ran. So we would give people, we'd show people an image like this, that has a person, also a side profile, and like sitting on this box. And what we do is we put a single dot on it. So you can see there's this green dot, like right here on my waist or whatever on this person's waist. And they were placed into one of four conditions. They stayed in that condition for the entire time. This was about, again, on body sort of touch. So number one, you're doing touch interface versus look interface, right? So how would you feel about touching the interface projected in this location versus how would you feel about looking at an interface projected in this location? So like I can have an interface on my shoe. I can touch it and interact with it, or I can look at it. Looking, obviously, is easier than touching my shoe. And then there's this notion of you touching versus other people touching, right? So how would you feel about me touching my shoe or asking you guys to touch my shoe? Okay, or how would I feel about someone coming up and groveling on the floor and checking their grade on Canvas on my shoe? So we're asking, you touching versus other people, and also other people looking, like do I care if other people look at my shoes? Probably less than if they're touching my shoes, right? So the example question that a mechanical tech worker would get is, as soon as they look at this, how do you feel about looking at it, and is there a way to get to that location? And it goes on a little survey, like, you know, I feel comfortable, I feel very comfortable versus I don't feel comfortable at all. And we were doing this kind of brute force, so there was all these dots in reality, no one ever saw more than one dot at a time, and they were asking it across all these different points of the body, and across many different poses, so sitting cross-legged, and standing, and you know, like, whatever, what they're called, like, man-sitting or something, I don't know. And so, again, they would see these dots, and what we did is we got responses from about 2,000 people, that gave us almost 40,000 responses, and we objected to a lot of people, they answered 20 questions in a row to get their, like, 50 cents. And what we were able to do from this data is build up a heat map of comfortability. So, here you go, here is your heat map. So, if you're thinking about doing this in a growing region, I'm telling you, the data does not support your starting point. And we found that no surprise to me, the arms, and especially the arms below the elbows, are the most apt point to basically do touch interaction, or looking, this is everything combined, so looking and touching, even other people looking and touching, so if you're at a bus stop, and someone's like, hey, can I borrow your phone to, like, call my friend, you'd be like, okay. And what you're going to give them is more likely to be your hand than, like, your thigh, or something else. And we actually quantified, you can break this apart by all these different conditions. So, here it is across all the different poses, and what's interesting is I can see that in certain cases, you have to start getting into this neat fringing here, where then it is possible to have, like, an on-body interface on your legs, for example. And again, you know, looking versus touching, as you break it down, you slice and dice it even more, you start to see that it gets noisier and noisier. But honestly, it's really hard to look at your own head with an interface. So I get a Facebook on my face, but I need to tear in a mirror. to be able to see my own face, right? So, faces are generally not good for looking, and they're okay for touching, but no one really wants to be like a pinch to zoom on their forehead, so it doesn't do well. But you do see that the arms, upper than those parts, and sort of the shoulders, right? You do see, like a bus stop, if you want to get someone's attention, you often will, at least in kind of American or Western societies, it is okay normally not to just tap people here, but you wouldn't pat them up here, or here. And so we were able to quantify that. We all sort of know it's true, but if you want to quantify it, we did study it. So, the great thing about crowdsourcing is that you can have thousands of people, super quickly, easily, we ran a study with 40,000 responses in like three days. And that was with all the people that we threw out, because they were just going in like, they were like, just like selecting, you know, find, find, find, find, find, find all the questions, so we ejected all those people, took down all the malicious people. And you're able to get data that you really would not be able to do any other way. The problem with this is that we didn't actually know how people were, we didn't know the details. We knew sort of the what, but not really the why. So I'll get to that in a second. So, you know, it's obviously limited to the web, generally, sometimes you can have it on a mobile app, so that's gonna limit the kind of things that you can do in a crowdsourcing study. You're not always entirely sure if your users are re-filtered by US IP addresses and US accounts, but even still, some people were VPNing in and saying they were from the US when they weren't. There's obviously a real danger of selection bias in crowdsourcing, because how many people, what kind of cross-section of society are signing up to do or send tasks on Mechanical Turk? Like, obviously that's not gonna be like some high-profile lawyer in Boston or some brain surgeon in Kansas, right? Like, it's gonna be a very particular demographic, which, of course, is gonna skew your results. So you generally want questions that are pretty easy, don't require a lot of background, and you generally get very little context. So for this particular example, what we did is we paired our Mechanical Turk study with interviews with experts. And we went to very particular people that knew very different things about the bottom, right? So here's the list of experts. So, you know, tattoo artists, jewelry smiths, massage therapists, yoga instructors, chiropractors. And these all knew very different things about the body. So, like, a lot of the chiropractors and massage therapists, they all knew about touching people as part of their jobs, right? They have clients that they've known for years and also people that show up for the first time. And you don't just, like, jump right in on someone and start, like, massaging them. There's sort of this ritual that humans go about where people are sensitive to get touched, where they're more okay with being touched, especially if you're a stranger. So, there's all sorts of really fascinating stories about how you get to know someone, how you kind of lead them into, like, a micro-massage experience and so on. Tattoo artists have really, kind of, body modification people, have really interesting thoughts on how to render, sort of, graphics on the body. So, if you're going to put graphics on the body, where are the best places? And that's because you want, you know, continuous areas. Very weird to have tattoos that go over joints, for example. And, you know, what parts are easy. Like, you know, this part, you obviously have a lot of tattoos here because it's a nice, big, flat area. And really interesting data about how you make straight lines look straight, even though the body is curved. Really cool stuff, but I'm not going to go into it because it's really neat. People like the accessory designer and the fashion designer, like, clothes are meant to emphasize some parts of you and de-emphasize other parts. Like, a good jewelry designer is trying to, like, draw attention to certain parts of your body and basically not draw attention to other parts of your body. Really interesting information about how you go about designing a piece of clothing, for example, that people feel, like, more comfortable in and, like, kind of highlights them in the right way and how they design for their clients like custom stuff. Really good. These are the best interviews I've ever run. Really fascinating. Spent two or more hours with each one of these people and I learned so much about them. This is why you always go to experts, just like in technical degree. You go to their place of work. You ask them to show them examples. Like, I was flipping through so many books of tattoos. I almost walked out of that place with a tattoo because it was just so fascinating. The science. You don't think of it as a science, but there really is a lot of incredible thought that goes behind, you know, tattoo art. So, anyway, in addition to just doing interviews. I also sort of made them do this very much like a mechanical chart study, but more exhaustive. So I gave them empty outlines, and I had them color it in. And so they had like a green marker for good, and a red marker for bad, and a blue marker for neutral. And by giving them these outlines, they were able to sketch all over it, sort of force them to consider every part of the body. So if I just ask them, tell me about the body, they may only focus on the arms. And the tattoo artists will never focus on the belly, because belly tattoos are rare or something, right? And so this forced them to basically go kind of top to bottom, any way they chose. And we talked with every single part. As they were putting something in, it was like, tell me why you think this is good, but this is neutral. And they said, oh, well, because you're crossing your legs, you can't see it. And it was just sort of a semi-structured way to guide the intuition. So it was quite successful. Okay, we have time for this, maybe one example here. So I want to put this in a context. I skipped day reconstruction because it's the least interesting, so don't worry about that. But here, let's say your mission here is that you're designing some sort of a new experience for ex-prisoners who are going to reintegrate into society and they're looking for jobs. And what you want to do is you want to better understand this demographic and how they even may use some of these existing systems. So what I want you to do is, in your group, you know, the group of roughly four, is take one of these techniques and how you might go about trying to understand how ex-prisoners are trying to reintegrate into society, okay? So break into groups. I list here what you have to pick. If you pick, for example, lifelogging, it's what data would you lifelog to understand this problem. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. if you can't hear me well. I have a several questions to you. Okay, so what team thinks they have a plausible design using one of these six methods? Or any design, even implausible? I heard some good ideas, so I'll let you set them up for me. Yes? Yeah, absolutely. So every time they do something that's jaw-hunting related, they take an interview, they take an application, they basically episode it, and then they basically fill it a little diary saying, hey, I picked up an application for whatever, and I'm going to have an interview next week, and blah blah blah, are these interviews going to be well, blah blah blah, I don't have a car, and again, you can elicit, I don't want to say elicit, I don't want to provoke you for elicitation, but you can suck out of them, what were the meaningful things about that interaction? Did they think it went well? Did they think it went poorly? Or did they think that their ex-prisoner status somehow affected the job interview? Okay, one more before I let you guys go. So imagine that you're in charge of designing a new version of Siri for cars, so it's a voice command, and you're tasked with defining what voice command should be available to drivers and passengers. So pick a method of how you would figure out that list of voice commands, and then share it. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second. I'm going to talk to you in a little second.\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T04:42:09.813961Z",
     "start_time": "2024-06-23T04:42:08.981294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "array = {'full_text': all_file_transcripts}\n",
    "df = pd.DataFrame(array)\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "allWords = ' '.join([twts for twts in df['full_text']])\n",
    "wordCloud = WordCloud(collocations=True, width = 1000, max_words=100,\n",
    "height=600, random_state = 21, max_font_size = 120, background_color='white').generate(allWords)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(6, 4))\n",
    "# ax.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "# ax.axis('off')\n",
    "# plt.show()\n",
    "# fig.savefig('../figure_panels/fig3b.svg', bbox_inches='tight', dpi=300)\n",
    "wordCloud.to_file('../figure_panels/fig3b.png')"
   ],
   "id": "308b7ec0b561f6b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x163d4ca30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "687b5970295093e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
